{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "from easydict import EasyDict\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2large = EasyDict({\n",
    "    '0':'구이',\n",
    "    '1':'국',\n",
    "    '2':'기타',\n",
    "    '3':'김치',\n",
    "    '4':'나물',\n",
    "    '5':'떡',\n",
    "    '6':'만두',\n",
    "    '7':'면',\n",
    "    '8':'무침',\n",
    "    '9':'밥',\n",
    "    '10':'볶음',\n",
    "    '11':'쌈',\n",
    "    '12':'음청류',\n",
    "    '13':'장',\n",
    "    '14':'장아찌',\n",
    "    '15':'적',\n",
    "    '16':'전',\n",
    "    '17':'전골',\n",
    "    '18':'조림',\n",
    "    '19':'죽',\n",
    "    '20':'찌개',\n",
    "    '21':'찜',\n",
    "    '22':'탕',\n",
    "    '23':'튀김',\n",
    "    '24':'한과',\n",
    "    '25':'해물',\n",
    "    '26':'회'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "large2idx = {large[-1]:idx for idx, large in enumerate(idx2large.items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class2idx = {'구이/갈비구이': 0, '구이/갈치구이': 1, '구이/고등어구이': 2, '구이/곱창구이': 3, '구이/닭갈비': 4, '구이/더덕구이': 5, '구이/떡갈비': 6, '구이/불고기': 7, '구이/삼겹살': 8, '구이/장어구이': 9, '구이/조개구이': 10, '구이/조기구이': 11, '구이/황태구이': 12, '구이/훈제오리': 13, '국/계란국': 14, '국/떡국_만두국': 15, '국/무국': 16, '국/미역국': 17, '국/북엇국': 18, '국/시래기국': 19, '국/육개장': 20, '국/콩나물국': 21, '기타/과메기': 22, '기타/양념치킨': 23, '기타/젓갈': 24, '기타/콩자반': 25, '기타/편육': 26, '기타/피자': 27, '기타/후라이드치킨': 28, '김치/갓김치': 29, '김치/깍두기': 30, '김치/나박김치': 31, '김치/무생채': 32, '김치/배추김치': 33, '김치/백김치': 34, '김치/부추김치': 35, '김치/열무김치': 36, '김치/오이소박이': 37, '김치/총각김치': 38, '김치/파김치': 39, '나물/가지볶음': 40, '나물/고사리나물': 41, '나물/미역줄기볶음': 42, '나물/숙주나물': 43, '나물/시금치나물': 44, '나물/애호박볶음': 45, '떡/경단': 46, '떡/꿀떡': 47, '떡/송편': 48, '만두/만두': 49, '면/라면': 50, '면/막국수': 51, '면/물냉면': 52, '면/비빔냉면': 53, '면/수제비': 54, '면/열무국수': 55, '면/잔치국수': 56, '면/짜장면': 57, '면/짬뽕': 58, '면/쫄면': 59, '면/칼국수': 60, '면/콩국수': 61, '무침/꽈리고추무침': 62, '무침/도라지무침': 63, '무침/도토리묵': 64, '무침/잡채': 65, '무침/콩나물무침': 66, '무침/홍어무침': 67, '무침/회무침': 68, '밥/김밥': 69, '밥/김치볶음밥': 70, '밥/누룽지': 71, '밥/비빔밥': 72, '밥/새우볶음밥': 73, '밥/알밥': 74, '밥/유부초밥': 75, '밥/잡곡밥': 76, '밥/주먹밥': 77, '볶음/감자채볶음': 78, '볶음/건새우볶음': 79, '볶음/고추장진미채볶음': 80, '볶음/두부김치': 81, '볶음/떡볶이': 82, '볶음/라볶이': 83, '볶음/멸치볶음': 84, '볶음/소세지볶음': 85, '볶음/어묵볶음': 86, '볶음/오징어채볶음': 87, '볶음/제육볶음': 88, '볶음/주꾸미볶음': 89, '쌈/보쌈': 90, '음청류/수정과': 91, '음청류/식혜': 92, '장/간장게장': 93, '장/양념게장': 94, '장아찌/깻잎장아찌': 95, '적/떡꼬치': 96, '전/감자전': 97, '전/계란말이': 98, '전/계란후라이': 99, '전/김치전': 100, '전/동그랑땡': 101, '전/생선전': 102, '전/파전': 103, '전/호박전': 104, '전골/곱창전골': 105, '조림/갈치조림': 106, '조림/감자조림': 107, '조림/고등어조림': 108, '조림/꽁치조림': 109, '조림/두부조림': 110, '조림/땅콩조림': 111, '조림/메추리알장조림': 112, '조림/연근조림': 113, '조림/우엉조림': 114, '조림/장조림': 115, '조림/코다리조림': 116, '죽/전복죽': 117, '죽/호박죽': 118, '찌개/김치찌개': 119, '찌개/닭계장': 120, '찌개/동태찌개': 121, '찌개/된장찌개': 122, '찌개/순두부찌개': 123, '찜/갈비찜': 124, '찜/계란찜': 125, '찜/김치찜': 126, '찜/꼬막찜': 127, '찜/닭볶음탕': 128, '찜/수육': 129, '찜/순대': 130, '찜/족발': 131, '찜/찜닭': 132, '찜/해물찜': 133, '탕/갈비탕': 134, '탕/감자탕': 135, '탕/곰탕_설렁탕': 136, '탕/매운탕': 137, '탕/삼계탕': 138, '탕/추어탕': 139, '튀김/고추튀김': 140, '튀김/새우튀김': 141, '튀김/오징어튀김': 142, '한과/약과': 143, '한과/약식': 144, '한과/한과': 145, '해물/멍게': 146, '해물/산낙지': 147, '회/물회': 148, '회/육회': 149}\n",
    "idx2class = {v:k.split('/')[-1] for k,v in class2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_large_id(s_id) :\n",
    "  '''\n",
    "  input : small_id\n",
    "  output : large_id\n",
    "  '''\n",
    "  if 0<=s_id and s_id<=13 :\n",
    "    return 0\n",
    "  elif 14<=s_id and s_id<=21 : \n",
    "    return 1\n",
    "  elif 22<=s_id and s_id<=28 :\n",
    "    return 2\n",
    "  elif 29<=s_id and s_id<=39 :\n",
    "    return 3\n",
    "  elif 40<=s_id and s_id<=45 :\n",
    "    return 4\n",
    "  elif 46<=s_id and s_id<=48 :\n",
    "    return 5\n",
    "  elif s_id == 49 : \n",
    "    return 6\n",
    "  elif 50<=s_id and s_id<=61 :\n",
    "    return 7\n",
    "  elif 62<=s_id and s_id<=68 :\n",
    "    return 8\n",
    "  elif 69<=s_id and s_id<=77 :\n",
    "    return 9\n",
    "  elif 78<=s_id and s_id<=89 :\n",
    "    return 10\n",
    "  elif s_id==90 :\n",
    "    return 11\n",
    "  elif 91<=s_id and s_id<=92 : \n",
    "    return 12\n",
    "  elif 93<=s_id and s_id<=94 :\n",
    "    return 13\n",
    "  elif s_id==95 :\n",
    "    return 14\n",
    "  elif s_id==96 :\n",
    "    return 15\n",
    "  elif 97<=s_id and s_id<=104 :\n",
    "    return 16\n",
    "  elif s_id==105 :\n",
    "    return 17\n",
    "  elif 106<=s_id and s_id<=116 :\n",
    "    return 18\n",
    "  elif 117<=s_id and s_id<=118 :\n",
    "    return 19\n",
    "  elif 119<=s_id and s_id<=123 :\n",
    "    return 20\n",
    "  elif 124<=s_id and s_id<=133 :\n",
    "    return 21\n",
    "  elif 134<=s_id and s_id<=139 :\n",
    "    return 22\n",
    "  elif 140<=s_id and s_id<=142 :\n",
    "    return 23\n",
    "  elif 143<=s_id and s_id<=145 :\n",
    "    return 24\n",
    "  elif 146<=s_id and s_id<=147 :\n",
    "    return 25\n",
    "  else :\n",
    "    return 26\n",
    "  return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_id = dict()\n",
    "l = [0,14,22,29,40,46,49,50,62,69,78,90,91,93,95,96,97,105,106,117,119,124,134,140,143,146,148]\n",
    "for idx, i in enumerate(large2idx) :\n",
    "  correct_id[i] = l[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터가 저장된 pkl path를 로드하고\n",
    "# target_large_category 에 맞는 데이터만 뽑는다\n",
    "class Food_DataSet(torch.utils.data.Dataset) : \n",
    "    def __init__(self, args, is_validated) :\n",
    "        self.args = args\n",
    "        if is_validated == True:\n",
    "          self.args.mode = 'val'\n",
    "          data_path = args.val_path\n",
    "        else :\n",
    "          data_path = args.data_path\n",
    "        \n",
    "        large_category = large2idx[args.category]\n",
    "        self.data = []\n",
    "\n",
    "        with open(data_path, 'rb') as f:\n",
    "          total_data = pickle.load(f)\n",
    "          for d, i in total_data : \n",
    "            idx = convert_to_large_id(i)\n",
    "            if idx != large_category :\n",
    "              continue\n",
    "            sub = l[idx]\n",
    "            self.data.append((d, i - sub))\n",
    "        \n",
    "    def __len__(self) : \n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, i) :\n",
    "        return self.data[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터가 저장된 pkl path를 로드하고\n",
    "# target_large_category 에 맞는 데이터만 뽑는다\n",
    "class Food_DataSet(torch.utils.data.Dataset) : \n",
    "    def __init__(self, args, is_validated) :\n",
    "        self.args = args\n",
    "        if is_validated == True:\n",
    "          self.args.mode = 'val'\n",
    "          data_path = args.val_path\n",
    "        else :\n",
    "          data_path = args.data_path\n",
    "        \n",
    "        large_category = large2idx[args.category]\n",
    "        self.data = []\n",
    "\n",
    "        with open(data_path, 'rb') as f:\n",
    "          total_data = pickle.load(f)\n",
    "          for d, i in total_data : \n",
    "            idx = convert_to_large_id(i)\n",
    "            if idx != large_category :\n",
    "              continue\n",
    "            sub = l[idx]\n",
    "            self.data.append((d, i - sub))\n",
    "        \n",
    "    def __len__(self) : \n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, i) :\n",
    "        return self.data[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Food_DataLoader(object) :\n",
    "    def __init__(self, args, is_validated=False, df=None):\n",
    "        super().__init__()\n",
    "\n",
    "        if df is None:\n",
    "            dataset = Food_DataSet(args, is_validated)\n",
    "        else:\n",
    "            dataset = INFERENCE_Dataset(args, df)\n",
    "\n",
    "        if args.mode == 'train':\n",
    "            self.data_loader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
    "        else:\n",
    "            self.data_loader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size)\n",
    "        \n",
    "        self.data_iter = self.data_loader.__iter__()\n",
    "    \n",
    "    def next_batch(self):\n",
    "        try:\n",
    "            batch = self.data_iter.__next__()\n",
    "        except StopIteration:\n",
    "            self.data_iter = self.data_loader.__iter__()\n",
    "            batch = self.data_iter.__next__()\n",
    "        \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=6, stride=2, padding=2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=6, stride=2, padding=2)\n",
    "        self.fc1 = nn.Linear(512, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, args.n_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.reshape(x.size(0), -1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82108\\Downloads\\Data\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer\n",
    "class Trainer():\n",
    "  def __init__(self, args):\n",
    "    self.args = args\n",
    "    \n",
    "    if self.args.mode != 'inference':\n",
    "        self.loader = Food_DataLoader(args, is_validated=False)\n",
    "    self.model = CNN(args)\n",
    "    if self.args.device == 'cuda':\n",
    "        self.model.cuda()\n",
    "\n",
    "    self.criterion = nn.CrossEntropyLoss()\n",
    "    self.optim = torch.optim.Adam(self.model.parameters())\n",
    "      \n",
    "  def train(self):\n",
    "    # train\n",
    "    total_step = len(self.loader.data_loader)\n",
    "    val_loader = Food_DataLoader(self.args, is_validated=True)\n",
    "    opt_epoch = 1\n",
    "    min_val_loss = 1e9\n",
    "\n",
    "    for epoch in range(self.args.epoch):\n",
    "      self.model.train()\n",
    "      total_loss = 0\n",
    "\n",
    "      for i in range(len(self.loader.data_loader)):\n",
    "          feature, label = self.loader.next_batch()\n",
    "          feature = torch.tensor(feature).to(device=self.args.device, dtype=torch.float)\n",
    "          label = torch.tensor(label).to(device=self.args.device)\n",
    "\n",
    "          pred = self.model(feature)\n",
    "          loss = self.criterion(pred, label)\n",
    "          \n",
    "          self.optim.zero_grad()\n",
    "          loss.backward()\n",
    "          self.optim.step()\n",
    "\n",
    "          total_loss += loss.item()\n",
    "          if (i + 1) % 1000 == 0:\n",
    "              print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, self.args.epoch, i + 1, total_step, loss.item()))\n",
    "\n",
    "      #if (epoch+1)%50 == 0:\n",
    "      val_loss = self.validate(val_loader)\n",
    "      print ('Validation Loss: {:.20f}'.format(val_loss))\n",
    "      if val_loss < min_val_loss:\n",
    "        val_loss = min_val_loss\n",
    "        opt_epoch = epoch+1\n",
    "        \n",
    "      torch.save(self.model.state_dict(), 'bbokum\\\\checkpoint_'+str(epoch+1)+'.pt')\n",
    "\n",
    "    print(\"Best Epoch : {}\".format(opt_epoch))\n",
    "\n",
    "  def validate(self, val_loader):\n",
    "      total_loss = 0\n",
    "      total_step = len(val_loader.data_loader)\n",
    "\n",
    "      self.model.eval()\n",
    "      with torch.no_grad():\n",
    "          for i in range(len(val_loader.data_loader)):\n",
    "              feature, label = val_loader.next_batch()\n",
    "              feature = torch.tensor(feature).to(device=self.args.device, dtype=torch.float)\n",
    "              label = torch.tensor(label).to(device=self.args.device)\n",
    "\n",
    "              pred = self.model(feature)\n",
    "              loss = self.criterion(pred, label)\n",
    "              total_loss += loss.item()\n",
    "\n",
    "              if (i + 1) % 100 == 0:\n",
    "                  print ('Validation Step [{}/{}], Loss: {:.4f}'.format(i + 1, total_step, loss.item()))\n",
    "\n",
    "      return total_loss/total_step\n",
    "\n",
    "\n",
    "  def test(self):\n",
    "      # test\n",
    "      if self.args.device == 'cuda':\n",
    "          self.model.load_state_dict(torch.load(self.args.ckpt))\n",
    "      else:\n",
    "          self.model.load_state_dict(torch.load(self.args.ckpt, map_location=torch.device('cpu')))\n",
    "      \n",
    "      self.model.eval()\n",
    "\n",
    "      pred_list = list()\n",
    "      label_list = list()\n",
    "\n",
    "      with torch.no_grad():\n",
    "          for i in range(len(self.loader.data_loader)):\n",
    "              feature, label = self.loader.next_batch()\n",
    "              feature = torch.tensor(feature).to(device=self.args.device, dtype=torch.float)\n",
    "              label = torch.tensor(label).to(device=self.args.device)\n",
    "\n",
    "              pred = self.model(feature)\n",
    "              pred = F.softmax(pred, dim=1)\n",
    "              pred = torch.argmax(pred, dim=1)\n",
    "              \n",
    "              pred_list.extend(pred.tolist())\n",
    "              label_list.extend(label.tolist())\n",
    "\n",
    "      acc = accuracy_score(label_list, pred_list)\n",
    "      print(acc)\n",
    "  \n",
    "  def inference(self, df):\n",
    "      # inference\n",
    "      if self.args.device == 'cuda':\n",
    "          self.model.load_state_dict(torch.load(self.args.ckpt))\n",
    "      else:\n",
    "          self.model.load_state_dict(torch.load(self.args.ckpt, map_location=torch.device('cpu')))\n",
    "      self.model.eval()\n",
    "      \n",
    "      loader = Food_DataLoader(self.args, df=df)\n",
    "\n",
    "      pred_list = list()\n",
    "      label_list = list()\n",
    "\n",
    "      with torch.no_grad():\n",
    "          for i in range(len(loader.data_loader)):\n",
    "              feature = loader.next_batch()\n",
    "              feature = torch.tensor(feature).to(device=self.args.device, dtype=torch.float)\n",
    "\n",
    "              pred = self.model(feature)\n",
    "              pred = F.softmax(pred, dim=1)\n",
    "              pred = torch.argmax(pred, dim=1)\n",
    "              \n",
    "              pred_list.extend(pred.tolist())\n",
    "\n",
    "      return pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    args = EasyDict({\n",
    "        \"epoch\":100,\n",
    "        \"batch_size\":10,\n",
    "        \"mode\":'train',\n",
    "        \"ckpt\":1,\n",
    "        \"device\":'cuda',\n",
    "        \"category\":'볶음',\n",
    "        \"n_class\":12,\n",
    "        \"data_path\":'train.pkl',\n",
    "        \"val_path\":'val.pkl'\n",
    "    })\n",
    "    return args\n",
    "def get_args2() :\n",
    "  args = EasyDict({\n",
    "      \"mode\":'test',\n",
    "      \"batch_size\":1,\n",
    "      \"ckpt\":'bbokum\\\\checkpoint_final.pt',\n",
    "      \"device\":'cuda',\n",
    "      \"category\":'볶음',\n",
    "      \"n_class\":12,\n",
    "  })\n",
    "  return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82108\\AppData\\Local\\Temp/ipykernel_7016/1961781390.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  feature = torch.tensor(feature).to(device=self.args.device, dtype=torch.float)\n",
      "C:\\Users\\82108\\AppData\\Local\\Temp/ipykernel_7016/1961781390.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(label).to(device=self.args.device)\n",
      "C:\\Users\\82108\\AppData\\Local\\Temp/ipykernel_7016/1961781390.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  feature = torch.tensor(feature).to(device=self.args.device, dtype=torch.float)\n",
      "C:\\Users\\82108\\AppData\\Local\\Temp/ipykernel_7016/1961781390.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(label).to(device=self.args.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Step [100/164], Loss: 2.4973\n",
      "Validation Loss: 2.48468226630513244757\n",
      "Validation Step [100/164], Loss: 2.4985\n",
      "Validation Loss: 2.46616266558809993370\n",
      "Validation Step [100/164], Loss: 2.4142\n",
      "Validation Loss: 2.37509409101997936986\n",
      "Validation Step [100/164], Loss: 2.3114\n",
      "Validation Loss: 2.28048945490906884714\n",
      "Validation Step [100/164], Loss: 2.5310\n",
      "Validation Loss: 2.26188282486869063348\n",
      "Validation Step [100/164], Loss: 2.4446\n",
      "Validation Loss: 2.25216786163609183902\n",
      "Validation Step [100/164], Loss: 2.4818\n",
      "Validation Loss: 2.23921628696162544614\n",
      "Validation Step [100/164], Loss: 2.2206\n",
      "Validation Loss: 2.24123116818869982936\n",
      "Validation Step [100/164], Loss: 2.2648\n",
      "Validation Loss: 2.24069168654883776881\n",
      "Validation Step [100/164], Loss: 2.2193\n",
      "Validation Loss: 2.34920832078631347528\n",
      "Validation Step [100/164], Loss: 2.2440\n",
      "Validation Loss: 2.36519997803176318385\n",
      "Validation Step [100/164], Loss: 2.3351\n",
      "Validation Loss: 2.35265379053790413266\n",
      "Validation Step [100/164], Loss: 2.0737\n",
      "Validation Loss: 2.43519426700545515985\n",
      "Validation Step [100/164], Loss: 2.3927\n",
      "Validation Loss: 2.46033506204442264931\n",
      "Validation Step [100/164], Loss: 2.1713\n",
      "Validation Loss: 2.68724565971188411595\n",
      "Validation Step [100/164], Loss: 2.0996\n",
      "Validation Loss: 2.73675932607999650159\n",
      "Validation Step [100/164], Loss: 2.3458\n",
      "Validation Loss: 2.73527945687131168739\n",
      "Validation Step [100/164], Loss: 2.0461\n",
      "Validation Loss: 2.97514646881964139880\n",
      "Validation Step [100/164], Loss: 2.6122\n",
      "Validation Loss: 2.86830302055289099172\n",
      "Validation Step [100/164], Loss: 2.0371\n",
      "Validation Loss: 3.23127718015414933106\n",
      "Validation Step [100/164], Loss: 2.2725\n",
      "Validation Loss: 3.27642997808572733476\n",
      "Validation Step [100/164], Loss: 2.2017\n",
      "Validation Loss: 3.41505227728587845704\n",
      "Validation Step [100/164], Loss: 2.1874\n",
      "Validation Loss: 3.63430879683029361971\n",
      "Validation Step [100/164], Loss: 2.1736\n",
      "Validation Loss: 3.74801944450634261230\n",
      "Validation Step [100/164], Loss: 3.0074\n",
      "Validation Loss: 4.09054041344945051861\n",
      "Validation Step [100/164], Loss: 2.5546\n",
      "Validation Loss: 3.99579411236251269557\n",
      "Validation Step [100/164], Loss: 2.6801\n",
      "Validation Loss: 4.28224189325076753931\n",
      "Validation Step [100/164], Loss: 2.4959\n",
      "Validation Loss: 4.30689699620735311214\n",
      "Validation Step [100/164], Loss: 2.3146\n",
      "Validation Loss: 4.34702820458063232678\n",
      "Validation Step [100/164], Loss: 3.0290\n",
      "Validation Loss: 4.50686499112989835680\n",
      "Validation Step [100/164], Loss: 2.2726\n",
      "Validation Loss: 4.99781531240881982114\n",
      "Validation Step [100/164], Loss: 2.4031\n",
      "Validation Loss: 4.86384183604542830182\n",
      "Validation Step [100/164], Loss: 2.2909\n",
      "Validation Loss: 4.97315313321788110557\n",
      "Validation Step [100/164], Loss: 3.0314\n",
      "Validation Loss: 4.97735498809232979056\n",
      "Validation Step [100/164], Loss: 2.2927\n",
      "Validation Loss: 5.53021466659336535798\n",
      "Validation Step [100/164], Loss: 2.0396\n",
      "Validation Loss: 5.46334725909116780684\n",
      "Validation Step [100/164], Loss: 2.2792\n",
      "Validation Loss: 5.61033241559819462196\n",
      "Validation Step [100/164], Loss: 3.0162\n",
      "Validation Loss: 6.46452341937437324759\n",
      "Validation Step [100/164], Loss: 3.4677\n",
      "Validation Loss: 5.77944093070379150134\n",
      "Validation Step [100/164], Loss: 2.7321\n",
      "Validation Loss: 6.42209795916952774775\n",
      "Validation Step [100/164], Loss: 2.8745\n",
      "Validation Loss: 6.05071901838953873920\n",
      "Validation Step [100/164], Loss: 4.7728\n",
      "Validation Loss: 6.45252826664505896304\n",
      "Validation Step [100/164], Loss: 3.3575\n",
      "Validation Loss: 6.92653331160545349121\n",
      "Validation Step [100/164], Loss: 2.8859\n",
      "Validation Loss: 6.76887147891812190892\n",
      "Validation Step [100/164], Loss: 2.9297\n",
      "Validation Loss: 7.32169809355968403963\n",
      "Validation Step [100/164], Loss: 3.5965\n",
      "Validation Loss: 6.88481749339801485377\n",
      "Validation Step [100/164], Loss: 2.9214\n",
      "Validation Loss: 6.91391442534400191278\n",
      "Validation Step [100/164], Loss: 4.0288\n",
      "Validation Loss: 7.10277058583934106650\n",
      "Validation Step [100/164], Loss: 3.1516\n",
      "Validation Loss: 7.45102680092904634535\n",
      "Validation Step [100/164], Loss: 3.7325\n",
      "Validation Loss: 7.70307176941778593005\n",
      "Validation Step [100/164], Loss: 4.0371\n",
      "Validation Loss: 7.23085428447258138362\n",
      "Validation Step [100/164], Loss: 4.6845\n",
      "Validation Loss: 7.97633388856562142166\n",
      "Validation Step [100/164], Loss: 2.8776\n",
      "Validation Loss: 7.41117870444204740465\n",
      "Validation Step [100/164], Loss: 3.4422\n",
      "Validation Loss: 7.99332615297015092182\n",
      "Validation Step [100/164], Loss: 4.7693\n",
      "Validation Loss: 7.86990980258802075298\n",
      "Validation Step [100/164], Loss: 3.9692\n",
      "Validation Loss: 8.18627202728899483475\n",
      "Validation Step [100/164], Loss: 4.2028\n",
      "Validation Loss: 8.37376047052988248254\n",
      "Validation Step [100/164], Loss: 4.6254\n",
      "Validation Loss: 7.89029792350966729941\n",
      "Validation Step [100/164], Loss: 4.6924\n",
      "Validation Loss: 8.34174399841122493626\n",
      "Validation Step [100/164], Loss: 6.4301\n",
      "Validation Loss: 8.74111757627347607524\n",
      "Validation Step [100/164], Loss: 4.2142\n",
      "Validation Loss: 9.50071848311075406457\n",
      "Validation Step [100/164], Loss: 4.0035\n",
      "Validation Loss: 9.31233841544244356214\n",
      "Validation Step [100/164], Loss: 5.5514\n",
      "Validation Loss: 10.12903241195329862023\n",
      "Validation Step [100/164], Loss: 6.8077\n",
      "Validation Loss: 9.42906941001008114256\n",
      "Validation Step [100/164], Loss: 5.0164\n",
      "Validation Loss: 8.90879297983355655788\n",
      "Validation Step [100/164], Loss: 5.1977\n",
      "Validation Loss: 8.93646863466355867445\n",
      "Validation Step [100/164], Loss: 5.3741\n",
      "Validation Loss: 10.30601739519979886950\n",
      "Validation Step [100/164], Loss: 6.8937\n",
      "Validation Loss: 10.02607483980132307977\n",
      "Validation Step [100/164], Loss: 5.1055\n",
      "Validation Loss: 10.19165279152916703254\n",
      "Validation Step [100/164], Loss: 5.2176\n",
      "Validation Loss: 9.64506146820580134715\n",
      "Validation Step [100/164], Loss: 7.0755\n",
      "Validation Loss: 9.35269007740951252572\n",
      "Validation Step [100/164], Loss: 6.2974\n",
      "Validation Loss: 9.22271717103516230907\n",
      "Validation Step [100/164], Loss: 4.5115\n",
      "Validation Loss: 9.88989918624482555742\n",
      "Validation Step [100/164], Loss: 4.2730\n",
      "Validation Loss: 9.62918424460946020815\n",
      "Validation Step [100/164], Loss: 6.1297\n",
      "Validation Loss: 10.35466034819440217518\n",
      "Validation Step [100/164], Loss: 6.2549\n",
      "Validation Loss: 9.70500613712682991263\n",
      "Validation Step [100/164], Loss: 5.9629\n",
      "Validation Loss: 10.52534444448424544305\n",
      "Validation Step [100/164], Loss: 6.1924\n",
      "Validation Loss: 10.50308488491104874640\n",
      "Validation Step [100/164], Loss: 6.8766\n",
      "Validation Loss: 10.60938757294561796130\n",
      "Validation Step [100/164], Loss: 5.7828\n",
      "Validation Loss: 10.70113708914779948600\n",
      "Validation Step [100/164], Loss: 7.2488\n",
      "Validation Loss: 10.50730411017813281660\n",
      "Validation Step [100/164], Loss: 6.3014\n",
      "Validation Loss: 11.78549261209441390008\n",
      "Validation Step [100/164], Loss: 5.4429\n",
      "Validation Loss: 11.87921070907174048159\n",
      "Validation Step [100/164], Loss: 5.7489\n",
      "Validation Loss: 10.88966536158468656481\n",
      "Validation Step [100/164], Loss: 4.1600\n",
      "Validation Loss: 11.84836829144780168122\n",
      "Validation Step [100/164], Loss: 5.0255\n",
      "Validation Loss: 11.08488319850549430612\n",
      "Validation Step [100/164], Loss: 7.2544\n",
      "Validation Loss: 11.90119129128572339482\n",
      "Validation Step [100/164], Loss: 7.7196\n",
      "Validation Loss: 11.22111359602067537367\n",
      "Validation Step [100/164], Loss: 5.7509\n",
      "Validation Loss: 12.00953718002249637209\n",
      "Validation Step [100/164], Loss: 6.5683\n",
      "Validation Loss: 11.41849351946900448240\n",
      "Validation Step [100/164], Loss: 7.9615\n",
      "Validation Loss: 11.88148929578501977744\n",
      "Validation Step [100/164], Loss: 6.8041\n",
      "Validation Loss: 12.31138831231652197573\n",
      "Validation Step [100/164], Loss: 7.9806\n",
      "Validation Loss: 12.02299684286117553711\n",
      "Validation Step [100/164], Loss: 6.3337\n",
      "Validation Loss: 12.93325784729748306745\n",
      "Validation Step [100/164], Loss: 6.4310\n",
      "Validation Loss: 11.74322925835121012028\n",
      "Validation Step [100/164], Loss: 6.2856\n",
      "Validation Loss: 11.89237805256029467671\n",
      "Validation Step [100/164], Loss: 7.6773\n",
      "Validation Loss: 13.01232422997311921620\n",
      "Validation Step [100/164], Loss: 6.2123\n",
      "Validation Loss: 12.13450805297711987407\n",
      "Validation Step [100/164], Loss: 6.7507\n",
      "Validation Loss: 11.53818644110749325193\n",
      "Validation Step [100/164], Loss: 7.4673\n",
      "Validation Loss: 12.00194169035771984966\n",
      "Best Epoch : 100\n"
     ]
    }
   ],
   "source": [
    "args = get_args()\n",
    "trainer = Trainer(args)\n",
    "\n",
    "if args.mode == 'train':\n",
    "    trainer.train()\n",
    "else:\n",
    "    trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EasyDict' object has no attribute 'data_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7016/1093478745.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_args2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7016/1961781390.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'inference'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFood_DataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_validated\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'cuda'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7016/1097012260.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, is_validated, df)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m             \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFood_DataSet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_validated\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mINFERENCE_Dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7016/3641604529.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, is_validated)\u001b[0m\n\u001b[0;32m      8\u001b[0m           \u001b[0mdata_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32melse\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m           \u001b[0mdata_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mlarge_category\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlarge2idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategory\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'EasyDict' object has no attribute 'data_path'"
     ]
    }
   ],
   "source": [
    "args = get_args2()\n",
    "trainer = Trainer(args)\n",
    "\n",
    "if args.mode == 'train':\n",
    "    trainer.train()\n",
    "else:\n",
    "    trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f53d89161f620bec7f3dae69540a16a389f21ba1b2ef26af02e3b36c55a2c22f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
