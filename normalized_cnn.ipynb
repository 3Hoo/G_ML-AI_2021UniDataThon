{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 27330,
     "status": "ok",
     "timestamp": 1636187341185,
     "user": {
      "displayName": "‍서형진[ 학부재학 / 컴퓨터학과 ]",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13375404922261589310"
     },
     "user_tz": -540
    },
    "id": "87Cx9xJSiBDr"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "import easydict\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import Binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 442,
     "status": "ok",
     "timestamp": 1636187363832,
     "user": {
      "displayName": "‍서형진[ 학부재학 / 컴퓨터학과 ]",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13375404922261589310"
     },
     "user_tz": -540
    },
    "id": "S7Sl4TE6NeOr",
    "outputId": "5a3d2db6-6505-4448-c8a3-dd08cf6808bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'model',\n",
       " 'test.csv',\n",
       " 'test_binary.pkl',\n",
       " 'train.csv',\n",
       " 'train.pkl',\n",
       " 'train_binary.pkl',\n",
       " 'val.csv',\n",
       " 'val_binary.pkl']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('64x64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YF7ZlrEuf2v5"
   },
   "outputs": [],
   "source": [
    "# class2idx\n",
    "class2idx = {'구이/갈비구이': 0, '구이/갈치구이': 1, '구이/고등어구이': 2, '구이/곱창구이': 3, '구이/닭갈비': 4, '구이/더덕구이': 5, '구이/떡갈비': 6, '구이/불고기': 7, '구이/삼겹살': 8, '구이/장어구이': 9, '구이/조개구이': 10, '구이/조기구이': 11, '구이/황태구이': 12, '구이/훈제오리': 13, '국/계란국': 14, '국/떡국_만두국': 15, '국/무국': 16, '국/미역국': 17, '국/북엇국': 18, '국/시래기국': 19, '국/육개장': 20, '국/콩나물국': 21, '기타/과메기': 22, '기타/양념치킨': 23, '기타/젓갈': 24, '기타/콩자반': 25, '기타/편육': 26, '기타/피자': 27, '기타/후라이드치킨': 28, '김치/갓김치': 29, '김치/깍두기': 30, '김치/나박김치': 31, '김치/무생채': 32, '김치/배추김치': 33, '김치/백김치': 34, '김치/부추김치': 35, '김치/열무김치': 36, '김치/오이소박이': 37, '김치/총각김치': 38, '김치/파김치': 39, '나물/가지볶음': 40, '나물/고사리나물': 41, '나물/미역줄기볶음': 42, '나물/숙주나물': 43, '나물/시금치나물': 44, '나물/애호박볶음': 45, '떡/경단': 46, '떡/꿀떡': 47, '떡/송편': 48, '만두/만두': 49, '면/라면': 50, '면/막국수': 51, '면/물냉면': 52, '면/비빔냉면': 53, '면/수제비': 54, '면/열무국수': 55, '면/잔치국수': 56, '면/짜장면': 57, '면/짬뽕': 58, '면/쫄면': 59, '면/칼국수': 60, '면/콩국수': 61, '무침/꽈리고추무침': 62, '무침/도라지무침': 63, '무침/도토리묵': 64, '무침/잡채': 65, '무침/콩나물무침': 66, '무침/홍어무침': 67, '무침/회무침': 68, '밥/김밥': 69, '밥/김치볶음밥': 70, '밥/누룽지': 71, '밥/비빔밥': 72, '밥/새우볶음밥': 73, '밥/알밥': 74, '밥/유부초밥': 75, '밥/잡곡밥': 76, '밥/주먹밥': 77, '볶음/감자채볶음': 78, '볶음/건새우볶음': 79, '볶음/고추장진미채볶음': 80, '볶음/두부김치': 81, '볶음/떡볶이': 82, '볶음/라볶이': 83, '볶음/멸치볶음': 84, '볶음/소세지볶음': 85, '볶음/어묵볶음': 86, '볶음/오징어채볶음': 87, '볶음/제육볶음': 88, '볶음/주꾸미볶음': 89, '쌈/보쌈': 90, '음청류/수정과': 91, '음청류/식혜': 92, '장/간장게장': 93, '장/양념게장': 94, '장아찌/깻잎장아찌': 95, '적/떡꼬치': 96, '전/감자전': 97, '전/계란말이': 98, '전/계란후라이': 99, '전/김치전': 100, '전/동그랑땡': 101, '전/생선전': 102, '전/파전': 103, '전/호박전': 104, '전골/곱창전골': 105, '조림/갈치조림': 106, '조림/감자조림': 107, '조림/고등어조림': 108, '조림/꽁치조림': 109, '조림/두부조림': 110, '조림/땅콩조림': 111, '조림/메추리알장조림': 112, '조림/연근조림': 113, '조림/우엉조림': 114, '조림/장조림': 115, '조림/코다리조림': 116, '죽/전복죽': 117, '죽/호박죽': 118, '찌개/김치찌개': 119, '찌개/닭계장': 120, '찌개/동태찌개': 121, '찌개/된장찌개': 122, '찌개/순두부찌개': 123, '찜/갈비찜': 124, '찜/계란찜': 125, '찜/김치찜': 126, '찜/꼬막찜': 127, '찜/닭볶음탕': 128, '찜/수육': 129, '찜/순대': 130, '찜/족발': 131, '찜/찜닭': 132, '찜/해물찜': 133, '탕/갈비탕': 134, '탕/감자탕': 135, '탕/곰탕_설렁탕': 136, '탕/매운탕': 137, '탕/삼계탕': 138, '탕/추어탕': 139, '튀김/고추튀김': 140, '튀김/새우튀김': 141, '튀김/오징어튀김': 142, '한과/약과': 143, '한과/약식': 144, '한과/한과': 145, '해물/멍게': 146, '해물/산낙지': 147, '회/물회': 148, '회/육회': 149}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "M8E0pCjdWORg"
   },
   "outputs": [],
   "source": [
    "# model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_class=150):\n",
    "        super(CNN, self).__init__()\n",
    "        # (64,64,1) => (32,32,16) => (16,16,16) => (8,8,32) => (4,4,64)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=6, stride=2, padding=2),\n",
    "            nn.Dropout(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=1, stride=1))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=6, stride=2, padding=2),\n",
    "            nn.Dropout(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=1, stride=1))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=6, stride=2, padding=2),\n",
    "            nn.Dropout(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=1, stride=1))\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=6, stride=2, padding=2),\n",
    "            nn.Dropout(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=1, stride=1))\n",
    "        self.fc = nn.Linear(1024, 150)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(1, x.shape)\n",
    "        out = self.layer1(x.unsqueeze(1))\n",
    "        #print(2, out.shape)\n",
    "        out = self.layer2(out)\n",
    "        #print(3, out.shape)\n",
    "        out = self.layer3(out)\n",
    "        #print(4, out.shape)\n",
    "        out = self.layer4(out)\n",
    "        #print(5, out.shape)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4sV6yxEiSxZp"
   },
   "outputs": [],
   "source": [
    "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "           'resnet152']\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth'\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, in_channels=1):\n",
    "        self.inplanes = 16\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 16, kernel_size=7, stride=1, padding=3,\n",
    "                               bias=False) # ori : stride = 2\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 16, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 128, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(1, stride=1)\n",
    "        self.fc = nn.Linear(128 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def resnet18(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet34(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet50(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet101(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet152(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-152 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "QOeE3graS0_G"
   },
   "outputs": [],
   "source": [
    "class background_resnet(nn.Module):\n",
    "    def __init__(self, embedding_size, num_classes, backbone='resnet18'):\n",
    "        super(background_resnet, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        # copying modules from pretrained models\n",
    "        if backbone == 'resnet50':\n",
    "            self.pretrained = resnet50(pretrained=False)\n",
    "        elif backbone == 'resnet101':\n",
    "            self.pretrained = resnet101(pretrained=False)\n",
    "        elif backbone == 'resnet152':\n",
    "            self.pretrained = resnet152(pretrained=False)\n",
    "        elif backbone == 'resnet18':\n",
    "            self.pretrained = resnet18(pretrained=False)\n",
    "        elif backbone == 'resnet34':\n",
    "            self.pretrained = resnet34(pretrained=False)\n",
    "        else:\n",
    "            raise RuntimeError('unknown backbone: {}'.format(backbone))\n",
    "            \n",
    "        self.fc0 = nn.Linear(128, embedding_size)\n",
    "        self.bn0 = nn.BatchNorm1d(embedding_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.last = nn.Linear(embedding_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input x: minibatch x 1 x 40 x 40\n",
    "        x = self.pretrained.conv1(x.unsqueeze(1))\n",
    "        x = self.pretrained.bn1(x)\n",
    "        x = self.pretrained.relu(x)\n",
    "        \n",
    "        x = self.pretrained.layer1(x)\n",
    "        x = self.pretrained.layer2(x)\n",
    "        x = self.pretrained.layer3(x)\n",
    "        x = self.pretrained.layer4(x)\n",
    "        \n",
    "        out = F.adaptive_avg_pool2d(x,1) # [batch, 128, 1, 1]\n",
    "        out = torch.squeeze(out) # [batch, n_embed]\n",
    "        # flatten the out so that the fully connected layer can be connected from here\n",
    "        out = out.view(x.size(0), -1) # (n_batch, n_embed)\n",
    "        spk_embedding = self.fc0(out)\n",
    "        out = F.relu(self.bn0(spk_embedding)) # [batch, n_embed]\n",
    "        out = self.last(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "wPbu0qsRWOUY"
   },
   "outputs": [],
   "source": [
    "# trainer\n",
    "class Trainer():\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.loader = IMG_Loader(args)\n",
    "        #self.model = CNN(args)\n",
    "        self.model = background_resnet(embedding_size=256, num_classes=150)\n",
    "        if self.args.device == 'cuda':\n",
    "          self.model.cuda()\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optim = torch.optim.Adam(self.model.parameters())\n",
    "        \n",
    "    def train(self):\n",
    "        # train\n",
    "        total_step = len(self.loader.data_loader)\n",
    "        val_loader = IMG_Loader(self.args, is_validated=True)\n",
    "        opt_epoch = 1\n",
    "        min_val_loss = 1e9\n",
    "\n",
    "        print(total_step)\n",
    "        for epoch in range(self.args.epoch):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            for i in range(len(self.loader.data_loader)):\n",
    "                feature, label = self.loader.next_batch()\n",
    "                feature = torch.tensor(feature).to(device=self.args.device, dtype=torch.float)\n",
    "                label = torch.tensor(label).to(device=self.args.device)\n",
    "\n",
    "                pred = self.model(feature)\n",
    "                loss = self.criterion(pred, label)\n",
    "\n",
    "                self.optim.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optim.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                if (i + 1) % 100 == 0:\n",
    "                    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, self.args.epoch, i + 1, total_step, loss.item()))\n",
    "\n",
    "            if (epoch+1)%50 == 0:\n",
    "                val_loss = self.validate(val_loader)\n",
    "                print ('Validation Loss: {:.20f}'.format(val_loss))\n",
    "                if val_loss < min_val_loss:\n",
    "                  val_loss = min_val_loss\n",
    "                  opt_epoch = epoch+1\n",
    "   \n",
    "            torch.save(self.model.state_dict(), '64x64\\\\model\\\\checkpoint2_'+str(epoch+1)+'.pt')\n",
    "\n",
    "    def validate(self, val_loader):\n",
    "        total_loss = 0\n",
    "        total_step = len(val_loader.data_loader)\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(val_loader.data_loader)):\n",
    "                feature, label = val_loader.next_batch()\n",
    "                feature = torch.tensor(feature).to(device=self.args.device, dtype=torch.float)\n",
    "                label = torch.tensor(label).to(device=self.args.device)\n",
    "\n",
    "                pred = self.model(feature)\n",
    "                loss = self.criterion(pred, label)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                if (i + 1) % 1000 == 0:\n",
    "                    print ('Validation Step [{}/{}], Loss: {:.4f}'.format(i + 1, total_step, loss.item()))\n",
    "\n",
    "        return total_loss/total_step\n",
    "\n",
    "\n",
    "    def test(self):\n",
    "        # test\n",
    "        #self.model.load_state_dict(torch.load(self.args.ckpt))\n",
    "        self.model.load_state_dict(torch.load('64x64\\\\model\\\\checkpoint_8.pt', map_location=torch.device('cpu')))\n",
    "        self.model.eval()\n",
    "\n",
    "        pred_list = list()\n",
    "        label_list = list()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(self.loader.data_loader)):\n",
    "                feature, label = self.loader.next_batch()\n",
    "                feature = torch.tensor(feature).to(device=self.args.device, dtype=torch.float)\n",
    "                label = torch.tensor(label).to(device=self.args.device)\n",
    "\n",
    "                pred = self.model(feature)\n",
    "                pred = F.softmax(pred, dim=1)\n",
    "                pred = torch.argmax(pred, dim=1)\n",
    "                \n",
    "                pred_list.extend(pred.tolist())\n",
    "                label_list.extend(label.tolist())\n",
    "\n",
    "        acc = accuracy_score(label_list, pred_list)\n",
    "        print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ozdaismMWONT"
   },
   "outputs": [],
   "source": [
    "# dataloader\n",
    "class IMG_Dataset(Dataset):\n",
    "    def __init__(self, args, is_validated=False):\n",
    "        self.args = args\n",
    "        if is_validated == True:\n",
    "          self.args.mode = 'val'\n",
    "\n",
    "        data_path = os.path.join('64x64', args.mode+'_binary') + '.pkl'\n",
    "\n",
    "        if os.path.isfile(data_path):\n",
    "            with open(data_path, 'rb') as f:\n",
    "                self.data = pickle.load(f)\n",
    "        else:\n",
    "            df = pd.read_csv(os.path.join('64x64', args.mode) + '.csv', names=range(0,4097))\n",
    "\n",
    "            self.data = []\n",
    "\n",
    "            y = df[0]\n",
    "            x = df.drop(labels=[0], axis=1)\n",
    "            x = normalize(x, axis=0)\n",
    "            x = x.reshape(-1,1,64,64)\n",
    "\n",
    "            for i in range(len(y)):\n",
    "                self.data.append((x[i], y[i]))\n",
    "                if(i % 1000 == 0):\n",
    "                    print(i)\n",
    "            print(len(self.data))\n",
    "            with open(data_path, 'wb') as f:\n",
    "                pickle.dump(self.data, f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i]\n",
    "\n",
    "\n",
    "class IMG_Loader(object):\n",
    "    def __init__(self, args, is_validated=False):\n",
    "        super().__init__()\n",
    "\n",
    "        dataset = IMG_Dataset(args)\n",
    "\n",
    "        if args.mode == 'train':\n",
    "            self.data_loader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
    "        else:\n",
    "            self.data_loader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size)\n",
    "        \n",
    "        self.data_iter = self.data_loader.__iter__()\n",
    "    \n",
    "    def next_batch(self):\n",
    "        try:\n",
    "            batch = self.data_iter.__next__()\n",
    "        except StopIteration:\n",
    "            self.data_iter = self.data_loader.__iter__()\n",
    "            batch = self.data_iter.__next__()\n",
    "        \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "sB-5UXylax07"
   },
   "outputs": [],
   "source": [
    "# args\n",
    "def get_args():\n",
    "    args = easydict.EasyDict({\n",
    "        \"epoch\": 100,\n",
    "        \"batch_size\": 32,\n",
    "        \"mode\": 'train',\n",
    "        \"ckpt\": 1,\n",
    "        \"device\": 'cuda'\n",
    "    })\n",
    "    return args\n",
    "\n",
    "def get_args2() :\n",
    "  args = EasyDict({\n",
    "      \"mode\":'test',\n",
    "      \"batch_size\":1,\n",
    "      \"ckpt\":'64x64\\\\model\\\\checkpoint_final.pt',\n",
    "      \"device\":'cuda',\n",
    "      \"n_class\":150,\n",
    "  })\n",
    "  return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GZX9F7KSWOZF",
    "outputId": "2b8334cd-8556-48f5-f812-50463530ca96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82108\\AppData\\Local\\Temp/ipykernel_2480/533309440.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  feature = torch.tensor(feature).to(device=self.args.device, dtype=torch.float)\n",
      "C:\\Users\\82108\\AppData\\Local\\Temp/ipykernel_2480/533309440.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(label).to(device=self.args.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [100/2266], Loss: 4.7666\n",
      "Epoch [1/100], Step [200/2266], Loss: 4.7768\n",
      "Epoch [1/100], Step [300/2266], Loss: 4.7121\n",
      "Epoch [1/100], Step [400/2266], Loss: 4.5109\n",
      "Epoch [1/100], Step [500/2266], Loss: 4.6344\n",
      "Epoch [1/100], Step [600/2266], Loss: 4.7143\n",
      "Epoch [1/100], Step [700/2266], Loss: 4.5015\n",
      "Epoch [1/100], Step [800/2266], Loss: 4.8101\n",
      "Epoch [1/100], Step [900/2266], Loss: 4.6599\n",
      "Epoch [1/100], Step [1000/2266], Loss: 4.6944\n",
      "Epoch [1/100], Step [1100/2266], Loss: 4.4226\n",
      "Epoch [1/100], Step [1200/2266], Loss: 4.7035\n",
      "Epoch [1/100], Step [1300/2266], Loss: 4.5225\n",
      "Epoch [1/100], Step [1400/2266], Loss: 4.3920\n",
      "Epoch [1/100], Step [1500/2266], Loss: 4.4117\n",
      "Epoch [1/100], Step [1600/2266], Loss: 4.5273\n",
      "Epoch [1/100], Step [1700/2266], Loss: 4.3812\n",
      "Epoch [1/100], Step [1800/2266], Loss: 4.3937\n",
      "Epoch [1/100], Step [1900/2266], Loss: 4.3434\n",
      "Epoch [1/100], Step [2000/2266], Loss: 4.5518\n",
      "Epoch [1/100], Step [2100/2266], Loss: 4.3696\n",
      "Epoch [1/100], Step [2200/2266], Loss: 4.5654\n",
      "Epoch [2/100], Step [100/2266], Loss: 4.9090\n",
      "Epoch [2/100], Step [200/2266], Loss: 4.0805\n",
      "Epoch [2/100], Step [300/2266], Loss: 4.1592\n",
      "Epoch [2/100], Step [400/2266], Loss: 4.4141\n",
      "Epoch [2/100], Step [500/2266], Loss: 4.1435\n",
      "Epoch [2/100], Step [600/2266], Loss: 4.2027\n",
      "Epoch [2/100], Step [700/2266], Loss: 3.9124\n",
      "Epoch [2/100], Step [800/2266], Loss: 4.3414\n",
      "Epoch [2/100], Step [900/2266], Loss: 4.4950\n",
      "Epoch [2/100], Step [1000/2266], Loss: 4.3186\n",
      "Epoch [2/100], Step [1100/2266], Loss: 4.2022\n",
      "Epoch [2/100], Step [1200/2266], Loss: 4.2099\n",
      "Epoch [2/100], Step [1300/2266], Loss: 3.7559\n",
      "Epoch [2/100], Step [1400/2266], Loss: 3.8812\n",
      "Epoch [2/100], Step [1500/2266], Loss: 3.8077\n",
      "Epoch [2/100], Step [1600/2266], Loss: 4.0886\n",
      "Epoch [2/100], Step [1700/2266], Loss: 4.2623\n",
      "Epoch [2/100], Step [1800/2266], Loss: 4.0118\n",
      "Epoch [2/100], Step [1900/2266], Loss: 4.0229\n",
      "Epoch [2/100], Step [2000/2266], Loss: 4.4258\n",
      "Epoch [2/100], Step [2100/2266], Loss: 4.4279\n",
      "Epoch [2/100], Step [2200/2266], Loss: 4.0062\n",
      "Epoch [3/100], Step [100/2266], Loss: 3.7461\n",
      "Epoch [3/100], Step [200/2266], Loss: 4.1187\n",
      "Epoch [3/100], Step [300/2266], Loss: 3.8130\n",
      "Epoch [3/100], Step [400/2266], Loss: 4.3405\n",
      "Epoch [3/100], Step [500/2266], Loss: 4.1907\n",
      "Epoch [3/100], Step [600/2266], Loss: 4.1678\n",
      "Epoch [3/100], Step [700/2266], Loss: 4.1201\n",
      "Epoch [3/100], Step [800/2266], Loss: 3.6552\n",
      "Epoch [3/100], Step [900/2266], Loss: 4.4894\n",
      "Epoch [3/100], Step [1000/2266], Loss: 4.2603\n",
      "Epoch [3/100], Step [1100/2266], Loss: 4.3743\n",
      "Epoch [3/100], Step [1200/2266], Loss: 4.0420\n",
      "Epoch [3/100], Step [1300/2266], Loss: 3.9418\n",
      "Epoch [3/100], Step [1400/2266], Loss: 4.1467\n",
      "Epoch [3/100], Step [1500/2266], Loss: 4.1682\n",
      "Epoch [3/100], Step [1600/2266], Loss: 4.0351\n",
      "Epoch [3/100], Step [1700/2266], Loss: 4.2555\n",
      "Epoch [3/100], Step [1800/2266], Loss: 3.7228\n",
      "Epoch [3/100], Step [1900/2266], Loss: 4.2808\n",
      "Epoch [3/100], Step [2000/2266], Loss: 3.8651\n",
      "Epoch [3/100], Step [2100/2266], Loss: 4.1576\n",
      "Epoch [3/100], Step [2200/2266], Loss: 3.8666\n",
      "Epoch [4/100], Step [100/2266], Loss: 3.9714\n",
      "Epoch [4/100], Step [200/2266], Loss: 3.7419\n",
      "Epoch [4/100], Step [300/2266], Loss: 4.0586\n",
      "Epoch [4/100], Step [400/2266], Loss: 3.9460\n",
      "Epoch [4/100], Step [500/2266], Loss: 3.8698\n",
      "Epoch [4/100], Step [600/2266], Loss: 4.0483\n",
      "Epoch [4/100], Step [700/2266], Loss: 3.3811\n",
      "Epoch [4/100], Step [800/2266], Loss: 3.9046\n",
      "Epoch [4/100], Step [900/2266], Loss: 3.4879\n",
      "Epoch [4/100], Step [1000/2266], Loss: 4.3894\n",
      "Epoch [4/100], Step [1100/2266], Loss: 3.7744\n",
      "Epoch [4/100], Step [1200/2266], Loss: 3.8443\n",
      "Epoch [4/100], Step [1300/2266], Loss: 4.0219\n",
      "Epoch [4/100], Step [1400/2266], Loss: 3.7613\n",
      "Epoch [4/100], Step [1500/2266], Loss: 4.0209\n",
      "Epoch [4/100], Step [1600/2266], Loss: 3.2743\n",
      "Epoch [4/100], Step [1700/2266], Loss: 3.7530\n",
      "Epoch [4/100], Step [1800/2266], Loss: 4.1271\n",
      "Epoch [4/100], Step [1900/2266], Loss: 3.9074\n",
      "Epoch [4/100], Step [2000/2266], Loss: 3.5951\n",
      "Epoch [4/100], Step [2100/2266], Loss: 3.8288\n",
      "Epoch [4/100], Step [2200/2266], Loss: 4.4822\n",
      "Epoch [5/100], Step [100/2266], Loss: 3.8817\n",
      "Epoch [5/100], Step [200/2266], Loss: 4.0134\n",
      "Epoch [5/100], Step [300/2266], Loss: 3.7844\n",
      "Epoch [5/100], Step [400/2266], Loss: 4.0079\n",
      "Epoch [5/100], Step [500/2266], Loss: 3.7680\n",
      "Epoch [5/100], Step [600/2266], Loss: 3.5435\n",
      "Epoch [5/100], Step [700/2266], Loss: 3.5664\n",
      "Epoch [5/100], Step [800/2266], Loss: 3.5175\n",
      "Epoch [5/100], Step [900/2266], Loss: 4.2266\n",
      "Epoch [5/100], Step [1000/2266], Loss: 3.6188\n",
      "Epoch [5/100], Step [1100/2266], Loss: 3.4956\n",
      "Epoch [5/100], Step [1200/2266], Loss: 3.8630\n",
      "Epoch [5/100], Step [1300/2266], Loss: 4.0992\n",
      "Epoch [5/100], Step [1400/2266], Loss: 3.8487\n",
      "Epoch [5/100], Step [1500/2266], Loss: 3.7179\n",
      "Epoch [5/100], Step [1600/2266], Loss: 3.3017\n",
      "Epoch [5/100], Step [1700/2266], Loss: 4.0001\n",
      "Epoch [5/100], Step [1800/2266], Loss: 3.9307\n",
      "Epoch [5/100], Step [1900/2266], Loss: 3.7459\n",
      "Epoch [5/100], Step [2000/2266], Loss: 3.7414\n",
      "Epoch [5/100], Step [2100/2266], Loss: 3.7741\n",
      "Epoch [5/100], Step [2200/2266], Loss: 3.3813\n",
      "Epoch [6/100], Step [100/2266], Loss: 3.9629\n",
      "Epoch [6/100], Step [200/2266], Loss: 3.4352\n",
      "Epoch [6/100], Step [300/2266], Loss: 3.5329\n",
      "Epoch [6/100], Step [400/2266], Loss: 3.3198\n",
      "Epoch [6/100], Step [500/2266], Loss: 4.0465\n",
      "Epoch [6/100], Step [600/2266], Loss: 3.3125\n",
      "Epoch [6/100], Step [700/2266], Loss: 3.7089\n",
      "Epoch [6/100], Step [800/2266], Loss: 3.7917\n",
      "Epoch [6/100], Step [900/2266], Loss: 3.5298\n",
      "Epoch [6/100], Step [1000/2266], Loss: 3.3649\n",
      "Epoch [6/100], Step [1100/2266], Loss: 3.5513\n",
      "Epoch [6/100], Step [1200/2266], Loss: 3.2378\n",
      "Epoch [6/100], Step [1300/2266], Loss: 3.6751\n",
      "Epoch [6/100], Step [1400/2266], Loss: 4.1519\n",
      "Epoch [6/100], Step [1500/2266], Loss: 4.0103\n",
      "Epoch [6/100], Step [1600/2266], Loss: 3.7607\n",
      "Epoch [6/100], Step [1700/2266], Loss: 3.6748\n",
      "Epoch [6/100], Step [1800/2266], Loss: 3.4587\n",
      "Epoch [6/100], Step [1900/2266], Loss: 3.5907\n",
      "Epoch [6/100], Step [2000/2266], Loss: 3.2682\n",
      "Epoch [6/100], Step [2100/2266], Loss: 3.2930\n",
      "Epoch [6/100], Step [2200/2266], Loss: 3.5855\n",
      "Epoch [7/100], Step [100/2266], Loss: 3.0208\n",
      "Epoch [7/100], Step [200/2266], Loss: 3.6029\n",
      "Epoch [7/100], Step [300/2266], Loss: 3.3226\n",
      "Epoch [7/100], Step [400/2266], Loss: 3.7808\n",
      "Epoch [7/100], Step [500/2266], Loss: 2.9635\n",
      "Epoch [7/100], Step [600/2266], Loss: 3.4596\n",
      "Epoch [7/100], Step [700/2266], Loss: 3.7083\n",
      "Epoch [7/100], Step [800/2266], Loss: 3.3962\n",
      "Epoch [7/100], Step [900/2266], Loss: 3.4613\n",
      "Epoch [7/100], Step [1000/2266], Loss: 3.6971\n",
      "Epoch [7/100], Step [1100/2266], Loss: 3.3294\n",
      "Epoch [7/100], Step [1200/2266], Loss: 3.8802\n",
      "Epoch [7/100], Step [1300/2266], Loss: 3.4486\n",
      "Epoch [7/100], Step [1400/2266], Loss: 3.4080\n",
      "Epoch [7/100], Step [1500/2266], Loss: 3.8310\n",
      "Epoch [7/100], Step [1600/2266], Loss: 3.5159\n",
      "Epoch [7/100], Step [1700/2266], Loss: 3.2923\n",
      "Epoch [7/100], Step [1800/2266], Loss: 4.1319\n",
      "Epoch [7/100], Step [1900/2266], Loss: 3.7080\n",
      "Epoch [7/100], Step [2000/2266], Loss: 3.6136\n",
      "Epoch [7/100], Step [2100/2266], Loss: 3.3233\n",
      "Epoch [7/100], Step [2200/2266], Loss: 3.1641\n",
      "Epoch [8/100], Step [100/2266], Loss: 3.6838\n",
      "Epoch [8/100], Step [200/2266], Loss: 3.3250\n",
      "Epoch [8/100], Step [300/2266], Loss: 3.1895\n",
      "Epoch [8/100], Step [400/2266], Loss: 3.3460\n",
      "Epoch [8/100], Step [500/2266], Loss: 3.2248\n",
      "Epoch [8/100], Step [600/2266], Loss: 3.9119\n",
      "Epoch [8/100], Step [700/2266], Loss: 3.6288\n",
      "Epoch [8/100], Step [800/2266], Loss: 3.2590\n",
      "Epoch [8/100], Step [900/2266], Loss: 3.2195\n",
      "Epoch [8/100], Step [1000/2266], Loss: 3.8568\n",
      "Epoch [8/100], Step [1100/2266], Loss: 3.1798\n",
      "Epoch [8/100], Step [1200/2266], Loss: 3.8577\n",
      "Epoch [8/100], Step [1300/2266], Loss: 3.3494\n",
      "Epoch [8/100], Step [1400/2266], Loss: 3.2540\n",
      "Epoch [8/100], Step [1500/2266], Loss: 3.4544\n",
      "Epoch [8/100], Step [1600/2266], Loss: 3.5156\n",
      "Epoch [8/100], Step [1700/2266], Loss: 3.8438\n",
      "Epoch [8/100], Step [1800/2266], Loss: 3.8254\n",
      "Epoch [8/100], Step [1900/2266], Loss: 3.7171\n",
      "Epoch [8/100], Step [2000/2266], Loss: 2.9698\n",
      "Epoch [8/100], Step [2100/2266], Loss: 3.3830\n",
      "Epoch [8/100], Step [2200/2266], Loss: 2.8928\n",
      "Epoch [9/100], Step [100/2266], Loss: 2.8769\n",
      "Epoch [9/100], Step [200/2266], Loss: 3.2146\n",
      "Epoch [9/100], Step [300/2266], Loss: 3.1781\n",
      "Epoch [9/100], Step [400/2266], Loss: 2.8960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100], Step [500/2266], Loss: 3.0026\n",
      "Epoch [9/100], Step [600/2266], Loss: 3.1506\n",
      "Epoch [9/100], Step [700/2266], Loss: 3.0871\n",
      "Epoch [9/100], Step [800/2266], Loss: 3.0198\n",
      "Epoch [9/100], Step [900/2266], Loss: 3.2335\n",
      "Epoch [9/100], Step [1000/2266], Loss: 3.4775\n",
      "Epoch [9/100], Step [1100/2266], Loss: 3.3326\n",
      "Epoch [9/100], Step [1200/2266], Loss: 3.8422\n",
      "Epoch [9/100], Step [1300/2266], Loss: 3.2291\n",
      "Epoch [9/100], Step [1400/2266], Loss: 3.2352\n",
      "Epoch [9/100], Step [1500/2266], Loss: 3.4084\n",
      "Epoch [9/100], Step [1600/2266], Loss: 3.2242\n",
      "Epoch [9/100], Step [1700/2266], Loss: 3.1586\n",
      "Epoch [9/100], Step [1800/2266], Loss: 3.6126\n",
      "Epoch [9/100], Step [1900/2266], Loss: 3.0731\n",
      "Epoch [9/100], Step [2000/2266], Loss: 3.3533\n",
      "Epoch [9/100], Step [2100/2266], Loss: 3.0942\n",
      "Epoch [9/100], Step [2200/2266], Loss: 3.3031\n",
      "Epoch [10/100], Step [100/2266], Loss: 3.5900\n",
      "Epoch [10/100], Step [200/2266], Loss: 2.9130\n",
      "Epoch [10/100], Step [300/2266], Loss: 3.1084\n",
      "Epoch [10/100], Step [400/2266], Loss: 3.2716\n",
      "Epoch [10/100], Step [500/2266], Loss: 3.3125\n",
      "Epoch [10/100], Step [600/2266], Loss: 2.8482\n",
      "Epoch [10/100], Step [700/2266], Loss: 3.6779\n",
      "Epoch [10/100], Step [800/2266], Loss: 3.4612\n",
      "Epoch [10/100], Step [900/2266], Loss: 3.4813\n",
      "Epoch [10/100], Step [1000/2266], Loss: 2.6840\n",
      "Epoch [10/100], Step [1100/2266], Loss: 2.7656\n",
      "Epoch [10/100], Step [1200/2266], Loss: 2.8622\n",
      "Epoch [10/100], Step [1300/2266], Loss: 2.7713\n",
      "Epoch [10/100], Step [1400/2266], Loss: 3.2569\n",
      "Epoch [10/100], Step [1500/2266], Loss: 3.3393\n",
      "Epoch [10/100], Step [1600/2266], Loss: 3.3640\n",
      "Epoch [10/100], Step [1700/2266], Loss: 3.0857\n",
      "Epoch [10/100], Step [1800/2266], Loss: 3.3982\n",
      "Epoch [10/100], Step [1900/2266], Loss: 3.8573\n",
      "Epoch [10/100], Step [2000/2266], Loss: 3.4240\n",
      "Epoch [10/100], Step [2100/2266], Loss: 3.2691\n",
      "Epoch [10/100], Step [2200/2266], Loss: 2.9250\n",
      "Epoch [11/100], Step [100/2266], Loss: 2.6843\n",
      "Epoch [11/100], Step [200/2266], Loss: 2.5213\n",
      "Epoch [11/100], Step [300/2266], Loss: 2.3682\n",
      "Epoch [11/100], Step [400/2266], Loss: 2.8957\n",
      "Epoch [11/100], Step [500/2266], Loss: 3.2349\n",
      "Epoch [11/100], Step [600/2266], Loss: 2.9551\n",
      "Epoch [11/100], Step [700/2266], Loss: 2.6287\n",
      "Epoch [11/100], Step [800/2266], Loss: 2.3235\n",
      "Epoch [11/100], Step [900/2266], Loss: 3.1429\n",
      "Epoch [11/100], Step [1000/2266], Loss: 2.9425\n",
      "Epoch [11/100], Step [1100/2266], Loss: 3.0017\n",
      "Epoch [11/100], Step [1200/2266], Loss: 3.3121\n",
      "Epoch [11/100], Step [1300/2266], Loss: 2.7854\n",
      "Epoch [11/100], Step [1400/2266], Loss: 2.6899\n",
      "Epoch [11/100], Step [1500/2266], Loss: 2.9978\n",
      "Epoch [11/100], Step [1600/2266], Loss: 3.6228\n",
      "Epoch [11/100], Step [1700/2266], Loss: 2.7935\n",
      "Epoch [11/100], Step [1800/2266], Loss: 2.6183\n",
      "Epoch [11/100], Step [1900/2266], Loss: 2.9149\n",
      "Epoch [11/100], Step [2000/2266], Loss: 3.6487\n",
      "Epoch [11/100], Step [2100/2266], Loss: 2.7826\n",
      "Epoch [11/100], Step [2200/2266], Loss: 2.9157\n",
      "Epoch [12/100], Step [100/2266], Loss: 2.0009\n",
      "Epoch [12/100], Step [200/2266], Loss: 2.4277\n",
      "Epoch [12/100], Step [300/2266], Loss: 2.6519\n",
      "Epoch [12/100], Step [400/2266], Loss: 2.8846\n",
      "Epoch [12/100], Step [500/2266], Loss: 2.4481\n",
      "Epoch [12/100], Step [600/2266], Loss: 2.7491\n",
      "Epoch [12/100], Step [700/2266], Loss: 3.2476\n",
      "Epoch [12/100], Step [800/2266], Loss: 2.6470\n",
      "Epoch [12/100], Step [900/2266], Loss: 2.7749\n",
      "Epoch [12/100], Step [1000/2266], Loss: 2.4872\n",
      "Epoch [12/100], Step [1100/2266], Loss: 3.0848\n",
      "Epoch [12/100], Step [1200/2266], Loss: 2.5198\n",
      "Epoch [12/100], Step [1300/2266], Loss: 2.7314\n",
      "Epoch [12/100], Step [1400/2266], Loss: 2.2970\n",
      "Epoch [12/100], Step [1500/2266], Loss: 3.0954\n",
      "Epoch [12/100], Step [1600/2266], Loss: 2.7252\n",
      "Epoch [12/100], Step [1700/2266], Loss: 2.3108\n",
      "Epoch [12/100], Step [1800/2266], Loss: 3.0988\n",
      "Epoch [12/100], Step [1900/2266], Loss: 2.7208\n",
      "Epoch [12/100], Step [2000/2266], Loss: 2.8707\n",
      "Epoch [12/100], Step [2100/2266], Loss: 2.7553\n",
      "Epoch [12/100], Step [2200/2266], Loss: 2.8963\n",
      "Epoch [13/100], Step [100/2266], Loss: 2.6504\n",
      "Epoch [13/100], Step [200/2266], Loss: 2.1771\n",
      "Epoch [13/100], Step [300/2266], Loss: 2.2716\n",
      "Epoch [13/100], Step [400/2266], Loss: 2.6222\n",
      "Epoch [13/100], Step [500/2266], Loss: 1.8681\n",
      "Epoch [13/100], Step [600/2266], Loss: 2.5869\n",
      "Epoch [13/100], Step [700/2266], Loss: 2.7988\n",
      "Epoch [13/100], Step [800/2266], Loss: 2.3862\n",
      "Epoch [13/100], Step [900/2266], Loss: 2.2967\n",
      "Epoch [13/100], Step [1000/2266], Loss: 2.8918\n",
      "Epoch [13/100], Step [1100/2266], Loss: 3.0925\n",
      "Epoch [13/100], Step [1200/2266], Loss: 2.0331\n",
      "Epoch [13/100], Step [1300/2266], Loss: 2.5248\n",
      "Epoch [13/100], Step [1400/2266], Loss: 2.0519\n",
      "Epoch [13/100], Step [1500/2266], Loss: 2.4234\n",
      "Epoch [13/100], Step [1600/2266], Loss: 2.5406\n",
      "Epoch [13/100], Step [1700/2266], Loss: 2.4799\n",
      "Epoch [13/100], Step [1800/2266], Loss: 2.5143\n",
      "Epoch [13/100], Step [1900/2266], Loss: 2.4128\n",
      "Epoch [13/100], Step [2000/2266], Loss: 3.1586\n",
      "Epoch [13/100], Step [2100/2266], Loss: 2.1238\n",
      "Epoch [13/100], Step [2200/2266], Loss: 2.8078\n",
      "Epoch [14/100], Step [100/2266], Loss: 2.1284\n",
      "Epoch [14/100], Step [200/2266], Loss: 2.0083\n",
      "Epoch [14/100], Step [300/2266], Loss: 2.7000\n",
      "Epoch [14/100], Step [400/2266], Loss: 2.8387\n",
      "Epoch [14/100], Step [500/2266], Loss: 2.3872\n",
      "Epoch [14/100], Step [600/2266], Loss: 2.4796\n",
      "Epoch [14/100], Step [700/2266], Loss: 2.2915\n",
      "Epoch [14/100], Step [800/2266], Loss: 2.4108\n",
      "Epoch [14/100], Step [900/2266], Loss: 3.1139\n",
      "Epoch [14/100], Step [1000/2266], Loss: 2.4394\n",
      "Epoch [14/100], Step [1100/2266], Loss: 2.5282\n",
      "Epoch [14/100], Step [1200/2266], Loss: 1.9299\n",
      "Epoch [14/100], Step [1300/2266], Loss: 2.7395\n",
      "Epoch [14/100], Step [1400/2266], Loss: 2.1918\n",
      "Epoch [14/100], Step [1500/2266], Loss: 2.1462\n",
      "Epoch [14/100], Step [1600/2266], Loss: 2.0395\n",
      "Epoch [14/100], Step [1700/2266], Loss: 2.8638\n",
      "Epoch [14/100], Step [1800/2266], Loss: 2.4000\n",
      "Epoch [14/100], Step [1900/2266], Loss: 2.2722\n",
      "Epoch [14/100], Step [2000/2266], Loss: 1.8919\n",
      "Epoch [14/100], Step [2100/2266], Loss: 2.0504\n",
      "Epoch [14/100], Step [2200/2266], Loss: 2.0160\n",
      "Epoch [15/100], Step [100/2266], Loss: 1.9387\n",
      "Epoch [15/100], Step [200/2266], Loss: 1.6341\n",
      "Epoch [15/100], Step [300/2266], Loss: 1.8836\n",
      "Epoch [15/100], Step [400/2266], Loss: 1.7917\n",
      "Epoch [15/100], Step [500/2266], Loss: 2.3903\n",
      "Epoch [15/100], Step [600/2266], Loss: 2.1067\n",
      "Epoch [15/100], Step [700/2266], Loss: 2.3953\n",
      "Epoch [15/100], Step [800/2266], Loss: 2.2184\n",
      "Epoch [15/100], Step [900/2266], Loss: 2.4985\n",
      "Epoch [15/100], Step [1000/2266], Loss: 1.3464\n",
      "Epoch [15/100], Step [1100/2266], Loss: 2.4277\n",
      "Epoch [15/100], Step [1200/2266], Loss: 2.4235\n",
      "Epoch [15/100], Step [1300/2266], Loss: 2.4419\n",
      "Epoch [15/100], Step [1400/2266], Loss: 2.6721\n",
      "Epoch [15/100], Step [1500/2266], Loss: 2.1100\n",
      "Epoch [15/100], Step [1600/2266], Loss: 2.7920\n",
      "Epoch [15/100], Step [1700/2266], Loss: 2.0903\n",
      "Epoch [15/100], Step [1800/2266], Loss: 1.9582\n",
      "Epoch [15/100], Step [1900/2266], Loss: 2.3935\n",
      "Epoch [15/100], Step [2000/2266], Loss: 2.0839\n",
      "Epoch [15/100], Step [2100/2266], Loss: 2.2272\n",
      "Epoch [15/100], Step [2200/2266], Loss: 2.4732\n",
      "Epoch [16/100], Step [100/2266], Loss: 2.1264\n",
      "Epoch [16/100], Step [200/2266], Loss: 1.4479\n",
      "Epoch [16/100], Step [300/2266], Loss: 1.4521\n",
      "Epoch [16/100], Step [400/2266], Loss: 2.0775\n",
      "Epoch [16/100], Step [500/2266], Loss: 2.2307\n",
      "Epoch [16/100], Step [600/2266], Loss: 2.1024\n",
      "Epoch [16/100], Step [700/2266], Loss: 1.7714\n",
      "Epoch [16/100], Step [800/2266], Loss: 1.9456\n",
      "Epoch [16/100], Step [900/2266], Loss: 2.2562\n",
      "Epoch [16/100], Step [1000/2266], Loss: 2.1889\n",
      "Epoch [16/100], Step [1100/2266], Loss: 1.8939\n",
      "Epoch [16/100], Step [1200/2266], Loss: 1.8374\n",
      "Epoch [16/100], Step [1300/2266], Loss: 1.5613\n",
      "Epoch [16/100], Step [1400/2266], Loss: 2.2663\n",
      "Epoch [16/100], Step [1500/2266], Loss: 2.3481\n",
      "Epoch [16/100], Step [1600/2266], Loss: 2.0654\n",
      "Epoch [16/100], Step [1700/2266], Loss: 2.1655\n",
      "Epoch [16/100], Step [1800/2266], Loss: 1.6700\n",
      "Epoch [16/100], Step [1900/2266], Loss: 2.3588\n",
      "Epoch [16/100], Step [2000/2266], Loss: 1.6117\n",
      "Epoch [16/100], Step [2100/2266], Loss: 2.6564\n",
      "Epoch [16/100], Step [2200/2266], Loss: 1.5025\n",
      "Epoch [17/100], Step [100/2266], Loss: 1.6452\n",
      "Epoch [17/100], Step [200/2266], Loss: 1.3569\n",
      "Epoch [17/100], Step [300/2266], Loss: 1.8803\n",
      "Epoch [17/100], Step [400/2266], Loss: 1.9187\n",
      "Epoch [17/100], Step [500/2266], Loss: 1.7251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/100], Step [600/2266], Loss: 1.3001\n",
      "Epoch [17/100], Step [700/2266], Loss: 1.7149\n",
      "Epoch [17/100], Step [800/2266], Loss: 1.8157\n",
      "Epoch [17/100], Step [900/2266], Loss: 1.9089\n",
      "Epoch [17/100], Step [1000/2266], Loss: 1.5832\n",
      "Epoch [17/100], Step [1100/2266], Loss: 1.7274\n",
      "Epoch [17/100], Step [1200/2266], Loss: 2.4323\n",
      "Epoch [17/100], Step [1300/2266], Loss: 2.0561\n",
      "Epoch [17/100], Step [1400/2266], Loss: 2.0859\n",
      "Epoch [17/100], Step [1500/2266], Loss: 1.3891\n",
      "Epoch [17/100], Step [1600/2266], Loss: 2.4038\n",
      "Epoch [17/100], Step [1700/2266], Loss: 2.0013\n",
      "Epoch [17/100], Step [1800/2266], Loss: 1.9147\n",
      "Epoch [17/100], Step [1900/2266], Loss: 1.7673\n",
      "Epoch [17/100], Step [2000/2266], Loss: 1.6354\n",
      "Epoch [17/100], Step [2100/2266], Loss: 1.9802\n",
      "Epoch [17/100], Step [2200/2266], Loss: 1.7200\n",
      "Epoch [18/100], Step [100/2266], Loss: 1.2580\n",
      "Epoch [18/100], Step [200/2266], Loss: 1.1754\n",
      "Epoch [18/100], Step [300/2266], Loss: 1.4602\n",
      "Epoch [18/100], Step [400/2266], Loss: 1.4857\n",
      "Epoch [18/100], Step [500/2266], Loss: 1.1224\n",
      "Epoch [18/100], Step [600/2266], Loss: 1.9932\n",
      "Epoch [18/100], Step [700/2266], Loss: 1.4921\n",
      "Epoch [18/100], Step [800/2266], Loss: 1.4200\n",
      "Epoch [18/100], Step [900/2266], Loss: 2.2402\n",
      "Epoch [18/100], Step [1000/2266], Loss: 1.8892\n",
      "Epoch [18/100], Step [1100/2266], Loss: 0.9074\n",
      "Epoch [18/100], Step [1200/2266], Loss: 1.6130\n",
      "Epoch [18/100], Step [1300/2266], Loss: 1.8021\n",
      "Epoch [18/100], Step [1400/2266], Loss: 2.1517\n",
      "Epoch [18/100], Step [1500/2266], Loss: 1.6954\n",
      "Epoch [18/100], Step [1600/2266], Loss: 1.1679\n",
      "Epoch [18/100], Step [1700/2266], Loss: 1.4816\n",
      "Epoch [18/100], Step [1800/2266], Loss: 1.6990\n",
      "Epoch [18/100], Step [1900/2266], Loss: 1.9250\n",
      "Epoch [18/100], Step [2000/2266], Loss: 2.2437\n",
      "Epoch [18/100], Step [2100/2266], Loss: 1.9080\n",
      "Epoch [18/100], Step [2200/2266], Loss: 1.3950\n",
      "Epoch [19/100], Step [100/2266], Loss: 1.5104\n",
      "Epoch [19/100], Step [200/2266], Loss: 1.0482\n",
      "Epoch [19/100], Step [300/2266], Loss: 1.4576\n",
      "Epoch [19/100], Step [400/2266], Loss: 1.5373\n",
      "Epoch [19/100], Step [500/2266], Loss: 1.1964\n",
      "Epoch [19/100], Step [600/2266], Loss: 0.8232\n",
      "Epoch [19/100], Step [700/2266], Loss: 1.3190\n",
      "Epoch [19/100], Step [800/2266], Loss: 1.4497\n",
      "Epoch [19/100], Step [900/2266], Loss: 1.3079\n",
      "Epoch [19/100], Step [1000/2266], Loss: 1.3898\n",
      "Epoch [19/100], Step [1100/2266], Loss: 2.0037\n",
      "Epoch [19/100], Step [1200/2266], Loss: 1.5091\n",
      "Epoch [19/100], Step [1300/2266], Loss: 1.4316\n",
      "Epoch [19/100], Step [1400/2266], Loss: 1.5281\n",
      "Epoch [19/100], Step [1500/2266], Loss: 2.1154\n",
      "Epoch [19/100], Step [1600/2266], Loss: 1.4122\n",
      "Epoch [19/100], Step [1700/2266], Loss: 1.5877\n",
      "Epoch [19/100], Step [1800/2266], Loss: 1.9595\n",
      "Epoch [19/100], Step [1900/2266], Loss: 1.5254\n",
      "Epoch [19/100], Step [2000/2266], Loss: 1.7423\n",
      "Epoch [19/100], Step [2100/2266], Loss: 1.3106\n",
      "Epoch [19/100], Step [2200/2266], Loss: 2.0939\n",
      "Epoch [20/100], Step [100/2266], Loss: 0.7974\n",
      "Epoch [20/100], Step [200/2266], Loss: 1.2639\n",
      "Epoch [20/100], Step [300/2266], Loss: 1.3440\n",
      "Epoch [20/100], Step [400/2266], Loss: 1.2815\n",
      "Epoch [20/100], Step [500/2266], Loss: 1.0672\n",
      "Epoch [20/100], Step [600/2266], Loss: 1.3477\n",
      "Epoch [20/100], Step [700/2266], Loss: 1.4877\n",
      "Epoch [20/100], Step [800/2266], Loss: 0.9897\n",
      "Epoch [20/100], Step [900/2266], Loss: 1.4139\n",
      "Epoch [20/100], Step [1000/2266], Loss: 1.6790\n",
      "Epoch [20/100], Step [1100/2266], Loss: 1.5541\n",
      "Epoch [20/100], Step [1200/2266], Loss: 1.5703\n",
      "Epoch [20/100], Step [1300/2266], Loss: 1.5338\n",
      "Epoch [20/100], Step [1400/2266], Loss: 1.1451\n",
      "Epoch [20/100], Step [1500/2266], Loss: 1.1448\n",
      "Epoch [20/100], Step [1600/2266], Loss: 1.8578\n",
      "Epoch [20/100], Step [1700/2266], Loss: 1.5096\n",
      "Epoch [20/100], Step [1800/2266], Loss: 1.4692\n",
      "Epoch [20/100], Step [1900/2266], Loss: 1.6886\n",
      "Epoch [20/100], Step [2000/2266], Loss: 1.5296\n",
      "Epoch [20/100], Step [2100/2266], Loss: 1.9424\n",
      "Epoch [20/100], Step [2200/2266], Loss: 1.2412\n",
      "Epoch [21/100], Step [100/2266], Loss: 1.0076\n",
      "Epoch [21/100], Step [200/2266], Loss: 0.9376\n",
      "Epoch [21/100], Step [300/2266], Loss: 0.8850\n",
      "Epoch [21/100], Step [400/2266], Loss: 1.1241\n",
      "Epoch [21/100], Step [500/2266], Loss: 0.8702\n",
      "Epoch [21/100], Step [600/2266], Loss: 1.2839\n",
      "Epoch [21/100], Step [700/2266], Loss: 1.2502\n",
      "Epoch [21/100], Step [800/2266], Loss: 0.8534\n",
      "Epoch [21/100], Step [900/2266], Loss: 1.2482\n",
      "Epoch [21/100], Step [1000/2266], Loss: 1.1083\n",
      "Epoch [21/100], Step [1100/2266], Loss: 1.3858\n",
      "Epoch [21/100], Step [1200/2266], Loss: 1.4026\n",
      "Epoch [21/100], Step [1300/2266], Loss: 1.1732\n",
      "Epoch [21/100], Step [1400/2266], Loss: 1.5534\n",
      "Epoch [21/100], Step [1500/2266], Loss: 1.5860\n",
      "Epoch [21/100], Step [1600/2266], Loss: 1.1683\n",
      "Epoch [21/100], Step [1700/2266], Loss: 1.5617\n",
      "Epoch [21/100], Step [1800/2266], Loss: 1.2083\n",
      "Epoch [21/100], Step [1900/2266], Loss: 1.6350\n",
      "Epoch [21/100], Step [2000/2266], Loss: 1.5100\n",
      "Epoch [21/100], Step [2100/2266], Loss: 1.2920\n",
      "Epoch [21/100], Step [2200/2266], Loss: 1.6777\n",
      "Epoch [22/100], Step [100/2266], Loss: 0.7650\n",
      "Epoch [22/100], Step [200/2266], Loss: 1.1374\n",
      "Epoch [22/100], Step [300/2266], Loss: 1.3422\n",
      "Epoch [22/100], Step [400/2266], Loss: 1.0878\n",
      "Epoch [22/100], Step [500/2266], Loss: 0.9212\n",
      "Epoch [22/100], Step [600/2266], Loss: 0.7718\n",
      "Epoch [22/100], Step [700/2266], Loss: 0.9335\n",
      "Epoch [22/100], Step [800/2266], Loss: 0.9308\n",
      "Epoch [22/100], Step [900/2266], Loss: 1.2202\n",
      "Epoch [22/100], Step [1000/2266], Loss: 0.7168\n",
      "Epoch [22/100], Step [1100/2266], Loss: 1.1888\n",
      "Epoch [22/100], Step [1200/2266], Loss: 0.9604\n",
      "Epoch [22/100], Step [1300/2266], Loss: 1.3731\n",
      "Epoch [22/100], Step [1400/2266], Loss: 1.6701\n",
      "Epoch [22/100], Step [1500/2266], Loss: 0.7554\n",
      "Epoch [22/100], Step [1600/2266], Loss: 1.1927\n",
      "Epoch [22/100], Step [1700/2266], Loss: 1.3955\n",
      "Epoch [22/100], Step [1800/2266], Loss: 1.2347\n",
      "Epoch [22/100], Step [1900/2266], Loss: 0.9797\n",
      "Epoch [22/100], Step [2000/2266], Loss: 1.5918\n",
      "Epoch [22/100], Step [2100/2266], Loss: 1.4748\n",
      "Epoch [22/100], Step [2200/2266], Loss: 1.5834\n",
      "Epoch [23/100], Step [100/2266], Loss: 0.9374\n",
      "Epoch [23/100], Step [200/2266], Loss: 0.6964\n",
      "Epoch [23/100], Step [300/2266], Loss: 0.5321\n",
      "Epoch [23/100], Step [400/2266], Loss: 0.7491\n",
      "Epoch [23/100], Step [500/2266], Loss: 0.9717\n",
      "Epoch [23/100], Step [600/2266], Loss: 0.7186\n",
      "Epoch [23/100], Step [700/2266], Loss: 1.3568\n",
      "Epoch [23/100], Step [800/2266], Loss: 1.2073\n",
      "Epoch [23/100], Step [900/2266], Loss: 1.0639\n",
      "Epoch [23/100], Step [1000/2266], Loss: 0.8999\n",
      "Epoch [23/100], Step [1100/2266], Loss: 0.8574\n",
      "Epoch [23/100], Step [1200/2266], Loss: 1.4526\n",
      "Epoch [23/100], Step [1300/2266], Loss: 1.4351\n",
      "Epoch [23/100], Step [1400/2266], Loss: 1.2928\n",
      "Epoch [23/100], Step [1500/2266], Loss: 1.2858\n",
      "Epoch [23/100], Step [1600/2266], Loss: 1.3775\n",
      "Epoch [23/100], Step [1700/2266], Loss: 0.9984\n",
      "Epoch [23/100], Step [1800/2266], Loss: 1.3025\n",
      "Epoch [23/100], Step [1900/2266], Loss: 1.0046\n",
      "Epoch [23/100], Step [2000/2266], Loss: 1.2204\n",
      "Epoch [23/100], Step [2100/2266], Loss: 1.3137\n",
      "Epoch [23/100], Step [2200/2266], Loss: 1.7609\n",
      "Epoch [24/100], Step [100/2266], Loss: 0.6301\n",
      "Epoch [24/100], Step [200/2266], Loss: 1.0847\n",
      "Epoch [24/100], Step [300/2266], Loss: 0.8910\n",
      "Epoch [24/100], Step [400/2266], Loss: 0.6046\n",
      "Epoch [24/100], Step [500/2266], Loss: 1.1117\n",
      "Epoch [24/100], Step [600/2266], Loss: 1.2721\n",
      "Epoch [24/100], Step [700/2266], Loss: 1.0401\n",
      "Epoch [24/100], Step [800/2266], Loss: 0.7666\n",
      "Epoch [24/100], Step [900/2266], Loss: 1.0588\n",
      "Epoch [24/100], Step [1000/2266], Loss: 1.0275\n",
      "Epoch [24/100], Step [1100/2266], Loss: 1.0515\n",
      "Epoch [24/100], Step [1200/2266], Loss: 0.9951\n",
      "Epoch [24/100], Step [1300/2266], Loss: 0.8851\n",
      "Epoch [24/100], Step [1400/2266], Loss: 1.8152\n",
      "Epoch [24/100], Step [1500/2266], Loss: 0.9627\n",
      "Epoch [24/100], Step [1600/2266], Loss: 1.0218\n",
      "Epoch [24/100], Step [1700/2266], Loss: 0.9749\n",
      "Epoch [24/100], Step [1800/2266], Loss: 1.3624\n",
      "Epoch [24/100], Step [1900/2266], Loss: 1.0704\n",
      "Epoch [24/100], Step [2000/2266], Loss: 1.0109\n",
      "Epoch [24/100], Step [2100/2266], Loss: 0.9993\n",
      "Epoch [24/100], Step [2200/2266], Loss: 1.3387\n",
      "Epoch [25/100], Step [100/2266], Loss: 0.7061\n",
      "Epoch [25/100], Step [200/2266], Loss: 0.6256\n",
      "Epoch [25/100], Step [300/2266], Loss: 0.8690\n",
      "Epoch [25/100], Step [400/2266], Loss: 0.8411\n",
      "Epoch [25/100], Step [500/2266], Loss: 0.5807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/100], Step [600/2266], Loss: 0.6645\n",
      "Epoch [25/100], Step [700/2266], Loss: 0.8158\n",
      "Epoch [25/100], Step [800/2266], Loss: 0.7994\n",
      "Epoch [25/100], Step [900/2266], Loss: 0.6503\n",
      "Epoch [25/100], Step [1000/2266], Loss: 0.7869\n",
      "Epoch [25/100], Step [1100/2266], Loss: 1.2629\n",
      "Epoch [25/100], Step [1200/2266], Loss: 0.9685\n",
      "Epoch [25/100], Step [1300/2266], Loss: 0.9108\n",
      "Epoch [25/100], Step [1400/2266], Loss: 0.8041\n",
      "Epoch [25/100], Step [1500/2266], Loss: 0.9837\n",
      "Epoch [25/100], Step [1600/2266], Loss: 1.1784\n",
      "Epoch [25/100], Step [1700/2266], Loss: 1.4493\n",
      "Epoch [25/100], Step [1800/2266], Loss: 1.3304\n",
      "Epoch [25/100], Step [1900/2266], Loss: 0.8491\n",
      "Epoch [25/100], Step [2000/2266], Loss: 1.2566\n",
      "Epoch [25/100], Step [2100/2266], Loss: 1.0231\n",
      "Epoch [25/100], Step [2200/2266], Loss: 0.6174\n",
      "Epoch [26/100], Step [100/2266], Loss: 0.7305\n",
      "Epoch [26/100], Step [200/2266], Loss: 1.0660\n",
      "Epoch [26/100], Step [300/2266], Loss: 0.7953\n",
      "Epoch [26/100], Step [400/2266], Loss: 0.8654\n",
      "Epoch [26/100], Step [500/2266], Loss: 0.6214\n",
      "Epoch [26/100], Step [600/2266], Loss: 0.8593\n",
      "Epoch [26/100], Step [700/2266], Loss: 0.6320\n",
      "Epoch [26/100], Step [800/2266], Loss: 0.8458\n",
      "Epoch [26/100], Step [900/2266], Loss: 0.8404\n",
      "Epoch [26/100], Step [1000/2266], Loss: 0.9873\n",
      "Epoch [26/100], Step [1100/2266], Loss: 0.6799\n",
      "Epoch [26/100], Step [1200/2266], Loss: 0.8691\n",
      "Epoch [26/100], Step [1300/2266], Loss: 0.7119\n",
      "Epoch [26/100], Step [1400/2266], Loss: 0.8054\n",
      "Epoch [26/100], Step [1500/2266], Loss: 0.9641\n",
      "Epoch [26/100], Step [1600/2266], Loss: 0.4546\n",
      "Epoch [26/100], Step [1700/2266], Loss: 0.8926\n",
      "Epoch [26/100], Step [1800/2266], Loss: 0.9656\n",
      "Epoch [26/100], Step [1900/2266], Loss: 1.2117\n",
      "Epoch [26/100], Step [2000/2266], Loss: 1.1069\n",
      "Epoch [26/100], Step [2100/2266], Loss: 0.8489\n",
      "Epoch [26/100], Step [2200/2266], Loss: 0.9108\n",
      "Epoch [27/100], Step [100/2266], Loss: 0.3927\n",
      "Epoch [27/100], Step [200/2266], Loss: 0.8694\n",
      "Epoch [27/100], Step [300/2266], Loss: 0.7616\n",
      "Epoch [27/100], Step [400/2266], Loss: 0.6169\n",
      "Epoch [27/100], Step [500/2266], Loss: 0.8240\n",
      "Epoch [27/100], Step [600/2266], Loss: 0.8648\n",
      "Epoch [27/100], Step [700/2266], Loss: 0.5217\n",
      "Epoch [27/100], Step [800/2266], Loss: 0.9809\n",
      "Epoch [27/100], Step [900/2266], Loss: 0.5720\n",
      "Epoch [27/100], Step [1000/2266], Loss: 0.6825\n",
      "Epoch [27/100], Step [1100/2266], Loss: 1.5702\n",
      "Epoch [27/100], Step [1200/2266], Loss: 0.8309\n",
      "Epoch [27/100], Step [1300/2266], Loss: 1.1889\n",
      "Epoch [27/100], Step [1400/2266], Loss: 0.7028\n",
      "Epoch [27/100], Step [1500/2266], Loss: 0.7537\n",
      "Epoch [27/100], Step [1600/2266], Loss: 0.3639\n",
      "Epoch [27/100], Step [1700/2266], Loss: 0.7180\n",
      "Epoch [27/100], Step [1800/2266], Loss: 0.7694\n",
      "Epoch [27/100], Step [1900/2266], Loss: 1.0350\n",
      "Epoch [27/100], Step [2000/2266], Loss: 0.4087\n",
      "Epoch [27/100], Step [2100/2266], Loss: 0.6315\n",
      "Epoch [27/100], Step [2200/2266], Loss: 0.6884\n",
      "Epoch [28/100], Step [100/2266], Loss: 0.7143\n",
      "Epoch [28/100], Step [200/2266], Loss: 0.5597\n",
      "Epoch [28/100], Step [300/2266], Loss: 0.8041\n",
      "Epoch [28/100], Step [400/2266], Loss: 0.7550\n",
      "Epoch [28/100], Step [500/2266], Loss: 1.0119\n",
      "Epoch [28/100], Step [600/2266], Loss: 0.6549\n",
      "Epoch [28/100], Step [700/2266], Loss: 0.8208\n",
      "Epoch [28/100], Step [800/2266], Loss: 1.2081\n",
      "Epoch [28/100], Step [900/2266], Loss: 0.6616\n",
      "Epoch [28/100], Step [1000/2266], Loss: 0.5511\n",
      "Epoch [28/100], Step [1100/2266], Loss: 0.8127\n",
      "Epoch [28/100], Step [1200/2266], Loss: 1.1737\n",
      "Epoch [28/100], Step [1300/2266], Loss: 0.4080\n",
      "Epoch [28/100], Step [1400/2266], Loss: 0.6718\n",
      "Epoch [28/100], Step [1500/2266], Loss: 1.0053\n",
      "Epoch [28/100], Step [1600/2266], Loss: 0.8731\n",
      "Epoch [28/100], Step [1700/2266], Loss: 1.0823\n",
      "Epoch [28/100], Step [1800/2266], Loss: 0.9881\n",
      "Epoch [28/100], Step [1900/2266], Loss: 0.7875\n",
      "Epoch [28/100], Step [2000/2266], Loss: 0.8958\n",
      "Epoch [28/100], Step [2100/2266], Loss: 1.0199\n",
      "Epoch [28/100], Step [2200/2266], Loss: 1.0899\n",
      "Epoch [29/100], Step [100/2266], Loss: 0.5564\n",
      "Epoch [29/100], Step [200/2266], Loss: 0.6742\n",
      "Epoch [29/100], Step [300/2266], Loss: 0.7875\n",
      "Epoch [29/100], Step [400/2266], Loss: 0.7695\n",
      "Epoch [29/100], Step [500/2266], Loss: 0.5830\n",
      "Epoch [29/100], Step [600/2266], Loss: 0.6876\n",
      "Epoch [29/100], Step [700/2266], Loss: 0.6358\n",
      "Epoch [29/100], Step [800/2266], Loss: 0.3900\n",
      "Epoch [29/100], Step [900/2266], Loss: 0.8334\n",
      "Epoch [29/100], Step [1000/2266], Loss: 0.7692\n",
      "Epoch [29/100], Step [1100/2266], Loss: 0.7769\n",
      "Epoch [29/100], Step [1200/2266], Loss: 1.1138\n",
      "Epoch [29/100], Step [1300/2266], Loss: 0.7508\n",
      "Epoch [29/100], Step [1400/2266], Loss: 0.6118\n",
      "Epoch [29/100], Step [1500/2266], Loss: 0.9298\n",
      "Epoch [29/100], Step [1600/2266], Loss: 0.6075\n",
      "Epoch [29/100], Step [1700/2266], Loss: 0.6991\n",
      "Epoch [29/100], Step [1800/2266], Loss: 0.7788\n",
      "Epoch [29/100], Step [1900/2266], Loss: 1.0028\n",
      "Epoch [29/100], Step [2000/2266], Loss: 1.1740\n",
      "Epoch [29/100], Step [2100/2266], Loss: 0.6467\n",
      "Epoch [29/100], Step [2200/2266], Loss: 0.6070\n",
      "Epoch [30/100], Step [100/2266], Loss: 0.4166\n",
      "Epoch [30/100], Step [200/2266], Loss: 0.3582\n",
      "Epoch [30/100], Step [300/2266], Loss: 0.9907\n",
      "Epoch [30/100], Step [400/2266], Loss: 0.5986\n",
      "Epoch [30/100], Step [500/2266], Loss: 0.6512\n",
      "Epoch [30/100], Step [600/2266], Loss: 0.5796\n",
      "Epoch [30/100], Step [700/2266], Loss: 0.5307\n",
      "Epoch [30/100], Step [800/2266], Loss: 0.5359\n",
      "Epoch [30/100], Step [900/2266], Loss: 1.0288\n",
      "Epoch [30/100], Step [1000/2266], Loss: 0.7306\n",
      "Epoch [30/100], Step [1100/2266], Loss: 0.6333\n",
      "Epoch [30/100], Step [1200/2266], Loss: 0.5844\n",
      "Epoch [30/100], Step [1300/2266], Loss: 0.5556\n",
      "Epoch [30/100], Step [1400/2266], Loss: 0.9622\n",
      "Epoch [30/100], Step [1500/2266], Loss: 0.7674\n",
      "Epoch [30/100], Step [1600/2266], Loss: 1.0209\n",
      "Epoch [30/100], Step [1700/2266], Loss: 0.8799\n",
      "Epoch [30/100], Step [1800/2266], Loss: 0.7176\n",
      "Epoch [30/100], Step [1900/2266], Loss: 0.6984\n",
      "Epoch [30/100], Step [2000/2266], Loss: 1.0091\n",
      "Epoch [30/100], Step [2100/2266], Loss: 0.5871\n",
      "Epoch [30/100], Step [2200/2266], Loss: 1.2714\n",
      "Epoch [31/100], Step [100/2266], Loss: 0.3826\n",
      "Epoch [31/100], Step [200/2266], Loss: 0.7320\n",
      "Epoch [31/100], Step [300/2266], Loss: 0.5878\n",
      "Epoch [31/100], Step [400/2266], Loss: 0.5794\n",
      "Epoch [31/100], Step [500/2266], Loss: 0.6216\n",
      "Epoch [31/100], Step [600/2266], Loss: 0.3103\n",
      "Epoch [31/100], Step [700/2266], Loss: 0.6215\n",
      "Epoch [31/100], Step [800/2266], Loss: 0.8088\n",
      "Epoch [31/100], Step [900/2266], Loss: 0.6571\n",
      "Epoch [31/100], Step [1000/2266], Loss: 0.8075\n",
      "Epoch [31/100], Step [1100/2266], Loss: 0.7453\n",
      "Epoch [31/100], Step [1200/2266], Loss: 0.6963\n",
      "Epoch [31/100], Step [1300/2266], Loss: 0.7487\n",
      "Epoch [31/100], Step [1400/2266], Loss: 0.6036\n",
      "Epoch [31/100], Step [1500/2266], Loss: 0.7556\n",
      "Epoch [31/100], Step [1600/2266], Loss: 0.6296\n",
      "Epoch [31/100], Step [1700/2266], Loss: 0.5902\n",
      "Epoch [31/100], Step [1800/2266], Loss: 1.0095\n",
      "Epoch [31/100], Step [1900/2266], Loss: 1.1412\n",
      "Epoch [31/100], Step [2000/2266], Loss: 0.6862\n",
      "Epoch [31/100], Step [2100/2266], Loss: 0.8886\n",
      "Epoch [31/100], Step [2200/2266], Loss: 0.6221\n",
      "Epoch [32/100], Step [100/2266], Loss: 0.5746\n",
      "Epoch [32/100], Step [200/2266], Loss: 0.4762\n",
      "Epoch [32/100], Step [300/2266], Loss: 0.3051\n",
      "Epoch [32/100], Step [400/2266], Loss: 0.1992\n",
      "Epoch [32/100], Step [500/2266], Loss: 0.2187\n",
      "Epoch [32/100], Step [600/2266], Loss: 0.7349\n",
      "Epoch [32/100], Step [700/2266], Loss: 0.4163\n",
      "Epoch [32/100], Step [800/2266], Loss: 0.7628\n",
      "Epoch [32/100], Step [900/2266], Loss: 0.2991\n",
      "Epoch [32/100], Step [1000/2266], Loss: 0.9210\n",
      "Epoch [32/100], Step [1100/2266], Loss: 0.7466\n",
      "Epoch [32/100], Step [1200/2266], Loss: 0.7799\n",
      "Epoch [32/100], Step [1300/2266], Loss: 0.5995\n",
      "Epoch [32/100], Step [1400/2266], Loss: 0.5039\n",
      "Epoch [32/100], Step [1500/2266], Loss: 0.8334\n",
      "Epoch [32/100], Step [1600/2266], Loss: 0.3973\n",
      "Epoch [32/100], Step [1700/2266], Loss: 1.4362\n",
      "Epoch [32/100], Step [1800/2266], Loss: 0.6034\n",
      "Epoch [32/100], Step [1900/2266], Loss: 0.3255\n",
      "Epoch [32/100], Step [2000/2266], Loss: 0.8854\n",
      "Epoch [32/100], Step [2100/2266], Loss: 0.8434\n",
      "Epoch [32/100], Step [2200/2266], Loss: 0.9419\n",
      "Epoch [33/100], Step [100/2266], Loss: 0.5200\n",
      "Epoch [33/100], Step [200/2266], Loss: 0.5353\n",
      "Epoch [33/100], Step [300/2266], Loss: 0.4127\n",
      "Epoch [33/100], Step [400/2266], Loss: 0.1565\n",
      "Epoch [33/100], Step [500/2266], Loss: 0.2771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/100], Step [600/2266], Loss: 0.3781\n",
      "Epoch [33/100], Step [700/2266], Loss: 0.5663\n",
      "Epoch [33/100], Step [800/2266], Loss: 0.4947\n",
      "Epoch [33/100], Step [900/2266], Loss: 0.4459\n",
      "Epoch [33/100], Step [1000/2266], Loss: 0.5849\n",
      "Epoch [33/100], Step [1100/2266], Loss: 0.3295\n",
      "Epoch [33/100], Step [1200/2266], Loss: 0.4678\n",
      "Epoch [33/100], Step [1300/2266], Loss: 0.5790\n",
      "Epoch [33/100], Step [1400/2266], Loss: 0.5191\n",
      "Epoch [33/100], Step [1500/2266], Loss: 0.7079\n",
      "Epoch [33/100], Step [1600/2266], Loss: 0.8258\n",
      "Epoch [33/100], Step [1700/2266], Loss: 0.8407\n",
      "Epoch [33/100], Step [1800/2266], Loss: 0.5297\n",
      "Epoch [33/100], Step [1900/2266], Loss: 0.5716\n",
      "Epoch [33/100], Step [2000/2266], Loss: 1.0484\n",
      "Epoch [33/100], Step [2100/2266], Loss: 0.9975\n",
      "Epoch [33/100], Step [2200/2266], Loss: 1.1470\n",
      "Epoch [34/100], Step [100/2266], Loss: 0.2377\n",
      "Epoch [34/100], Step [200/2266], Loss: 0.3089\n",
      "Epoch [34/100], Step [300/2266], Loss: 0.5877\n",
      "Epoch [34/100], Step [400/2266], Loss: 0.1748\n",
      "Epoch [34/100], Step [500/2266], Loss: 0.5206\n",
      "Epoch [34/100], Step [600/2266], Loss: 0.3763\n",
      "Epoch [34/100], Step [700/2266], Loss: 0.3715\n",
      "Epoch [34/100], Step [800/2266], Loss: 0.6608\n",
      "Epoch [34/100], Step [900/2266], Loss: 0.3793\n",
      "Epoch [34/100], Step [1000/2266], Loss: 0.4602\n",
      "Epoch [34/100], Step [1100/2266], Loss: 0.4556\n",
      "Epoch [34/100], Step [1200/2266], Loss: 0.3947\n",
      "Epoch [34/100], Step [1300/2266], Loss: 0.5713\n",
      "Epoch [34/100], Step [1400/2266], Loss: 0.9766\n",
      "Epoch [34/100], Step [1500/2266], Loss: 0.5262\n",
      "Epoch [34/100], Step [1600/2266], Loss: 0.6158\n",
      "Epoch [34/100], Step [1700/2266], Loss: 0.5443\n",
      "Epoch [34/100], Step [1800/2266], Loss: 0.5281\n",
      "Epoch [34/100], Step [1900/2266], Loss: 0.6824\n",
      "Epoch [34/100], Step [2000/2266], Loss: 0.5748\n",
      "Epoch [34/100], Step [2100/2266], Loss: 1.0451\n",
      "Epoch [34/100], Step [2200/2266], Loss: 0.5541\n",
      "Epoch [35/100], Step [100/2266], Loss: 0.4679\n",
      "Epoch [35/100], Step [200/2266], Loss: 0.3893\n",
      "Epoch [35/100], Step [300/2266], Loss: 0.6288\n",
      "Epoch [35/100], Step [400/2266], Loss: 0.2711\n",
      "Epoch [35/100], Step [500/2266], Loss: 0.5432\n",
      "Epoch [35/100], Step [600/2266], Loss: 0.2212\n",
      "Epoch [35/100], Step [700/2266], Loss: 0.3591\n",
      "Epoch [35/100], Step [800/2266], Loss: 0.7991\n",
      "Epoch [35/100], Step [900/2266], Loss: 0.3709\n",
      "Epoch [35/100], Step [1000/2266], Loss: 0.6229\n",
      "Epoch [35/100], Step [1100/2266], Loss: 0.2739\n",
      "Epoch [35/100], Step [1200/2266], Loss: 0.6544\n",
      "Epoch [35/100], Step [1300/2266], Loss: 0.5086\n",
      "Epoch [35/100], Step [1400/2266], Loss: 0.6509\n",
      "Epoch [35/100], Step [1500/2266], Loss: 0.6012\n",
      "Epoch [35/100], Step [1600/2266], Loss: 0.5289\n",
      "Epoch [35/100], Step [1700/2266], Loss: 0.3324\n",
      "Epoch [35/100], Step [1800/2266], Loss: 0.7975\n",
      "Epoch [35/100], Step [1900/2266], Loss: 0.4664\n",
      "Epoch [35/100], Step [2000/2266], Loss: 0.6460\n",
      "Epoch [35/100], Step [2100/2266], Loss: 0.6453\n",
      "Epoch [35/100], Step [2200/2266], Loss: 0.5094\n",
      "Epoch [36/100], Step [100/2266], Loss: 0.3053\n",
      "Epoch [36/100], Step [200/2266], Loss: 0.1395\n",
      "Epoch [36/100], Step [300/2266], Loss: 0.6697\n",
      "Epoch [36/100], Step [400/2266], Loss: 0.4777\n",
      "Epoch [36/100], Step [500/2266], Loss: 0.4024\n",
      "Epoch [36/100], Step [600/2266], Loss: 0.3458\n",
      "Epoch [36/100], Step [700/2266], Loss: 0.3093\n",
      "Epoch [36/100], Step [800/2266], Loss: 0.4826\n",
      "Epoch [36/100], Step [900/2266], Loss: 0.3406\n",
      "Epoch [36/100], Step [1000/2266], Loss: 0.3911\n",
      "Epoch [36/100], Step [1100/2266], Loss: 0.9072\n",
      "Epoch [36/100], Step [1200/2266], Loss: 0.8379\n",
      "Epoch [36/100], Step [1300/2266], Loss: 0.6569\n",
      "Epoch [36/100], Step [1400/2266], Loss: 0.3004\n",
      "Epoch [36/100], Step [1500/2266], Loss: 0.2277\n",
      "Epoch [36/100], Step [1600/2266], Loss: 0.5034\n",
      "Epoch [36/100], Step [1700/2266], Loss: 0.5192\n",
      "Epoch [36/100], Step [1800/2266], Loss: 0.6476\n",
      "Epoch [36/100], Step [1900/2266], Loss: 0.4285\n",
      "Epoch [36/100], Step [2000/2266], Loss: 0.6141\n",
      "Epoch [36/100], Step [2100/2266], Loss: 0.3033\n",
      "Epoch [36/100], Step [2200/2266], Loss: 1.1205\n",
      "Epoch [37/100], Step [100/2266], Loss: 0.3115\n",
      "Epoch [37/100], Step [200/2266], Loss: 0.3633\n",
      "Epoch [37/100], Step [300/2266], Loss: 0.4456\n",
      "Epoch [37/100], Step [400/2266], Loss: 0.2576\n",
      "Epoch [37/100], Step [500/2266], Loss: 0.3926\n",
      "Epoch [37/100], Step [600/2266], Loss: 0.4203\n",
      "Epoch [37/100], Step [700/2266], Loss: 0.3436\n",
      "Epoch [37/100], Step [800/2266], Loss: 0.2883\n",
      "Epoch [37/100], Step [900/2266], Loss: 0.2205\n",
      "Epoch [37/100], Step [1000/2266], Loss: 0.2046\n",
      "Epoch [37/100], Step [1100/2266], Loss: 0.3179\n",
      "Epoch [37/100], Step [1200/2266], Loss: 0.4641\n",
      "Epoch [37/100], Step [1300/2266], Loss: 0.7082\n",
      "Epoch [37/100], Step [1400/2266], Loss: 0.6537\n",
      "Epoch [37/100], Step [1500/2266], Loss: 0.4629\n",
      "Epoch [37/100], Step [1600/2266], Loss: 1.2816\n",
      "Epoch [37/100], Step [1700/2266], Loss: 0.9553\n",
      "Epoch [37/100], Step [1800/2266], Loss: 0.3442\n",
      "Epoch [37/100], Step [1900/2266], Loss: 0.7003\n",
      "Epoch [37/100], Step [2000/2266], Loss: 0.4570\n",
      "Epoch [37/100], Step [2100/2266], Loss: 0.2281\n",
      "Epoch [37/100], Step [2200/2266], Loss: 0.6780\n",
      "Epoch [38/100], Step [100/2266], Loss: 0.2065\n",
      "Epoch [38/100], Step [200/2266], Loss: 0.2252\n",
      "Epoch [38/100], Step [300/2266], Loss: 0.1957\n",
      "Epoch [38/100], Step [400/2266], Loss: 0.3771\n",
      "Epoch [38/100], Step [500/2266], Loss: 0.4054\n",
      "Epoch [38/100], Step [600/2266], Loss: 0.2565\n",
      "Epoch [38/100], Step [700/2266], Loss: 0.3820\n",
      "Epoch [38/100], Step [800/2266], Loss: 0.5975\n",
      "Epoch [38/100], Step [900/2266], Loss: 0.3971\n",
      "Epoch [38/100], Step [1000/2266], Loss: 0.4669\n",
      "Epoch [38/100], Step [1100/2266], Loss: 0.4422\n",
      "Epoch [38/100], Step [1200/2266], Loss: 0.2551\n",
      "Epoch [38/100], Step [1300/2266], Loss: 0.4214\n",
      "Epoch [38/100], Step [1400/2266], Loss: 0.7451\n",
      "Epoch [38/100], Step [1500/2266], Loss: 0.3181\n",
      "Epoch [38/100], Step [1600/2266], Loss: 0.3982\n",
      "Epoch [38/100], Step [1700/2266], Loss: 0.8360\n",
      "Epoch [38/100], Step [1800/2266], Loss: 0.7444\n",
      "Epoch [38/100], Step [1900/2266], Loss: 0.9365\n",
      "Epoch [38/100], Step [2000/2266], Loss: 0.6931\n",
      "Epoch [38/100], Step [2100/2266], Loss: 0.6188\n",
      "Epoch [38/100], Step [2200/2266], Loss: 0.6365\n",
      "Epoch [39/100], Step [100/2266], Loss: 0.2279\n",
      "Epoch [39/100], Step [200/2266], Loss: 0.4066\n",
      "Epoch [39/100], Step [300/2266], Loss: 0.4511\n",
      "Epoch [39/100], Step [400/2266], Loss: 0.4765\n",
      "Epoch [39/100], Step [500/2266], Loss: 0.1461\n",
      "Epoch [39/100], Step [600/2266], Loss: 0.5973\n",
      "Epoch [39/100], Step [700/2266], Loss: 0.5034\n",
      "Epoch [39/100], Step [800/2266], Loss: 0.4223\n",
      "Epoch [39/100], Step [900/2266], Loss: 0.3456\n",
      "Epoch [39/100], Step [1000/2266], Loss: 0.5081\n",
      "Epoch [39/100], Step [1100/2266], Loss: 0.4241\n",
      "Epoch [39/100], Step [1200/2266], Loss: 0.3337\n",
      "Epoch [39/100], Step [1300/2266], Loss: 0.2752\n",
      "Epoch [39/100], Step [1400/2266], Loss: 0.3646\n",
      "Epoch [39/100], Step [1500/2266], Loss: 0.4506\n",
      "Epoch [39/100], Step [1600/2266], Loss: 0.7979\n",
      "Epoch [39/100], Step [1700/2266], Loss: 0.6587\n",
      "Epoch [39/100], Step [1800/2266], Loss: 0.5054\n",
      "Epoch [39/100], Step [1900/2266], Loss: 0.7620\n",
      "Epoch [39/100], Step [2000/2266], Loss: 1.1130\n",
      "Epoch [39/100], Step [2100/2266], Loss: 0.5088\n",
      "Epoch [39/100], Step [2200/2266], Loss: 0.6698\n",
      "Epoch [40/100], Step [100/2266], Loss: 0.3043\n",
      "Epoch [40/100], Step [200/2266], Loss: 0.3083\n",
      "Epoch [40/100], Step [300/2266], Loss: 0.5476\n",
      "Epoch [40/100], Step [400/2266], Loss: 0.3130\n",
      "Epoch [40/100], Step [500/2266], Loss: 0.4876\n",
      "Epoch [40/100], Step [600/2266], Loss: 0.1889\n",
      "Epoch [40/100], Step [700/2266], Loss: 0.1858\n",
      "Epoch [40/100], Step [800/2266], Loss: 0.2944\n",
      "Epoch [40/100], Step [900/2266], Loss: 0.1513\n",
      "Epoch [40/100], Step [1000/2266], Loss: 0.5206\n",
      "Epoch [40/100], Step [1100/2266], Loss: 0.2464\n",
      "Epoch [40/100], Step [1200/2266], Loss: 0.4573\n",
      "Epoch [40/100], Step [1300/2266], Loss: 0.7814\n",
      "Epoch [40/100], Step [1400/2266], Loss: 0.3364\n",
      "Epoch [40/100], Step [1500/2266], Loss: 0.2739\n",
      "Epoch [40/100], Step [1600/2266], Loss: 0.5082\n",
      "Epoch [40/100], Step [1700/2266], Loss: 0.6414\n",
      "Epoch [40/100], Step [1800/2266], Loss: 0.4859\n",
      "Epoch [40/100], Step [1900/2266], Loss: 0.6802\n",
      "Epoch [40/100], Step [2000/2266], Loss: 0.6564\n",
      "Epoch [40/100], Step [2100/2266], Loss: 1.0419\n",
      "Epoch [40/100], Step [2200/2266], Loss: 0.5667\n",
      "Epoch [41/100], Step [100/2266], Loss: 0.4446\n",
      "Epoch [41/100], Step [200/2266], Loss: 0.1740\n",
      "Epoch [41/100], Step [300/2266], Loss: 0.6126\n",
      "Epoch [41/100], Step [400/2266], Loss: 0.4597\n",
      "Epoch [41/100], Step [500/2266], Loss: 0.4365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/100], Step [600/2266], Loss: 0.2137\n",
      "Epoch [41/100], Step [700/2266], Loss: 0.3541\n",
      "Epoch [41/100], Step [800/2266], Loss: 0.4264\n",
      "Epoch [41/100], Step [900/2266], Loss: 0.6107\n",
      "Epoch [41/100], Step [1000/2266], Loss: 0.6662\n",
      "Epoch [41/100], Step [1100/2266], Loss: 0.8119\n",
      "Epoch [41/100], Step [1200/2266], Loss: 0.5100\n",
      "Epoch [41/100], Step [1300/2266], Loss: 0.5178\n",
      "Epoch [41/100], Step [1400/2266], Loss: 0.5400\n",
      "Epoch [41/100], Step [1500/2266], Loss: 0.4238\n",
      "Epoch [41/100], Step [1600/2266], Loss: 0.7642\n",
      "Epoch [41/100], Step [1700/2266], Loss: 0.3584\n",
      "Epoch [41/100], Step [1800/2266], Loss: 0.9085\n",
      "Epoch [41/100], Step [1900/2266], Loss: 0.4381\n",
      "Epoch [41/100], Step [2000/2266], Loss: 0.4332\n",
      "Epoch [41/100], Step [2100/2266], Loss: 0.6852\n",
      "Epoch [41/100], Step [2200/2266], Loss: 0.3493\n",
      "Epoch [42/100], Step [100/2266], Loss: 0.4388\n",
      "Epoch [42/100], Step [200/2266], Loss: 0.6297\n",
      "Epoch [42/100], Step [300/2266], Loss: 0.7408\n",
      "Epoch [42/100], Step [400/2266], Loss: 0.1548\n",
      "Epoch [42/100], Step [500/2266], Loss: 0.3871\n",
      "Epoch [42/100], Step [600/2266], Loss: 0.3036\n",
      "Epoch [42/100], Step [700/2266], Loss: 0.5328\n",
      "Epoch [42/100], Step [800/2266], Loss: 0.4200\n",
      "Epoch [42/100], Step [900/2266], Loss: 0.5812\n",
      "Epoch [42/100], Step [1000/2266], Loss: 0.6322\n",
      "Epoch [42/100], Step [1100/2266], Loss: 0.3717\n",
      "Epoch [42/100], Step [1200/2266], Loss: 0.2543\n",
      "Epoch [42/100], Step [1300/2266], Loss: 0.5424\n",
      "Epoch [42/100], Step [1400/2266], Loss: 0.3644\n",
      "Epoch [42/100], Step [1500/2266], Loss: 0.4992\n",
      "Epoch [42/100], Step [1600/2266], Loss: 0.3788\n",
      "Epoch [42/100], Step [1700/2266], Loss: 0.8086\n",
      "Epoch [42/100], Step [1800/2266], Loss: 1.2281\n",
      "Epoch [42/100], Step [1900/2266], Loss: 0.7122\n",
      "Epoch [42/100], Step [2000/2266], Loss: 0.5630\n",
      "Epoch [42/100], Step [2100/2266], Loss: 0.7019\n",
      "Epoch [42/100], Step [2200/2266], Loss: 0.2409\n",
      "Epoch [43/100], Step [100/2266], Loss: 0.2528\n",
      "Epoch [43/100], Step [200/2266], Loss: 0.3355\n",
      "Epoch [43/100], Step [300/2266], Loss: 0.6120\n",
      "Epoch [43/100], Step [400/2266], Loss: 0.2258\n",
      "Epoch [43/100], Step [500/2266], Loss: 0.2923\n",
      "Epoch [43/100], Step [600/2266], Loss: 0.4692\n",
      "Epoch [43/100], Step [700/2266], Loss: 0.3684\n",
      "Epoch [43/100], Step [800/2266], Loss: 0.7183\n",
      "Epoch [43/100], Step [900/2266], Loss: 0.2444\n",
      "Epoch [43/100], Step [1000/2266], Loss: 0.3633\n",
      "Epoch [43/100], Step [1100/2266], Loss: 0.1283\n",
      "Epoch [43/100], Step [1200/2266], Loss: 0.3420\n",
      "Epoch [43/100], Step [1300/2266], Loss: 0.7364\n",
      "Epoch [43/100], Step [1400/2266], Loss: 0.7947\n",
      "Epoch [43/100], Step [1500/2266], Loss: 0.9002\n",
      "Epoch [43/100], Step [1600/2266], Loss: 0.3822\n",
      "Epoch [43/100], Step [1700/2266], Loss: 0.5316\n",
      "Epoch [43/100], Step [1800/2266], Loss: 0.9930\n",
      "Epoch [43/100], Step [1900/2266], Loss: 0.5098\n",
      "Epoch [43/100], Step [2000/2266], Loss: 0.1106\n",
      "Epoch [43/100], Step [2100/2266], Loss: 0.5691\n",
      "Epoch [43/100], Step [2200/2266], Loss: 0.4617\n",
      "Epoch [44/100], Step [100/2266], Loss: 0.2690\n",
      "Epoch [44/100], Step [200/2266], Loss: 0.5537\n",
      "Epoch [44/100], Step [300/2266], Loss: 0.3183\n",
      "Epoch [44/100], Step [400/2266], Loss: 0.4457\n",
      "Epoch [44/100], Step [500/2266], Loss: 0.2859\n",
      "Epoch [44/100], Step [600/2266], Loss: 0.1845\n",
      "Epoch [44/100], Step [700/2266], Loss: 0.3145\n",
      "Epoch [44/100], Step [800/2266], Loss: 0.4767\n",
      "Epoch [44/100], Step [900/2266], Loss: 0.6261\n",
      "Epoch [44/100], Step [1000/2266], Loss: 0.2730\n",
      "Epoch [44/100], Step [1100/2266], Loss: 0.5975\n",
      "Epoch [44/100], Step [1200/2266], Loss: 0.5458\n",
      "Epoch [44/100], Step [1300/2266], Loss: 0.3717\n",
      "Epoch [44/100], Step [1400/2266], Loss: 1.2125\n",
      "Epoch [44/100], Step [1500/2266], Loss: 0.4610\n",
      "Epoch [44/100], Step [1600/2266], Loss: 0.5769\n",
      "Epoch [44/100], Step [1700/2266], Loss: 0.1711\n",
      "Epoch [44/100], Step [1800/2266], Loss: 0.2245\n",
      "Epoch [44/100], Step [1900/2266], Loss: 0.2202\n",
      "Epoch [44/100], Step [2000/2266], Loss: 0.5566\n",
      "Epoch [44/100], Step [2100/2266], Loss: 0.6281\n",
      "Epoch [44/100], Step [2200/2266], Loss: 0.8363\n",
      "Epoch [45/100], Step [100/2266], Loss: 0.5362\n",
      "Epoch [45/100], Step [200/2266], Loss: 0.5238\n",
      "Epoch [45/100], Step [300/2266], Loss: 0.3802\n",
      "Epoch [45/100], Step [400/2266], Loss: 0.1199\n",
      "Epoch [45/100], Step [500/2266], Loss: 0.6883\n",
      "Epoch [45/100], Step [600/2266], Loss: 0.2982\n",
      "Epoch [45/100], Step [700/2266], Loss: 0.0753\n",
      "Epoch [45/100], Step [800/2266], Loss: 0.4192\n",
      "Epoch [45/100], Step [900/2266], Loss: 0.5305\n",
      "Epoch [45/100], Step [1000/2266], Loss: 0.1767\n",
      "Epoch [45/100], Step [1100/2266], Loss: 0.4999\n",
      "Epoch [45/100], Step [1200/2266], Loss: 0.4466\n",
      "Epoch [45/100], Step [1300/2266], Loss: 0.2570\n",
      "Epoch [45/100], Step [1400/2266], Loss: 0.3882\n",
      "Epoch [45/100], Step [1500/2266], Loss: 0.5205\n",
      "Epoch [45/100], Step [1600/2266], Loss: 0.4188\n",
      "Epoch [45/100], Step [1700/2266], Loss: 0.4918\n",
      "Epoch [45/100], Step [1800/2266], Loss: 0.6594\n",
      "Epoch [45/100], Step [1900/2266], Loss: 0.4412\n",
      "Epoch [45/100], Step [2000/2266], Loss: 0.8125\n",
      "Epoch [45/100], Step [2100/2266], Loss: 0.3877\n",
      "Epoch [45/100], Step [2200/2266], Loss: 0.5159\n",
      "Epoch [46/100], Step [100/2266], Loss: 0.2133\n",
      "Epoch [46/100], Step [200/2266], Loss: 0.1217\n",
      "Epoch [46/100], Step [300/2266], Loss: 0.1845\n",
      "Epoch [46/100], Step [400/2266], Loss: 0.1094\n",
      "Epoch [46/100], Step [500/2266], Loss: 0.2433\n",
      "Epoch [46/100], Step [600/2266], Loss: 0.2751\n",
      "Epoch [46/100], Step [700/2266], Loss: 0.5264\n",
      "Epoch [46/100], Step [800/2266], Loss: 0.1551\n",
      "Epoch [46/100], Step [900/2266], Loss: 0.4784\n",
      "Epoch [46/100], Step [1000/2266], Loss: 0.2243\n",
      "Epoch [46/100], Step [1100/2266], Loss: 0.4848\n",
      "Epoch [46/100], Step [1200/2266], Loss: 0.4512\n",
      "Epoch [46/100], Step [1300/2266], Loss: 0.7095\n",
      "Epoch [46/100], Step [1400/2266], Loss: 0.5888\n",
      "Epoch [46/100], Step [1500/2266], Loss: 0.2161\n",
      "Epoch [46/100], Step [1600/2266], Loss: 0.4170\n",
      "Epoch [46/100], Step [1700/2266], Loss: 0.2819\n",
      "Epoch [46/100], Step [1800/2266], Loss: 0.2572\n",
      "Epoch [46/100], Step [1900/2266], Loss: 0.3527\n",
      "Epoch [46/100], Step [2000/2266], Loss: 0.3988\n",
      "Epoch [46/100], Step [2100/2266], Loss: 0.3451\n",
      "Epoch [46/100], Step [2200/2266], Loss: 0.4166\n",
      "Epoch [47/100], Step [100/2266], Loss: 0.1568\n",
      "Epoch [47/100], Step [200/2266], Loss: 0.3196\n",
      "Epoch [47/100], Step [300/2266], Loss: 0.3199\n",
      "Epoch [47/100], Step [400/2266], Loss: 0.5844\n",
      "Epoch [47/100], Step [500/2266], Loss: 0.1812\n",
      "Epoch [47/100], Step [600/2266], Loss: 0.3508\n",
      "Epoch [47/100], Step [700/2266], Loss: 0.1971\n",
      "Epoch [47/100], Step [800/2266], Loss: 0.3782\n",
      "Epoch [47/100], Step [900/2266], Loss: 0.3781\n",
      "Epoch [47/100], Step [1000/2266], Loss: 0.3004\n",
      "Epoch [47/100], Step [1100/2266], Loss: 0.1824\n",
      "Epoch [47/100], Step [1200/2266], Loss: 0.0878\n",
      "Epoch [47/100], Step [1300/2266], Loss: 0.8254\n",
      "Epoch [47/100], Step [1400/2266], Loss: 0.5418\n",
      "Epoch [47/100], Step [1500/2266], Loss: 0.1947\n",
      "Epoch [47/100], Step [1600/2266], Loss: 0.1337\n",
      "Epoch [47/100], Step [1700/2266], Loss: 0.4931\n",
      "Epoch [47/100], Step [1800/2266], Loss: 0.5598\n",
      "Epoch [47/100], Step [1900/2266], Loss: 0.4407\n",
      "Epoch [47/100], Step [2000/2266], Loss: 0.9844\n",
      "Epoch [47/100], Step [2100/2266], Loss: 0.2454\n",
      "Epoch [47/100], Step [2200/2266], Loss: 0.3791\n",
      "Epoch [48/100], Step [100/2266], Loss: 0.2192\n",
      "Epoch [48/100], Step [200/2266], Loss: 0.3359\n",
      "Epoch [48/100], Step [300/2266], Loss: 0.2135\n",
      "Epoch [48/100], Step [400/2266], Loss: 0.3180\n",
      "Epoch [48/100], Step [500/2266], Loss: 0.4960\n",
      "Epoch [48/100], Step [600/2266], Loss: 0.2657\n",
      "Epoch [48/100], Step [700/2266], Loss: 0.2734\n",
      "Epoch [48/100], Step [800/2266], Loss: 0.2153\n",
      "Epoch [48/100], Step [900/2266], Loss: 0.2923\n",
      "Epoch [48/100], Step [1000/2266], Loss: 0.4166\n",
      "Epoch [48/100], Step [1100/2266], Loss: 0.4814\n",
      "Epoch [48/100], Step [1200/2266], Loss: 0.4943\n",
      "Epoch [48/100], Step [1300/2266], Loss: 0.5299\n",
      "Epoch [48/100], Step [1400/2266], Loss: 0.5583\n",
      "Epoch [48/100], Step [1500/2266], Loss: 0.6455\n",
      "Epoch [48/100], Step [1600/2266], Loss: 0.4674\n",
      "Epoch [48/100], Step [1700/2266], Loss: 0.4562\n",
      "Epoch [48/100], Step [1800/2266], Loss: 0.7437\n",
      "Epoch [48/100], Step [1900/2266], Loss: 0.4145\n",
      "Epoch [48/100], Step [2000/2266], Loss: 0.4908\n",
      "Epoch [48/100], Step [2100/2266], Loss: 0.6896\n",
      "Epoch [48/100], Step [2200/2266], Loss: 0.4572\n",
      "Epoch [49/100], Step [100/2266], Loss: 0.2919\n",
      "Epoch [49/100], Step [200/2266], Loss: 0.0423\n",
      "Epoch [49/100], Step [300/2266], Loss: 0.2951\n",
      "Epoch [49/100], Step [400/2266], Loss: 0.5270\n",
      "Epoch [49/100], Step [500/2266], Loss: 0.4115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/100], Step [600/2266], Loss: 0.1288\n",
      "Epoch [49/100], Step [700/2266], Loss: 0.4195\n",
      "Epoch [49/100], Step [800/2266], Loss: 0.2158\n",
      "Epoch [49/100], Step [900/2266], Loss: 0.3396\n",
      "Epoch [49/100], Step [1000/2266], Loss: 0.5347\n",
      "Epoch [49/100], Step [1100/2266], Loss: 0.2226\n",
      "Epoch [49/100], Step [1200/2266], Loss: 0.8549\n",
      "Epoch [49/100], Step [1300/2266], Loss: 0.4514\n",
      "Epoch [49/100], Step [1400/2266], Loss: 0.2964\n",
      "Epoch [49/100], Step [1500/2266], Loss: 0.2508\n",
      "Epoch [49/100], Step [1600/2266], Loss: 0.2161\n",
      "Epoch [49/100], Step [1700/2266], Loss: 0.5071\n",
      "Epoch [49/100], Step [1800/2266], Loss: 0.3226\n",
      "Epoch [49/100], Step [1900/2266], Loss: 0.6792\n",
      "Epoch [49/100], Step [2000/2266], Loss: 0.4418\n",
      "Epoch [49/100], Step [2100/2266], Loss: 0.3095\n",
      "Epoch [49/100], Step [2200/2266], Loss: 0.5891\n",
      "Epoch [50/100], Step [100/2266], Loss: 0.5508\n",
      "Epoch [50/100], Step [200/2266], Loss: 0.4556\n",
      "Epoch [50/100], Step [300/2266], Loss: 0.4479\n",
      "Epoch [50/100], Step [400/2266], Loss: 0.2951\n",
      "Epoch [50/100], Step [500/2266], Loss: 0.0886\n",
      "Epoch [50/100], Step [600/2266], Loss: 0.1734\n",
      "Epoch [50/100], Step [700/2266], Loss: 0.6490\n",
      "Epoch [50/100], Step [800/2266], Loss: 0.3232\n",
      "Epoch [50/100], Step [900/2266], Loss: 0.5166\n",
      "Epoch [50/100], Step [1000/2266], Loss: 0.1200\n",
      "Epoch [50/100], Step [1100/2266], Loss: 0.2010\n",
      "Epoch [50/100], Step [1200/2266], Loss: 0.0631\n",
      "Epoch [50/100], Step [1300/2266], Loss: 0.1638\n",
      "Epoch [50/100], Step [1400/2266], Loss: 0.2328\n",
      "Epoch [50/100], Step [1500/2266], Loss: 0.6617\n",
      "Epoch [50/100], Step [1600/2266], Loss: 0.4663\n",
      "Epoch [50/100], Step [1700/2266], Loss: 0.3938\n",
      "Epoch [50/100], Step [1800/2266], Loss: 0.2409\n",
      "Epoch [50/100], Step [1900/2266], Loss: 0.0737\n",
      "Epoch [50/100], Step [2000/2266], Loss: 0.3350\n",
      "Epoch [50/100], Step [2100/2266], Loss: 0.3814\n",
      "Epoch [50/100], Step [2200/2266], Loss: 0.2304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82108\\AppData\\Local\\Temp/ipykernel_2480/533309440.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  feature = torch.tensor(feature).to(device=self.args.device, dtype=torch.float)\n",
      "C:\\Users\\82108\\AppData\\Local\\Temp/ipykernel_2480/533309440.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(label).to(device=self.args.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Step [1000/2266], Loss: 0.0778\n",
      "Validation Step [2000/2266], Loss: 0.3004\n",
      "Validation Loss: 0.16245259853476118339\n",
      "Epoch [51/100], Step [100/2266], Loss: 0.2577\n",
      "Epoch [51/100], Step [200/2266], Loss: 0.4458\n",
      "Epoch [51/100], Step [300/2266], Loss: 0.1823\n",
      "Epoch [51/100], Step [400/2266], Loss: 0.2152\n",
      "Epoch [51/100], Step [500/2266], Loss: 0.1837\n",
      "Epoch [51/100], Step [600/2266], Loss: 0.2286\n",
      "Epoch [51/100], Step [700/2266], Loss: 0.3032\n",
      "Epoch [51/100], Step [800/2266], Loss: 0.4561\n",
      "Epoch [51/100], Step [900/2266], Loss: 0.3839\n",
      "Epoch [51/100], Step [1000/2266], Loss: 0.2297\n",
      "Epoch [51/100], Step [1100/2266], Loss: 0.2458\n",
      "Epoch [51/100], Step [1200/2266], Loss: 0.4673\n",
      "Epoch [51/100], Step [1300/2266], Loss: 0.1292\n",
      "Epoch [51/100], Step [1400/2266], Loss: 0.8367\n",
      "Epoch [51/100], Step [1500/2266], Loss: 0.1634\n",
      "Epoch [51/100], Step [1600/2266], Loss: 0.7642\n",
      "Epoch [51/100], Step [1700/2266], Loss: 0.6980\n",
      "Epoch [51/100], Step [1800/2266], Loss: 0.5895\n",
      "Epoch [51/100], Step [1900/2266], Loss: 0.7608\n",
      "Epoch [51/100], Step [2000/2266], Loss: 0.7111\n",
      "Epoch [51/100], Step [2100/2266], Loss: 0.3487\n",
      "Epoch [51/100], Step [2200/2266], Loss: 0.6960\n",
      "Epoch [52/100], Step [100/2266], Loss: 0.3196\n",
      "Epoch [52/100], Step [200/2266], Loss: 0.2815\n",
      "Epoch [52/100], Step [300/2266], Loss: 0.2728\n",
      "Epoch [52/100], Step [400/2266], Loss: 0.3713\n",
      "Epoch [52/100], Step [500/2266], Loss: 0.1371\n",
      "Epoch [52/100], Step [600/2266], Loss: 0.5826\n",
      "Epoch [52/100], Step [700/2266], Loss: 0.2180\n",
      "Epoch [52/100], Step [800/2266], Loss: 0.5004\n",
      "Epoch [52/100], Step [900/2266], Loss: 0.2196\n",
      "Epoch [52/100], Step [1000/2266], Loss: 0.2925\n",
      "Epoch [52/100], Step [1100/2266], Loss: 0.3750\n",
      "Epoch [52/100], Step [1200/2266], Loss: 0.2960\n",
      "Epoch [52/100], Step [1300/2266], Loss: 0.2883\n",
      "Epoch [52/100], Step [1400/2266], Loss: 0.2107\n",
      "Epoch [52/100], Step [1500/2266], Loss: 0.2910\n",
      "Epoch [52/100], Step [1600/2266], Loss: 0.0791\n",
      "Epoch [52/100], Step [1700/2266], Loss: 0.4009\n",
      "Epoch [52/100], Step [1800/2266], Loss: 0.4308\n",
      "Epoch [52/100], Step [1900/2266], Loss: 0.3000\n",
      "Epoch [52/100], Step [2000/2266], Loss: 0.4939\n",
      "Epoch [52/100], Step [2100/2266], Loss: 0.1665\n",
      "Epoch [52/100], Step [2200/2266], Loss: 0.4120\n",
      "Epoch [53/100], Step [100/2266], Loss: 0.2744\n",
      "Epoch [53/100], Step [200/2266], Loss: 0.3905\n",
      "Epoch [53/100], Step [300/2266], Loss: 0.2331\n",
      "Epoch [53/100], Step [400/2266], Loss: 0.2068\n",
      "Epoch [53/100], Step [500/2266], Loss: 0.5293\n",
      "Epoch [53/100], Step [600/2266], Loss: 0.3353\n",
      "Epoch [53/100], Step [700/2266], Loss: 0.4252\n",
      "Epoch [53/100], Step [800/2266], Loss: 0.1505\n",
      "Epoch [53/100], Step [900/2266], Loss: 0.2088\n",
      "Epoch [53/100], Step [1000/2266], Loss: 0.3182\n",
      "Epoch [53/100], Step [1100/2266], Loss: 0.3097\n",
      "Epoch [53/100], Step [1200/2266], Loss: 0.3406\n",
      "Epoch [53/100], Step [1300/2266], Loss: 0.1059\n",
      "Epoch [53/100], Step [1400/2266], Loss: 0.4638\n",
      "Epoch [53/100], Step [1500/2266], Loss: 0.3848\n",
      "Epoch [53/100], Step [1600/2266], Loss: 0.3292\n",
      "Epoch [53/100], Step [1700/2266], Loss: 0.2480\n",
      "Epoch [53/100], Step [1800/2266], Loss: 0.7408\n",
      "Epoch [53/100], Step [1900/2266], Loss: 0.5531\n",
      "Epoch [53/100], Step [2000/2266], Loss: 0.3450\n",
      "Epoch [53/100], Step [2100/2266], Loss: 0.4803\n",
      "Epoch [53/100], Step [2200/2266], Loss: 0.5714\n",
      "Epoch [54/100], Step [100/2266], Loss: 0.5337\n",
      "Epoch [54/100], Step [200/2266], Loss: 0.1638\n",
      "Epoch [54/100], Step [300/2266], Loss: 0.2055\n",
      "Epoch [54/100], Step [400/2266], Loss: 0.1130\n",
      "Epoch [54/100], Step [500/2266], Loss: 0.3504\n",
      "Epoch [54/100], Step [600/2266], Loss: 0.3869\n",
      "Epoch [54/100], Step [700/2266], Loss: 0.4525\n",
      "Epoch [54/100], Step [800/2266], Loss: 0.2351\n",
      "Epoch [54/100], Step [900/2266], Loss: 0.6937\n",
      "Epoch [54/100], Step [1000/2266], Loss: 0.3297\n",
      "Epoch [54/100], Step [1100/2266], Loss: 0.1650\n",
      "Epoch [54/100], Step [1200/2266], Loss: 0.3039\n",
      "Epoch [54/100], Step [1300/2266], Loss: 0.4574\n",
      "Epoch [54/100], Step [1400/2266], Loss: 0.6149\n",
      "Epoch [54/100], Step [1500/2266], Loss: 0.4361\n",
      "Epoch [54/100], Step [1600/2266], Loss: 0.3357\n",
      "Epoch [54/100], Step [1700/2266], Loss: 0.1224\n",
      "Epoch [54/100], Step [1800/2266], Loss: 0.3162\n",
      "Epoch [54/100], Step [1900/2266], Loss: 0.4857\n",
      "Epoch [54/100], Step [2000/2266], Loss: 0.2487\n",
      "Epoch [54/100], Step [2100/2266], Loss: 0.3577\n",
      "Epoch [54/100], Step [2200/2266], Loss: 0.1508\n",
      "Epoch [55/100], Step [100/2266], Loss: 0.3898\n",
      "Epoch [55/100], Step [200/2266], Loss: 0.1700\n",
      "Epoch [55/100], Step [300/2266], Loss: 0.1639\n",
      "Epoch [55/100], Step [400/2266], Loss: 0.1847\n",
      "Epoch [55/100], Step [500/2266], Loss: 0.1165\n",
      "Epoch [55/100], Step [600/2266], Loss: 0.0980\n",
      "Epoch [55/100], Step [700/2266], Loss: 0.2284\n",
      "Epoch [55/100], Step [800/2266], Loss: 0.2726\n",
      "Epoch [55/100], Step [900/2266], Loss: 0.2111\n",
      "Epoch [55/100], Step [1000/2266], Loss: 0.6200\n",
      "Epoch [55/100], Step [1100/2266], Loss: 0.4313\n",
      "Epoch [55/100], Step [1200/2266], Loss: 0.2632\n",
      "Epoch [55/100], Step [1300/2266], Loss: 0.5739\n",
      "Epoch [55/100], Step [1400/2266], Loss: 0.5875\n",
      "Epoch [55/100], Step [1500/2266], Loss: 0.4638\n",
      "Epoch [55/100], Step [1600/2266], Loss: 0.2303\n",
      "Epoch [55/100], Step [1700/2266], Loss: 0.2893\n",
      "Epoch [55/100], Step [1800/2266], Loss: 0.6250\n",
      "Epoch [55/100], Step [1900/2266], Loss: 0.5243\n",
      "Epoch [55/100], Step [2000/2266], Loss: 0.6703\n",
      "Epoch [55/100], Step [2100/2266], Loss: 0.2955\n",
      "Epoch [55/100], Step [2200/2266], Loss: 0.2642\n",
      "Epoch [56/100], Step [100/2266], Loss: 0.4272\n",
      "Epoch [56/100], Step [200/2266], Loss: 0.3480\n",
      "Epoch [56/100], Step [300/2266], Loss: 0.0860\n",
      "Epoch [56/100], Step [400/2266], Loss: 0.4481\n",
      "Epoch [56/100], Step [500/2266], Loss: 0.4424\n",
      "Epoch [56/100], Step [600/2266], Loss: 0.5367\n",
      "Epoch [56/100], Step [700/2266], Loss: 0.3907\n",
      "Epoch [56/100], Step [800/2266], Loss: 0.3140\n",
      "Epoch [56/100], Step [900/2266], Loss: 0.3057\n",
      "Epoch [56/100], Step [1000/2266], Loss: 0.5243\n",
      "Epoch [56/100], Step [1100/2266], Loss: 0.2760\n",
      "Epoch [56/100], Step [1200/2266], Loss: 0.3528\n",
      "Epoch [56/100], Step [1300/2266], Loss: 0.3987\n",
      "Epoch [56/100], Step [1400/2266], Loss: 0.6364\n",
      "Epoch [56/100], Step [1500/2266], Loss: 0.3919\n",
      "Epoch [56/100], Step [1600/2266], Loss: 0.3954\n",
      "Epoch [56/100], Step [1700/2266], Loss: 0.5437\n",
      "Epoch [56/100], Step [1800/2266], Loss: 0.2387\n",
      "Epoch [56/100], Step [1900/2266], Loss: 0.5348\n",
      "Epoch [56/100], Step [2000/2266], Loss: 0.5797\n",
      "Epoch [56/100], Step [2100/2266], Loss: 0.3304\n",
      "Epoch [56/100], Step [2200/2266], Loss: 0.1546\n",
      "Epoch [57/100], Step [100/2266], Loss: 0.1450\n",
      "Epoch [57/100], Step [200/2266], Loss: 0.0451\n",
      "Epoch [57/100], Step [300/2266], Loss: 0.5606\n",
      "Epoch [57/100], Step [400/2266], Loss: 0.3159\n",
      "Epoch [57/100], Step [500/2266], Loss: 0.4845\n",
      "Epoch [57/100], Step [600/2266], Loss: 0.4279\n",
      "Epoch [57/100], Step [700/2266], Loss: 0.2420\n",
      "Epoch [57/100], Step [800/2266], Loss: 0.1571\n",
      "Epoch [57/100], Step [900/2266], Loss: 0.2014\n",
      "Epoch [57/100], Step [1000/2266], Loss: 0.2396\n",
      "Epoch [57/100], Step [1100/2266], Loss: 0.1059\n",
      "Epoch [57/100], Step [1200/2266], Loss: 0.2099\n",
      "Epoch [57/100], Step [1300/2266], Loss: 0.2403\n",
      "Epoch [57/100], Step [1400/2266], Loss: 0.8159\n",
      "Epoch [57/100], Step [1500/2266], Loss: 0.5145\n",
      "Epoch [57/100], Step [1600/2266], Loss: 0.3253\n",
      "Epoch [57/100], Step [1700/2266], Loss: 0.5489\n",
      "Epoch [57/100], Step [1800/2266], Loss: 0.1918\n",
      "Epoch [57/100], Step [1900/2266], Loss: 0.5755\n",
      "Epoch [57/100], Step [2000/2266], Loss: 0.3612\n",
      "Epoch [57/100], Step [2100/2266], Loss: 0.4964\n",
      "Epoch [57/100], Step [2200/2266], Loss: 0.4907\n",
      "Epoch [58/100], Step [100/2266], Loss: 0.1167\n",
      "Epoch [58/100], Step [200/2266], Loss: 0.0998\n",
      "Epoch [58/100], Step [300/2266], Loss: 0.1688\n",
      "Epoch [58/100], Step [400/2266], Loss: 0.1681\n",
      "Epoch [58/100], Step [500/2266], Loss: 0.2584\n",
      "Epoch [58/100], Step [600/2266], Loss: 0.4166\n",
      "Epoch [58/100], Step [700/2266], Loss: 0.3856\n",
      "Epoch [58/100], Step [800/2266], Loss: 0.3854\n",
      "Epoch [58/100], Step [900/2266], Loss: 0.4190\n",
      "Epoch [58/100], Step [1000/2266], Loss: 0.6464\n",
      "Epoch [58/100], Step [1100/2266], Loss: 0.0775\n",
      "Epoch [58/100], Step [1200/2266], Loss: 0.6167\n",
      "Epoch [58/100], Step [1300/2266], Loss: 0.2354\n",
      "Epoch [58/100], Step [1400/2266], Loss: 0.1563\n",
      "Epoch [58/100], Step [1500/2266], Loss: 0.2010\n",
      "Epoch [58/100], Step [1600/2266], Loss: 0.3772\n",
      "Epoch [58/100], Step [1700/2266], Loss: 0.7028\n",
      "Epoch [58/100], Step [1800/2266], Loss: 0.5548\n",
      "Epoch [58/100], Step [1900/2266], Loss: 0.4069\n",
      "Epoch [58/100], Step [2000/2266], Loss: 0.5046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [58/100], Step [2100/2266], Loss: 0.0534\n",
      "Epoch [58/100], Step [2200/2266], Loss: 0.3259\n",
      "Epoch [59/100], Step [100/2266], Loss: 0.3374\n",
      "Epoch [59/100], Step [200/2266], Loss: 0.4002\n",
      "Epoch [59/100], Step [300/2266], Loss: 0.0882\n",
      "Epoch [59/100], Step [400/2266], Loss: 0.1809\n",
      "Epoch [59/100], Step [500/2266], Loss: 0.0958\n",
      "Epoch [59/100], Step [600/2266], Loss: 0.2232\n",
      "Epoch [59/100], Step [700/2266], Loss: 0.3442\n",
      "Epoch [59/100], Step [800/2266], Loss: 0.2947\n",
      "Epoch [59/100], Step [900/2266], Loss: 0.4640\n",
      "Epoch [59/100], Step [1000/2266], Loss: 0.3383\n",
      "Epoch [59/100], Step [1100/2266], Loss: 0.3890\n",
      "Epoch [59/100], Step [1200/2266], Loss: 0.0620\n",
      "Epoch [59/100], Step [1300/2266], Loss: 0.2358\n",
      "Epoch [59/100], Step [1400/2266], Loss: 0.3382\n",
      "Epoch [59/100], Step [1500/2266], Loss: 0.2098\n",
      "Epoch [59/100], Step [1600/2266], Loss: 0.2395\n",
      "Epoch [59/100], Step [1700/2266], Loss: 0.2175\n",
      "Epoch [59/100], Step [1800/2266], Loss: 0.3900\n",
      "Epoch [59/100], Step [1900/2266], Loss: 0.3094\n",
      "Epoch [59/100], Step [2000/2266], Loss: 0.5442\n",
      "Epoch [59/100], Step [2100/2266], Loss: 0.2252\n",
      "Epoch [59/100], Step [2200/2266], Loss: 0.2877\n",
      "Epoch [60/100], Step [100/2266], Loss: 0.2460\n",
      "Epoch [60/100], Step [200/2266], Loss: 0.1394\n",
      "Epoch [60/100], Step [300/2266], Loss: 0.5206\n",
      "Epoch [60/100], Step [400/2266], Loss: 0.2586\n",
      "Epoch [60/100], Step [500/2266], Loss: 0.1622\n",
      "Epoch [60/100], Step [600/2266], Loss: 0.2366\n",
      "Epoch [60/100], Step [700/2266], Loss: 0.4646\n",
      "Epoch [60/100], Step [800/2266], Loss: 0.4245\n",
      "Epoch [60/100], Step [900/2266], Loss: 0.4805\n",
      "Epoch [60/100], Step [1000/2266], Loss: 0.4790\n",
      "Epoch [60/100], Step [1100/2266], Loss: 0.2749\n",
      "Epoch [60/100], Step [1200/2266], Loss: 0.3762\n",
      "Epoch [60/100], Step [1300/2266], Loss: 0.3427\n",
      "Epoch [60/100], Step [1400/2266], Loss: 0.4111\n",
      "Epoch [60/100], Step [1500/2266], Loss: 0.4803\n",
      "Epoch [60/100], Step [1600/2266], Loss: 0.2971\n",
      "Epoch [60/100], Step [1700/2266], Loss: 0.2388\n",
      "Epoch [60/100], Step [1800/2266], Loss: 0.5993\n",
      "Epoch [60/100], Step [1900/2266], Loss: 0.2205\n",
      "Epoch [60/100], Step [2000/2266], Loss: 0.4036\n",
      "Epoch [60/100], Step [2100/2266], Loss: 0.4177\n",
      "Epoch [60/100], Step [2200/2266], Loss: 0.3197\n",
      "Epoch [61/100], Step [100/2266], Loss: 0.3823\n",
      "Epoch [61/100], Step [200/2266], Loss: 0.0999\n",
      "Epoch [61/100], Step [300/2266], Loss: 0.2245\n",
      "Epoch [61/100], Step [400/2266], Loss: 0.3607\n",
      "Epoch [61/100], Step [500/2266], Loss: 0.4226\n",
      "Epoch [61/100], Step [600/2266], Loss: 0.4963\n",
      "Epoch [61/100], Step [700/2266], Loss: 0.2122\n",
      "Epoch [61/100], Step [800/2266], Loss: 0.3445\n",
      "Epoch [61/100], Step [900/2266], Loss: 0.1664\n",
      "Epoch [61/100], Step [1000/2266], Loss: 0.1125\n",
      "Epoch [61/100], Step [1100/2266], Loss: 0.1815\n",
      "Epoch [61/100], Step [1200/2266], Loss: 0.2402\n",
      "Epoch [61/100], Step [1300/2266], Loss: 0.7773\n",
      "Epoch [61/100], Step [1400/2266], Loss: 0.0921\n",
      "Epoch [61/100], Step [1500/2266], Loss: 0.1612\n",
      "Epoch [61/100], Step [1600/2266], Loss: 0.5003\n",
      "Epoch [61/100], Step [1700/2266], Loss: 0.4448\n",
      "Epoch [61/100], Step [1800/2266], Loss: 0.3551\n",
      "Epoch [61/100], Step [1900/2266], Loss: 0.3137\n",
      "Epoch [61/100], Step [2000/2266], Loss: 0.2398\n",
      "Epoch [61/100], Step [2100/2266], Loss: 0.2231\n",
      "Epoch [61/100], Step [2200/2266], Loss: 0.3222\n",
      "Epoch [62/100], Step [100/2266], Loss: 0.1898\n",
      "Epoch [62/100], Step [200/2266], Loss: 0.5653\n",
      "Epoch [62/100], Step [300/2266], Loss: 0.2301\n",
      "Epoch [62/100], Step [400/2266], Loss: 0.0609\n",
      "Epoch [62/100], Step [500/2266], Loss: 0.3051\n",
      "Epoch [62/100], Step [600/2266], Loss: 0.3352\n",
      "Epoch [62/100], Step [700/2266], Loss: 0.2083\n",
      "Epoch [62/100], Step [800/2266], Loss: 0.4697\n",
      "Epoch [62/100], Step [900/2266], Loss: 0.4156\n",
      "Epoch [62/100], Step [1000/2266], Loss: 0.5058\n",
      "Epoch [62/100], Step [1100/2266], Loss: 0.1981\n",
      "Epoch [62/100], Step [1200/2266], Loss: 0.3597\n",
      "Epoch [62/100], Step [1300/2266], Loss: 0.1382\n",
      "Epoch [62/100], Step [1400/2266], Loss: 0.2521\n",
      "Epoch [62/100], Step [1500/2266], Loss: 0.0748\n",
      "Epoch [62/100], Step [1600/2266], Loss: 0.5348\n",
      "Epoch [62/100], Step [1700/2266], Loss: 0.3279\n",
      "Epoch [62/100], Step [1800/2266], Loss: 0.6630\n",
      "Epoch [62/100], Step [1900/2266], Loss: 0.2376\n",
      "Epoch [62/100], Step [2000/2266], Loss: 0.3948\n",
      "Epoch [62/100], Step [2100/2266], Loss: 0.2423\n",
      "Epoch [62/100], Step [2200/2266], Loss: 0.2499\n",
      "Epoch [63/100], Step [100/2266], Loss: 0.0643\n",
      "Epoch [63/100], Step [200/2266], Loss: 0.2102\n",
      "Epoch [63/100], Step [300/2266], Loss: 0.2653\n",
      "Epoch [63/100], Step [400/2266], Loss: 0.2688\n",
      "Epoch [63/100], Step [500/2266], Loss: 0.1618\n",
      "Epoch [63/100], Step [600/2266], Loss: 0.4942\n",
      "Epoch [63/100], Step [700/2266], Loss: 0.2955\n",
      "Epoch [63/100], Step [800/2266], Loss: 0.0792\n",
      "Epoch [63/100], Step [900/2266], Loss: 0.4541\n",
      "Epoch [63/100], Step [1000/2266], Loss: 0.2995\n",
      "Epoch [63/100], Step [1100/2266], Loss: 0.1252\n",
      "Epoch [63/100], Step [1200/2266], Loss: 0.2902\n",
      "Epoch [63/100], Step [1300/2266], Loss: 0.1740\n",
      "Epoch [63/100], Step [1400/2266], Loss: 0.5281\n",
      "Epoch [63/100], Step [1500/2266], Loss: 0.4124\n",
      "Epoch [63/100], Step [1600/2266], Loss: 0.4089\n",
      "Epoch [63/100], Step [1700/2266], Loss: 0.0254\n",
      "Epoch [63/100], Step [1800/2266], Loss: 0.3483\n",
      "Epoch [63/100], Step [1900/2266], Loss: 0.1689\n",
      "Epoch [63/100], Step [2000/2266], Loss: 0.2079\n",
      "Epoch [63/100], Step [2100/2266], Loss: 0.4000\n",
      "Epoch [63/100], Step [2200/2266], Loss: 0.2671\n",
      "Epoch [64/100], Step [100/2266], Loss: 0.1777\n",
      "Epoch [64/100], Step [200/2266], Loss: 0.2039\n",
      "Epoch [64/100], Step [300/2266], Loss: 0.1421\n",
      "Epoch [64/100], Step [400/2266], Loss: 0.1422\n",
      "Epoch [64/100], Step [500/2266], Loss: 0.1740\n",
      "Epoch [64/100], Step [600/2266], Loss: 0.1108\n",
      "Epoch [64/100], Step [700/2266], Loss: 0.1955\n",
      "Epoch [64/100], Step [800/2266], Loss: 0.2736\n",
      "Epoch [64/100], Step [900/2266], Loss: 0.3324\n",
      "Epoch [64/100], Step [1000/2266], Loss: 0.1397\n",
      "Epoch [64/100], Step [1100/2266], Loss: 0.5205\n",
      "Epoch [64/100], Step [1200/2266], Loss: 0.2944\n",
      "Epoch [64/100], Step [1300/2266], Loss: 0.4318\n",
      "Epoch [64/100], Step [1400/2266], Loss: 0.4700\n",
      "Epoch [64/100], Step [1500/2266], Loss: 0.0858\n",
      "Epoch [64/100], Step [1600/2266], Loss: 0.3497\n",
      "Epoch [64/100], Step [1700/2266], Loss: 0.4395\n",
      "Epoch [64/100], Step [1800/2266], Loss: 0.3872\n",
      "Epoch [64/100], Step [1900/2266], Loss: 0.2485\n",
      "Epoch [64/100], Step [2000/2266], Loss: 0.1245\n",
      "Epoch [64/100], Step [2100/2266], Loss: 0.5116\n",
      "Epoch [64/100], Step [2200/2266], Loss: 0.6283\n",
      "Epoch [65/100], Step [100/2266], Loss: 0.3336\n",
      "Epoch [65/100], Step [200/2266], Loss: 0.1620\n",
      "Epoch [65/100], Step [300/2266], Loss: 0.2193\n",
      "Epoch [65/100], Step [400/2266], Loss: 0.0463\n",
      "Epoch [65/100], Step [500/2266], Loss: 0.1297\n",
      "Epoch [65/100], Step [600/2266], Loss: 0.1022\n",
      "Epoch [65/100], Step [700/2266], Loss: 0.2120\n",
      "Epoch [65/100], Step [800/2266], Loss: 0.1492\n",
      "Epoch [65/100], Step [900/2266], Loss: 0.3030\n",
      "Epoch [65/100], Step [1000/2266], Loss: 0.1765\n",
      "Epoch [65/100], Step [1100/2266], Loss: 0.1729\n",
      "Epoch [65/100], Step [1200/2266], Loss: 0.3176\n",
      "Epoch [65/100], Step [1300/2266], Loss: 0.2373\n",
      "Epoch [65/100], Step [1400/2266], Loss: 0.2799\n",
      "Epoch [65/100], Step [1500/2266], Loss: 0.2875\n",
      "Epoch [65/100], Step [1600/2266], Loss: 0.1644\n",
      "Epoch [65/100], Step [1700/2266], Loss: 0.2797\n",
      "Epoch [65/100], Step [1800/2266], Loss: 0.3390\n",
      "Epoch [65/100], Step [1900/2266], Loss: 0.3936\n",
      "Epoch [65/100], Step [2000/2266], Loss: 0.1531\n",
      "Epoch [65/100], Step [2100/2266], Loss: 0.3009\n",
      "Epoch [65/100], Step [2200/2266], Loss: 0.2513\n",
      "Epoch [66/100], Step [100/2266], Loss: 0.1917\n",
      "Epoch [66/100], Step [200/2266], Loss: 0.3909\n",
      "Epoch [66/100], Step [300/2266], Loss: 0.2255\n",
      "Epoch [66/100], Step [400/2266], Loss: 0.3350\n",
      "Epoch [66/100], Step [500/2266], Loss: 0.3945\n",
      "Epoch [66/100], Step [600/2266], Loss: 0.2772\n",
      "Epoch [66/100], Step [700/2266], Loss: 0.1692\n",
      "Epoch [66/100], Step [800/2266], Loss: 0.0823\n",
      "Epoch [66/100], Step [900/2266], Loss: 0.1526\n",
      "Epoch [66/100], Step [1000/2266], Loss: 0.1835\n",
      "Epoch [66/100], Step [1100/2266], Loss: 0.3439\n",
      "Epoch [66/100], Step [1200/2266], Loss: 0.0542\n",
      "Epoch [66/100], Step [1300/2266], Loss: 0.1114\n",
      "Epoch [66/100], Step [1400/2266], Loss: 0.4891\n",
      "Epoch [66/100], Step [1500/2266], Loss: 0.1288\n",
      "Epoch [66/100], Step [1600/2266], Loss: 0.1937\n",
      "Epoch [66/100], Step [1700/2266], Loss: 0.3603\n",
      "Epoch [66/100], Step [1800/2266], Loss: 0.2846\n",
      "Epoch [66/100], Step [1900/2266], Loss: 0.3333\n",
      "Epoch [66/100], Step [2000/2266], Loss: 0.6202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [66/100], Step [2100/2266], Loss: 0.4790\n",
      "Epoch [66/100], Step [2200/2266], Loss: 0.8120\n",
      "Epoch [67/100], Step [100/2266], Loss: 0.1353\n",
      "Epoch [67/100], Step [200/2266], Loss: 0.4107\n",
      "Epoch [67/100], Step [300/2266], Loss: 0.4209\n",
      "Epoch [67/100], Step [400/2266], Loss: 0.2645\n",
      "Epoch [67/100], Step [500/2266], Loss: 0.2627\n",
      "Epoch [67/100], Step [600/2266], Loss: 0.3024\n",
      "Epoch [67/100], Step [700/2266], Loss: 0.0751\n",
      "Epoch [67/100], Step [800/2266], Loss: 0.4723\n",
      "Epoch [67/100], Step [900/2266], Loss: 0.4467\n",
      "Epoch [67/100], Step [1000/2266], Loss: 0.1789\n",
      "Epoch [67/100], Step [1100/2266], Loss: 0.1327\n",
      "Epoch [67/100], Step [1200/2266], Loss: 0.2914\n",
      "Epoch [67/100], Step [1300/2266], Loss: 0.1353\n",
      "Epoch [67/100], Step [1400/2266], Loss: 0.6166\n",
      "Epoch [67/100], Step [1500/2266], Loss: 0.5967\n",
      "Epoch [67/100], Step [1600/2266], Loss: 0.4835\n",
      "Epoch [67/100], Step [1700/2266], Loss: 0.2494\n",
      "Epoch [67/100], Step [1800/2266], Loss: 0.2635\n",
      "Epoch [67/100], Step [1900/2266], Loss: 0.4237\n",
      "Epoch [67/100], Step [2000/2266], Loss: 0.5032\n",
      "Epoch [67/100], Step [2100/2266], Loss: 0.3806\n",
      "Epoch [67/100], Step [2200/2266], Loss: 0.1554\n",
      "Epoch [68/100], Step [100/2266], Loss: 0.1386\n",
      "Epoch [68/100], Step [200/2266], Loss: 0.1055\n",
      "Epoch [68/100], Step [300/2266], Loss: 0.1031\n",
      "Epoch [68/100], Step [400/2266], Loss: 0.1005\n",
      "Epoch [68/100], Step [500/2266], Loss: 0.0820\n",
      "Epoch [68/100], Step [600/2266], Loss: 0.2408\n",
      "Epoch [68/100], Step [700/2266], Loss: 0.3187\n",
      "Epoch [68/100], Step [800/2266], Loss: 0.2510\n",
      "Epoch [68/100], Step [900/2266], Loss: 0.7974\n",
      "Epoch [68/100], Step [1000/2266], Loss: 0.1450\n",
      "Epoch [68/100], Step [1100/2266], Loss: 0.2142\n",
      "Epoch [68/100], Step [1200/2266], Loss: 0.1256\n",
      "Epoch [68/100], Step [1300/2266], Loss: 0.2583\n",
      "Epoch [68/100], Step [1400/2266], Loss: 0.2826\n",
      "Epoch [68/100], Step [1500/2266], Loss: 0.3813\n",
      "Epoch [68/100], Step [1600/2266], Loss: 0.1036\n",
      "Epoch [68/100], Step [1700/2266], Loss: 0.3145\n",
      "Epoch [68/100], Step [1800/2266], Loss: 0.4523\n",
      "Epoch [68/100], Step [1900/2266], Loss: 0.2336\n",
      "Epoch [68/100], Step [2000/2266], Loss: 0.0912\n",
      "Epoch [68/100], Step [2100/2266], Loss: 0.1575\n",
      "Epoch [68/100], Step [2200/2266], Loss: 0.2654\n",
      "Epoch [69/100], Step [100/2266], Loss: 0.0350\n",
      "Epoch [69/100], Step [200/2266], Loss: 0.6688\n",
      "Epoch [69/100], Step [300/2266], Loss: 0.0755\n",
      "Epoch [69/100], Step [400/2266], Loss: 0.0479\n",
      "Epoch [69/100], Step [500/2266], Loss: 0.2184\n",
      "Epoch [69/100], Step [600/2266], Loss: 0.1919\n",
      "Epoch [69/100], Step [700/2266], Loss: 0.4529\n",
      "Epoch [69/100], Step [800/2266], Loss: 0.0906\n",
      "Epoch [69/100], Step [900/2266], Loss: 0.3178\n",
      "Epoch [69/100], Step [1000/2266], Loss: 0.5498\n",
      "Epoch [69/100], Step [1100/2266], Loss: 0.2118\n",
      "Epoch [69/100], Step [1200/2266], Loss: 0.1421\n",
      "Epoch [69/100], Step [1300/2266], Loss: 0.1770\n",
      "Epoch [69/100], Step [1400/2266], Loss: 0.3742\n",
      "Epoch [69/100], Step [1500/2266], Loss: 0.2091\n",
      "Epoch [69/100], Step [1600/2266], Loss: 0.2974\n",
      "Epoch [69/100], Step [1700/2266], Loss: 0.1357\n",
      "Epoch [69/100], Step [1800/2266], Loss: 0.2050\n",
      "Epoch [69/100], Step [1900/2266], Loss: 0.2345\n",
      "Epoch [69/100], Step [2000/2266], Loss: 0.3862\n",
      "Epoch [69/100], Step [2100/2266], Loss: 0.1905\n",
      "Epoch [69/100], Step [2200/2266], Loss: 0.2765\n",
      "Epoch [70/100], Step [100/2266], Loss: 0.1686\n",
      "Epoch [70/100], Step [200/2266], Loss: 0.1809\n",
      "Epoch [70/100], Step [300/2266], Loss: 0.0411\n",
      "Epoch [70/100], Step [400/2266], Loss: 0.1821\n",
      "Epoch [70/100], Step [500/2266], Loss: 0.1038\n",
      "Epoch [70/100], Step [600/2266], Loss: 0.2516\n",
      "Epoch [70/100], Step [700/2266], Loss: 0.0312\n",
      "Epoch [70/100], Step [800/2266], Loss: 0.1594\n",
      "Epoch [70/100], Step [900/2266], Loss: 0.4393\n",
      "Epoch [70/100], Step [1000/2266], Loss: 0.1527\n",
      "Epoch [70/100], Step [1100/2266], Loss: 0.1596\n",
      "Epoch [70/100], Step [1200/2266], Loss: 0.4052\n",
      "Epoch [70/100], Step [1300/2266], Loss: 0.3298\n",
      "Epoch [70/100], Step [1400/2266], Loss: 0.1386\n",
      "Epoch [70/100], Step [1500/2266], Loss: 0.0760\n",
      "Epoch [70/100], Step [1600/2266], Loss: 0.7719\n",
      "Epoch [70/100], Step [1700/2266], Loss: 0.1031\n",
      "Epoch [70/100], Step [1800/2266], Loss: 0.2329\n",
      "Epoch [70/100], Step [1900/2266], Loss: 0.1550\n",
      "Epoch [70/100], Step [2000/2266], Loss: 0.1767\n",
      "Epoch [70/100], Step [2100/2266], Loss: 0.4787\n",
      "Epoch [70/100], Step [2200/2266], Loss: 0.1940\n",
      "Epoch [71/100], Step [100/2266], Loss: 0.2635\n",
      "Epoch [71/100], Step [200/2266], Loss: 0.2073\n",
      "Epoch [71/100], Step [300/2266], Loss: 0.1809\n",
      "Epoch [71/100], Step [400/2266], Loss: 0.3682\n",
      "Epoch [71/100], Step [500/2266], Loss: 0.2007\n",
      "Epoch [71/100], Step [600/2266], Loss: 0.2770\n",
      "Epoch [71/100], Step [700/2266], Loss: 0.1374\n",
      "Epoch [71/100], Step [800/2266], Loss: 0.1734\n",
      "Epoch [71/100], Step [900/2266], Loss: 0.1580\n",
      "Epoch [71/100], Step [1000/2266], Loss: 0.4563\n",
      "Epoch [71/100], Step [1100/2266], Loss: 0.0874\n",
      "Epoch [71/100], Step [1200/2266], Loss: 0.2534\n",
      "Epoch [71/100], Step [1300/2266], Loss: 0.4988\n",
      "Epoch [71/100], Step [1400/2266], Loss: 0.1025\n",
      "Epoch [71/100], Step [1500/2266], Loss: 0.4494\n",
      "Epoch [71/100], Step [1600/2266], Loss: 0.3460\n",
      "Epoch [71/100], Step [1700/2266], Loss: 0.2233\n",
      "Epoch [71/100], Step [1800/2266], Loss: 0.2375\n",
      "Epoch [71/100], Step [1900/2266], Loss: 0.4583\n",
      "Epoch [71/100], Step [2000/2266], Loss: 0.3041\n",
      "Epoch [71/100], Step [2100/2266], Loss: 0.4775\n",
      "Epoch [71/100], Step [2200/2266], Loss: 0.0864\n",
      "Epoch [72/100], Step [100/2266], Loss: 0.1567\n",
      "Epoch [72/100], Step [200/2266], Loss: 0.1290\n",
      "Epoch [72/100], Step [300/2266], Loss: 0.0625\n",
      "Epoch [72/100], Step [400/2266], Loss: 0.1210\n",
      "Epoch [72/100], Step [500/2266], Loss: 0.0902\n",
      "Epoch [72/100], Step [600/2266], Loss: 0.3273\n",
      "Epoch [72/100], Step [700/2266], Loss: 0.4575\n",
      "Epoch [72/100], Step [800/2266], Loss: 0.1250\n",
      "Epoch [72/100], Step [900/2266], Loss: 0.2801\n",
      "Epoch [72/100], Step [1000/2266], Loss: 0.2322\n",
      "Epoch [72/100], Step [1100/2266], Loss: 0.1692\n",
      "Epoch [72/100], Step [1200/2266], Loss: 0.2068\n",
      "Epoch [72/100], Step [1300/2266], Loss: 0.2995\n",
      "Epoch [72/100], Step [1400/2266], Loss: 0.2321\n",
      "Epoch [72/100], Step [1500/2266], Loss: 0.1705\n",
      "Epoch [72/100], Step [1600/2266], Loss: 0.0787\n",
      "Epoch [72/100], Step [1700/2266], Loss: 0.1800\n",
      "Epoch [72/100], Step [1800/2266], Loss: 0.1294\n",
      "Epoch [72/100], Step [1900/2266], Loss: 0.1955\n",
      "Epoch [72/100], Step [2000/2266], Loss: 0.2852\n",
      "Epoch [72/100], Step [2100/2266], Loss: 0.2490\n",
      "Epoch [72/100], Step [2200/2266], Loss: 0.2065\n",
      "Epoch [73/100], Step [100/2266], Loss: 0.2394\n",
      "Epoch [73/100], Step [200/2266], Loss: 0.1530\n",
      "Epoch [73/100], Step [300/2266], Loss: 0.0942\n",
      "Epoch [73/100], Step [400/2266], Loss: 0.3272\n",
      "Epoch [73/100], Step [500/2266], Loss: 0.1609\n",
      "Epoch [73/100], Step [600/2266], Loss: 0.2870\n",
      "Epoch [73/100], Step [700/2266], Loss: 0.3592\n",
      "Epoch [73/100], Step [800/2266], Loss: 0.1081\n",
      "Epoch [73/100], Step [900/2266], Loss: 0.6061\n",
      "Epoch [73/100], Step [1000/2266], Loss: 0.1027\n",
      "Epoch [73/100], Step [1100/2266], Loss: 0.4581\n",
      "Epoch [73/100], Step [1200/2266], Loss: 0.0515\n",
      "Epoch [73/100], Step [1300/2266], Loss: 0.1803\n",
      "Epoch [73/100], Step [1400/2266], Loss: 0.5389\n",
      "Epoch [73/100], Step [1500/2266], Loss: 0.1648\n",
      "Epoch [73/100], Step [1600/2266], Loss: 0.3249\n",
      "Epoch [73/100], Step [1700/2266], Loss: 0.2266\n",
      "Epoch [73/100], Step [1800/2266], Loss: 0.0904\n",
      "Epoch [73/100], Step [1900/2266], Loss: 0.3588\n",
      "Epoch [73/100], Step [2000/2266], Loss: 0.1323\n",
      "Epoch [73/100], Step [2100/2266], Loss: 0.3199\n",
      "Epoch [73/100], Step [2200/2266], Loss: 0.6469\n",
      "Epoch [74/100], Step [100/2266], Loss: 0.0984\n",
      "Epoch [74/100], Step [200/2266], Loss: 0.4083\n",
      "Epoch [74/100], Step [300/2266], Loss: 0.3054\n",
      "Epoch [74/100], Step [400/2266], Loss: 0.3545\n",
      "Epoch [74/100], Step [500/2266], Loss: 0.2296\n",
      "Epoch [74/100], Step [600/2266], Loss: 0.0658\n",
      "Epoch [74/100], Step [700/2266], Loss: 0.0970\n",
      "Epoch [74/100], Step [800/2266], Loss: 0.3761\n",
      "Epoch [74/100], Step [900/2266], Loss: 0.1588\n",
      "Epoch [74/100], Step [1000/2266], Loss: 0.1022\n",
      "Epoch [74/100], Step [1100/2266], Loss: 0.1588\n",
      "Epoch [74/100], Step [1200/2266], Loss: 0.3224\n",
      "Epoch [74/100], Step [1300/2266], Loss: 0.1258\n",
      "Epoch [74/100], Step [1400/2266], Loss: 0.3251\n",
      "Epoch [74/100], Step [1500/2266], Loss: 0.4491\n",
      "Epoch [74/100], Step [1600/2266], Loss: 0.4558\n",
      "Epoch [74/100], Step [1700/2266], Loss: 0.1991\n",
      "Epoch [74/100], Step [1800/2266], Loss: 0.1819\n",
      "Epoch [74/100], Step [1900/2266], Loss: 0.1209\n",
      "Epoch [74/100], Step [2000/2266], Loss: 0.6233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [74/100], Step [2100/2266], Loss: 0.2248\n",
      "Epoch [74/100], Step [2200/2266], Loss: 0.0438\n",
      "Epoch [75/100], Step [100/2266], Loss: 0.1729\n",
      "Epoch [75/100], Step [200/2266], Loss: 0.1762\n",
      "Epoch [75/100], Step [300/2266], Loss: 0.0892\n",
      "Epoch [75/100], Step [400/2266], Loss: 0.1320\n",
      "Epoch [75/100], Step [500/2266], Loss: 0.4448\n",
      "Epoch [75/100], Step [600/2266], Loss: 0.0722\n",
      "Epoch [75/100], Step [700/2266], Loss: 0.3158\n",
      "Epoch [75/100], Step [800/2266], Loss: 0.3333\n",
      "Epoch [75/100], Step [900/2266], Loss: 0.1711\n",
      "Epoch [75/100], Step [1000/2266], Loss: 0.0913\n",
      "Epoch [75/100], Step [1100/2266], Loss: 0.2058\n",
      "Epoch [75/100], Step [1200/2266], Loss: 0.1760\n",
      "Epoch [75/100], Step [1300/2266], Loss: 0.2706\n",
      "Epoch [75/100], Step [1400/2266], Loss: 0.3185\n",
      "Epoch [75/100], Step [1500/2266], Loss: 0.2923\n",
      "Epoch [75/100], Step [1600/2266], Loss: 0.3013\n",
      "Epoch [75/100], Step [1700/2266], Loss: 0.3101\n",
      "Epoch [75/100], Step [1800/2266], Loss: 0.1844\n",
      "Epoch [75/100], Step [1900/2266], Loss: 0.1720\n",
      "Epoch [75/100], Step [2000/2266], Loss: 0.4954\n",
      "Epoch [75/100], Step [2100/2266], Loss: 0.2128\n",
      "Epoch [75/100], Step [2200/2266], Loss: 0.4419\n",
      "Epoch [76/100], Step [100/2266], Loss: 0.4777\n",
      "Epoch [76/100], Step [200/2266], Loss: 0.1514\n",
      "Epoch [76/100], Step [300/2266], Loss: 0.1812\n",
      "Epoch [76/100], Step [400/2266], Loss: 0.2282\n",
      "Epoch [76/100], Step [500/2266], Loss: 0.2302\n",
      "Epoch [76/100], Step [600/2266], Loss: 0.3496\n",
      "Epoch [76/100], Step [700/2266], Loss: 0.1062\n",
      "Epoch [76/100], Step [800/2266], Loss: 0.1177\n",
      "Epoch [76/100], Step [900/2266], Loss: 0.0637\n",
      "Epoch [76/100], Step [1000/2266], Loss: 0.2583\n",
      "Epoch [76/100], Step [1100/2266], Loss: 0.1965\n",
      "Epoch [76/100], Step [1200/2266], Loss: 0.2262\n",
      "Epoch [76/100], Step [1300/2266], Loss: 0.1190\n",
      "Epoch [76/100], Step [1400/2266], Loss: 0.3030\n",
      "Epoch [76/100], Step [1500/2266], Loss: 0.2438\n",
      "Epoch [76/100], Step [1600/2266], Loss: 0.0679\n",
      "Epoch [76/100], Step [1700/2266], Loss: 0.2193\n",
      "Epoch [76/100], Step [1800/2266], Loss: 0.1758\n",
      "Epoch [76/100], Step [1900/2266], Loss: 0.3775\n",
      "Epoch [76/100], Step [2000/2266], Loss: 0.2157\n",
      "Epoch [76/100], Step [2100/2266], Loss: 0.2232\n",
      "Epoch [76/100], Step [2200/2266], Loss: 0.5442\n",
      "Epoch [77/100], Step [100/2266], Loss: 0.2899\n",
      "Epoch [77/100], Step [200/2266], Loss: 0.0867\n",
      "Epoch [77/100], Step [300/2266], Loss: 0.3224\n",
      "Epoch [77/100], Step [400/2266], Loss: 0.1694\n",
      "Epoch [77/100], Step [500/2266], Loss: 0.1043\n",
      "Epoch [77/100], Step [600/2266], Loss: 0.1447\n",
      "Epoch [77/100], Step [700/2266], Loss: 0.0473\n",
      "Epoch [77/100], Step [800/2266], Loss: 0.0738\n",
      "Epoch [77/100], Step [900/2266], Loss: 0.2161\n",
      "Epoch [77/100], Step [1000/2266], Loss: 0.1547\n",
      "Epoch [77/100], Step [1100/2266], Loss: 0.1334\n",
      "Epoch [77/100], Step [1200/2266], Loss: 0.2516\n",
      "Epoch [77/100], Step [1300/2266], Loss: 0.1475\n",
      "Epoch [77/100], Step [1400/2266], Loss: 0.1858\n",
      "Epoch [77/100], Step [1500/2266], Loss: 0.2073\n",
      "Epoch [77/100], Step [1600/2266], Loss: 0.5945\n",
      "Epoch [77/100], Step [1700/2266], Loss: 0.1354\n",
      "Epoch [77/100], Step [1800/2266], Loss: 0.5045\n",
      "Epoch [77/100], Step [1900/2266], Loss: 0.0687\n",
      "Epoch [77/100], Step [2000/2266], Loss: 0.1910\n",
      "Epoch [77/100], Step [2100/2266], Loss: 0.3166\n",
      "Epoch [77/100], Step [2200/2266], Loss: 0.3007\n",
      "Epoch [78/100], Step [100/2266], Loss: 0.1626\n",
      "Epoch [78/100], Step [200/2266], Loss: 0.2017\n",
      "Epoch [78/100], Step [300/2266], Loss: 0.1437\n",
      "Epoch [78/100], Step [400/2266], Loss: 0.2840\n",
      "Epoch [78/100], Step [500/2266], Loss: 0.3181\n",
      "Epoch [78/100], Step [600/2266], Loss: 0.2430\n",
      "Epoch [78/100], Step [700/2266], Loss: 0.2490\n",
      "Epoch [78/100], Step [800/2266], Loss: 0.5546\n",
      "Epoch [78/100], Step [900/2266], Loss: 0.2028\n",
      "Epoch [78/100], Step [1000/2266], Loss: 0.2701\n",
      "Epoch [78/100], Step [1100/2266], Loss: 0.0876\n",
      "Epoch [78/100], Step [1200/2266], Loss: 0.4485\n",
      "Epoch [78/100], Step [1300/2266], Loss: 0.3264\n",
      "Epoch [78/100], Step [1400/2266], Loss: 0.1762\n",
      "Epoch [78/100], Step [1500/2266], Loss: 0.2470\n",
      "Epoch [78/100], Step [1600/2266], Loss: 0.0706\n",
      "Epoch [78/100], Step [1700/2266], Loss: 0.4715\n",
      "Epoch [78/100], Step [1800/2266], Loss: 0.0987\n",
      "Epoch [78/100], Step [1900/2266], Loss: 0.3249\n",
      "Epoch [78/100], Step [2000/2266], Loss: 0.3818\n",
      "Epoch [78/100], Step [2100/2266], Loss: 0.5018\n",
      "Epoch [78/100], Step [2200/2266], Loss: 0.1126\n",
      "Epoch [79/100], Step [100/2266], Loss: 0.2616\n",
      "Epoch [79/100], Step [200/2266], Loss: 0.1421\n",
      "Epoch [79/100], Step [300/2266], Loss: 0.1464\n",
      "Epoch [79/100], Step [400/2266], Loss: 0.1208\n",
      "Epoch [79/100], Step [500/2266], Loss: 0.0993\n",
      "Epoch [79/100], Step [600/2266], Loss: 0.4205\n",
      "Epoch [79/100], Step [700/2266], Loss: 0.2457\n",
      "Epoch [79/100], Step [800/2266], Loss: 0.1923\n",
      "Epoch [79/100], Step [900/2266], Loss: 0.2624\n",
      "Epoch [79/100], Step [1000/2266], Loss: 0.0383\n",
      "Epoch [79/100], Step [1100/2266], Loss: 0.4230\n",
      "Epoch [79/100], Step [1200/2266], Loss: 0.2726\n",
      "Epoch [79/100], Step [1300/2266], Loss: 0.0305\n",
      "Epoch [79/100], Step [1400/2266], Loss: 0.2005\n",
      "Epoch [79/100], Step [1500/2266], Loss: 0.2713\n",
      "Epoch [79/100], Step [1600/2266], Loss: 0.2489\n",
      "Epoch [79/100], Step [1700/2266], Loss: 0.2021\n",
      "Epoch [79/100], Step [1800/2266], Loss: 0.4520\n",
      "Epoch [79/100], Step [1900/2266], Loss: 0.1584\n",
      "Epoch [79/100], Step [2000/2266], Loss: 0.3195\n",
      "Epoch [79/100], Step [2100/2266], Loss: 0.2274\n",
      "Epoch [79/100], Step [2200/2266], Loss: 0.4953\n",
      "Epoch [80/100], Step [100/2266], Loss: 0.2931\n",
      "Epoch [80/100], Step [200/2266], Loss: 0.1576\n",
      "Epoch [80/100], Step [300/2266], Loss: 0.1155\n",
      "Epoch [80/100], Step [400/2266], Loss: 0.3385\n",
      "Epoch [80/100], Step [500/2266], Loss: 0.2531\n",
      "Epoch [80/100], Step [600/2266], Loss: 0.2943\n",
      "Epoch [80/100], Step [700/2266], Loss: 0.0650\n",
      "Epoch [80/100], Step [800/2266], Loss: 0.3312\n",
      "Epoch [80/100], Step [900/2266], Loss: 0.0572\n",
      "Epoch [80/100], Step [1000/2266], Loss: 0.3515\n",
      "Epoch [80/100], Step [1100/2266], Loss: 0.1560\n",
      "Epoch [80/100], Step [1200/2266], Loss: 0.2682\n",
      "Epoch [80/100], Step [1300/2266], Loss: 0.1085\n",
      "Epoch [80/100], Step [1400/2266], Loss: 0.2569\n",
      "Epoch [80/100], Step [1500/2266], Loss: 0.1596\n",
      "Epoch [80/100], Step [1600/2266], Loss: 0.2893\n",
      "Epoch [80/100], Step [1700/2266], Loss: 0.2911\n",
      "Epoch [80/100], Step [1800/2266], Loss: 0.3188\n",
      "Epoch [80/100], Step [1900/2266], Loss: 0.0559\n",
      "Epoch [80/100], Step [2000/2266], Loss: 0.4793\n",
      "Epoch [80/100], Step [2100/2266], Loss: 0.0978\n",
      "Epoch [80/100], Step [2200/2266], Loss: 0.2286\n",
      "Epoch [81/100], Step [100/2266], Loss: 0.1692\n",
      "Epoch [81/100], Step [200/2266], Loss: 0.0988\n",
      "Epoch [81/100], Step [300/2266], Loss: 0.1267\n",
      "Epoch [81/100], Step [400/2266], Loss: 0.2465\n",
      "Epoch [81/100], Step [500/2266], Loss: 0.1972\n",
      "Epoch [81/100], Step [600/2266], Loss: 0.2642\n",
      "Epoch [81/100], Step [700/2266], Loss: 0.0950\n",
      "Epoch [81/100], Step [800/2266], Loss: 0.1493\n",
      "Epoch [81/100], Step [900/2266], Loss: 0.4594\n",
      "Epoch [81/100], Step [1000/2266], Loss: 0.0725\n",
      "Epoch [81/100], Step [1100/2266], Loss: 0.1975\n",
      "Epoch [81/100], Step [1200/2266], Loss: 0.3094\n",
      "Epoch [81/100], Step [1300/2266], Loss: 0.1270\n",
      "Epoch [81/100], Step [1400/2266], Loss: 0.1586\n",
      "Epoch [81/100], Step [1500/2266], Loss: 0.1432\n",
      "Epoch [81/100], Step [1600/2266], Loss: 0.5295\n",
      "Epoch [81/100], Step [1700/2266], Loss: 0.5067\n",
      "Epoch [81/100], Step [1800/2266], Loss: 0.2594\n",
      "Epoch [81/100], Step [1900/2266], Loss: 0.3670\n",
      "Epoch [81/100], Step [2000/2266], Loss: 0.2740\n",
      "Epoch [81/100], Step [2100/2266], Loss: 0.1302\n",
      "Epoch [81/100], Step [2200/2266], Loss: 0.3642\n",
      "Epoch [82/100], Step [100/2266], Loss: 0.2291\n",
      "Epoch [82/100], Step [200/2266], Loss: 0.3255\n",
      "Epoch [82/100], Step [300/2266], Loss: 0.1472\n",
      "Epoch [82/100], Step [400/2266], Loss: 0.6399\n",
      "Epoch [82/100], Step [500/2266], Loss: 0.3143\n",
      "Epoch [82/100], Step [600/2266], Loss: 0.0537\n",
      "Epoch [82/100], Step [700/2266], Loss: 0.4131\n",
      "Epoch [82/100], Step [800/2266], Loss: 0.1478\n",
      "Epoch [82/100], Step [900/2266], Loss: 0.3888\n",
      "Epoch [82/100], Step [1000/2266], Loss: 0.3374\n",
      "Epoch [82/100], Step [1100/2266], Loss: 0.3868\n",
      "Epoch [82/100], Step [1200/2266], Loss: 0.0993\n",
      "Epoch [82/100], Step [1300/2266], Loss: 0.0481\n",
      "Epoch [82/100], Step [1400/2266], Loss: 0.2116\n",
      "Epoch [82/100], Step [1500/2266], Loss: 0.2270\n",
      "Epoch [82/100], Step [1600/2266], Loss: 0.2201\n",
      "Epoch [82/100], Step [1700/2266], Loss: 0.5481\n",
      "Epoch [82/100], Step [1800/2266], Loss: 0.4094\n",
      "Epoch [82/100], Step [1900/2266], Loss: 0.2863\n",
      "Epoch [82/100], Step [2000/2266], Loss: 0.3039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [82/100], Step [2100/2266], Loss: 0.1267\n",
      "Epoch [82/100], Step [2200/2266], Loss: 0.0377\n",
      "Epoch [83/100], Step [100/2266], Loss: 0.1388\n",
      "Epoch [83/100], Step [200/2266], Loss: 0.0864\n",
      "Epoch [83/100], Step [300/2266], Loss: 0.3166\n",
      "Epoch [83/100], Step [400/2266], Loss: 0.2609\n",
      "Epoch [83/100], Step [500/2266], Loss: 0.2394\n",
      "Epoch [83/100], Step [600/2266], Loss: 0.1255\n",
      "Epoch [83/100], Step [700/2266], Loss: 0.1694\n",
      "Epoch [83/100], Step [800/2266], Loss: 0.3693\n",
      "Epoch [83/100], Step [900/2266], Loss: 0.0852\n",
      "Epoch [83/100], Step [1000/2266], Loss: 0.2171\n",
      "Epoch [83/100], Step [1100/2266], Loss: 0.1474\n",
      "Epoch [83/100], Step [1200/2266], Loss: 0.4387\n",
      "Epoch [83/100], Step [1300/2266], Loss: 0.3407\n",
      "Epoch [83/100], Step [1400/2266], Loss: 0.2783\n",
      "Epoch [83/100], Step [1500/2266], Loss: 0.0756\n",
      "Epoch [83/100], Step [1600/2266], Loss: 0.2577\n",
      "Epoch [83/100], Step [1700/2266], Loss: 0.4178\n",
      "Epoch [83/100], Step [1800/2266], Loss: 0.2999\n",
      "Epoch [83/100], Step [1900/2266], Loss: 0.1127\n",
      "Epoch [83/100], Step [2000/2266], Loss: 0.4638\n",
      "Epoch [83/100], Step [2100/2266], Loss: 0.1929\n",
      "Epoch [83/100], Step [2200/2266], Loss: 0.4026\n",
      "Epoch [84/100], Step [100/2266], Loss: 0.0800\n",
      "Epoch [84/100], Step [200/2266], Loss: 0.1427\n",
      "Epoch [84/100], Step [300/2266], Loss: 0.1455\n",
      "Epoch [84/100], Step [400/2266], Loss: 0.0662\n",
      "Epoch [84/100], Step [500/2266], Loss: 0.2743\n",
      "Epoch [84/100], Step [600/2266], Loss: 0.3296\n",
      "Epoch [84/100], Step [700/2266], Loss: 0.2932\n",
      "Epoch [84/100], Step [800/2266], Loss: 0.2068\n",
      "Epoch [84/100], Step [900/2266], Loss: 0.0422\n",
      "Epoch [84/100], Step [1000/2266], Loss: 0.3267\n",
      "Epoch [84/100], Step [1100/2266], Loss: 0.1094\n",
      "Epoch [84/100], Step [1200/2266], Loss: 0.2017\n",
      "Epoch [84/100], Step [1300/2266], Loss: 0.3716\n",
      "Epoch [84/100], Step [1400/2266], Loss: 0.0756\n",
      "Epoch [84/100], Step [1500/2266], Loss: 0.1836\n",
      "Epoch [84/100], Step [1600/2266], Loss: 0.1062\n",
      "Epoch [84/100], Step [1700/2266], Loss: 0.1514\n",
      "Epoch [84/100], Step [1800/2266], Loss: 0.2790\n",
      "Epoch [84/100], Step [1900/2266], Loss: 0.3063\n",
      "Epoch [84/100], Step [2000/2266], Loss: 0.2688\n",
      "Epoch [84/100], Step [2100/2266], Loss: 0.1626\n",
      "Epoch [84/100], Step [2200/2266], Loss: 0.3446\n",
      "Epoch [85/100], Step [100/2266], Loss: 0.3548\n",
      "Epoch [85/100], Step [200/2266], Loss: 0.3005\n",
      "Epoch [85/100], Step [300/2266], Loss: 0.4753\n",
      "Epoch [85/100], Step [400/2266], Loss: 0.0501\n",
      "Epoch [85/100], Step [500/2266], Loss: 0.1492\n",
      "Epoch [85/100], Step [600/2266], Loss: 0.1129\n",
      "Epoch [85/100], Step [700/2266], Loss: 0.2774\n",
      "Epoch [85/100], Step [800/2266], Loss: 0.1564\n",
      "Epoch [85/100], Step [900/2266], Loss: 0.2353\n",
      "Epoch [85/100], Step [1000/2266], Loss: 0.2717\n",
      "Epoch [85/100], Step [1100/2266], Loss: 0.0184\n",
      "Epoch [85/100], Step [1200/2266], Loss: 0.1349\n",
      "Epoch [85/100], Step [1300/2266], Loss: 0.0386\n",
      "Epoch [85/100], Step [1400/2266], Loss: 0.3734\n",
      "Epoch [85/100], Step [1500/2266], Loss: 0.1113\n",
      "Epoch [85/100], Step [1600/2266], Loss: 0.0540\n",
      "Epoch [85/100], Step [1700/2266], Loss: 0.1746\n",
      "Epoch [85/100], Step [1800/2266], Loss: 0.6213\n",
      "Epoch [85/100], Step [1900/2266], Loss: 0.1680\n",
      "Epoch [85/100], Step [2000/2266], Loss: 0.4408\n",
      "Epoch [85/100], Step [2100/2266], Loss: 0.2499\n",
      "Epoch [85/100], Step [2200/2266], Loss: 0.3609\n",
      "Epoch [86/100], Step [100/2266], Loss: 0.1729\n",
      "Epoch [86/100], Step [200/2266], Loss: 0.2491\n",
      "Epoch [86/100], Step [300/2266], Loss: 0.1526\n",
      "Epoch [86/100], Step [400/2266], Loss: 0.0373\n",
      "Epoch [86/100], Step [500/2266], Loss: 0.1731\n",
      "Epoch [86/100], Step [600/2266], Loss: 0.1602\n",
      "Epoch [86/100], Step [700/2266], Loss: 0.3523\n",
      "Epoch [86/100], Step [800/2266], Loss: 0.4908\n",
      "Epoch [86/100], Step [900/2266], Loss: 0.1862\n",
      "Epoch [86/100], Step [1000/2266], Loss: 0.2364\n",
      "Epoch [86/100], Step [1100/2266], Loss: 0.2345\n",
      "Epoch [86/100], Step [1200/2266], Loss: 0.1506\n",
      "Epoch [86/100], Step [1300/2266], Loss: 0.0388\n",
      "Epoch [86/100], Step [1400/2266], Loss: 0.3047\n",
      "Epoch [86/100], Step [1500/2266], Loss: 0.3611\n",
      "Epoch [86/100], Step [1600/2266], Loss: 0.1618\n",
      "Epoch [86/100], Step [1700/2266], Loss: 0.1094\n",
      "Epoch [86/100], Step [1800/2266], Loss: 0.2610\n",
      "Epoch [86/100], Step [1900/2266], Loss: 0.0533\n",
      "Epoch [86/100], Step [2000/2266], Loss: 0.2871\n",
      "Epoch [86/100], Step [2100/2266], Loss: 0.1892\n",
      "Epoch [86/100], Step [2200/2266], Loss: 0.0824\n",
      "Epoch [87/100], Step [100/2266], Loss: 0.2766\n",
      "Epoch [87/100], Step [200/2266], Loss: 0.1475\n",
      "Epoch [87/100], Step [300/2266], Loss: 0.3260\n",
      "Epoch [87/100], Step [400/2266], Loss: 0.3863\n",
      "Epoch [87/100], Step [500/2266], Loss: 0.2314\n",
      "Epoch [87/100], Step [600/2266], Loss: 0.0824\n",
      "Epoch [87/100], Step [700/2266], Loss: 0.0858\n",
      "Epoch [87/100], Step [800/2266], Loss: 0.1247\n",
      "Epoch [87/100], Step [900/2266], Loss: 0.2263\n",
      "Epoch [87/100], Step [1000/2266], Loss: 0.0772\n",
      "Epoch [87/100], Step [1100/2266], Loss: 0.2506\n",
      "Epoch [87/100], Step [1200/2266], Loss: 0.2753\n",
      "Epoch [87/100], Step [1300/2266], Loss: 0.1481\n",
      "Epoch [87/100], Step [1400/2266], Loss: 0.0775\n",
      "Epoch [87/100], Step [1500/2266], Loss: 0.1158\n",
      "Epoch [87/100], Step [1600/2266], Loss: 0.3387\n",
      "Epoch [87/100], Step [1700/2266], Loss: 0.1142\n",
      "Epoch [87/100], Step [1800/2266], Loss: 0.2658\n",
      "Epoch [87/100], Step [1900/2266], Loss: 0.3742\n",
      "Epoch [87/100], Step [2000/2266], Loss: 0.2657\n",
      "Epoch [87/100], Step [2100/2266], Loss: 0.1870\n",
      "Epoch [87/100], Step [2200/2266], Loss: 0.2514\n",
      "Epoch [88/100], Step [100/2266], Loss: 0.2675\n",
      "Epoch [88/100], Step [200/2266], Loss: 0.2125\n",
      "Epoch [88/100], Step [300/2266], Loss: 0.2739\n",
      "Epoch [88/100], Step [400/2266], Loss: 0.3553\n",
      "Epoch [88/100], Step [500/2266], Loss: 0.8540\n",
      "Epoch [88/100], Step [600/2266], Loss: 0.1757\n",
      "Epoch [88/100], Step [700/2266], Loss: 0.3260\n",
      "Epoch [88/100], Step [800/2266], Loss: 0.1417\n",
      "Epoch [88/100], Step [900/2266], Loss: 0.2065\n",
      "Epoch [88/100], Step [1000/2266], Loss: 0.4388\n",
      "Epoch [88/100], Step [1100/2266], Loss: 0.1017\n",
      "Epoch [88/100], Step [1200/2266], Loss: 0.0371\n",
      "Epoch [88/100], Step [1300/2266], Loss: 0.1188\n",
      "Epoch [88/100], Step [1400/2266], Loss: 0.0940\n",
      "Epoch [88/100], Step [1500/2266], Loss: 0.2572\n",
      "Epoch [88/100], Step [1600/2266], Loss: 0.0908\n",
      "Epoch [88/100], Step [1700/2266], Loss: 0.3840\n",
      "Epoch [88/100], Step [1800/2266], Loss: 0.0794\n",
      "Epoch [88/100], Step [1900/2266], Loss: 0.2148\n",
      "Epoch [88/100], Step [2000/2266], Loss: 0.2175\n",
      "Epoch [88/100], Step [2100/2266], Loss: 0.2190\n",
      "Epoch [88/100], Step [2200/2266], Loss: 0.1549\n",
      "Epoch [89/100], Step [100/2266], Loss: 0.0683\n",
      "Epoch [89/100], Step [200/2266], Loss: 0.2859\n",
      "Epoch [89/100], Step [300/2266], Loss: 0.2567\n",
      "Epoch [89/100], Step [400/2266], Loss: 0.0984\n",
      "Epoch [89/100], Step [500/2266], Loss: 0.1160\n",
      "Epoch [89/100], Step [600/2266], Loss: 0.1385\n",
      "Epoch [89/100], Step [700/2266], Loss: 0.3551\n",
      "Epoch [89/100], Step [800/2266], Loss: 0.1698\n",
      "Epoch [89/100], Step [900/2266], Loss: 0.1363\n",
      "Epoch [89/100], Step [1000/2266], Loss: 0.0664\n",
      "Epoch [89/100], Step [1100/2266], Loss: 0.6963\n",
      "Epoch [89/100], Step [1200/2266], Loss: 0.1728\n",
      "Epoch [89/100], Step [1300/2266], Loss: 0.1188\n",
      "Epoch [89/100], Step [1400/2266], Loss: 0.4779\n",
      "Epoch [89/100], Step [1500/2266], Loss: 0.0815\n",
      "Epoch [89/100], Step [1600/2266], Loss: 0.1181\n",
      "Epoch [89/100], Step [1700/2266], Loss: 0.4565\n",
      "Epoch [89/100], Step [1800/2266], Loss: 0.1107\n",
      "Epoch [89/100], Step [1900/2266], Loss: 0.0584\n",
      "Epoch [89/100], Step [2000/2266], Loss: 0.1262\n",
      "Epoch [89/100], Step [2100/2266], Loss: 0.2837\n",
      "Epoch [89/100], Step [2200/2266], Loss: 0.0508\n",
      "Epoch [90/100], Step [100/2266], Loss: 0.0674\n",
      "Epoch [90/100], Step [200/2266], Loss: 0.0585\n",
      "Epoch [90/100], Step [300/2266], Loss: 0.2249\n",
      "Epoch [90/100], Step [400/2266], Loss: 0.1249\n",
      "Epoch [90/100], Step [500/2266], Loss: 0.1680\n",
      "Epoch [90/100], Step [600/2266], Loss: 0.2308\n",
      "Epoch [90/100], Step [700/2266], Loss: 0.2643\n",
      "Epoch [90/100], Step [800/2266], Loss: 0.1071\n",
      "Epoch [90/100], Step [900/2266], Loss: 0.0512\n",
      "Epoch [90/100], Step [1000/2266], Loss: 0.2269\n",
      "Epoch [90/100], Step [1100/2266], Loss: 0.3634\n",
      "Epoch [90/100], Step [1200/2266], Loss: 0.1232\n",
      "Epoch [90/100], Step [1300/2266], Loss: 0.0776\n",
      "Epoch [90/100], Step [1400/2266], Loss: 0.3682\n",
      "Epoch [90/100], Step [1500/2266], Loss: 0.3765\n",
      "Epoch [90/100], Step [1600/2266], Loss: 0.1755\n",
      "Epoch [90/100], Step [1700/2266], Loss: 0.3490\n",
      "Epoch [90/100], Step [1800/2266], Loss: 0.2522\n",
      "Epoch [90/100], Step [1900/2266], Loss: 0.1534\n",
      "Epoch [90/100], Step [2000/2266], Loss: 0.0673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/100], Step [2100/2266], Loss: 0.3224\n",
      "Epoch [90/100], Step [2200/2266], Loss: 0.0789\n",
      "Epoch [91/100], Step [100/2266], Loss: 0.1671\n",
      "Epoch [91/100], Step [200/2266], Loss: 0.0272\n",
      "Epoch [91/100], Step [300/2266], Loss: 0.4709\n",
      "Epoch [91/100], Step [400/2266], Loss: 0.1175\n",
      "Epoch [91/100], Step [500/2266], Loss: 0.1543\n",
      "Epoch [91/100], Step [600/2266], Loss: 0.4981\n",
      "Epoch [91/100], Step [700/2266], Loss: 0.1094\n",
      "Epoch [91/100], Step [800/2266], Loss: 0.1135\n",
      "Epoch [91/100], Step [900/2266], Loss: 0.0579\n",
      "Epoch [91/100], Step [1000/2266], Loss: 0.1515\n",
      "Epoch [91/100], Step [1100/2266], Loss: 0.4032\n",
      "Epoch [91/100], Step [1200/2266], Loss: 0.2392\n",
      "Epoch [91/100], Step [1300/2266], Loss: 0.0500\n",
      "Epoch [91/100], Step [1400/2266], Loss: 0.1885\n",
      "Epoch [91/100], Step [1500/2266], Loss: 0.3855\n",
      "Epoch [91/100], Step [1600/2266], Loss: 0.4479\n",
      "Epoch [91/100], Step [1700/2266], Loss: 0.0651\n",
      "Epoch [91/100], Step [1800/2266], Loss: 0.1428\n",
      "Epoch [91/100], Step [1900/2266], Loss: 0.0710\n",
      "Epoch [91/100], Step [2000/2266], Loss: 0.6597\n",
      "Epoch [91/100], Step [2100/2266], Loss: 0.3443\n",
      "Epoch [91/100], Step [2200/2266], Loss: 0.1437\n",
      "Epoch [92/100], Step [100/2266], Loss: 0.1016\n",
      "Epoch [92/100], Step [200/2266], Loss: 0.1109\n",
      "Epoch [92/100], Step [300/2266], Loss: 0.2197\n",
      "Epoch [92/100], Step [400/2266], Loss: 0.1354\n",
      "Epoch [92/100], Step [500/2266], Loss: 0.0264\n",
      "Epoch [92/100], Step [600/2266], Loss: 0.2120\n",
      "Epoch [92/100], Step [700/2266], Loss: 0.0276\n",
      "Epoch [92/100], Step [800/2266], Loss: 0.3127\n",
      "Epoch [92/100], Step [900/2266], Loss: 0.1440\n",
      "Epoch [92/100], Step [1000/2266], Loss: 0.1795\n",
      "Epoch [92/100], Step [1100/2266], Loss: 0.1656\n",
      "Epoch [92/100], Step [1200/2266], Loss: 0.2709\n",
      "Epoch [92/100], Step [1300/2266], Loss: 0.2785\n",
      "Epoch [92/100], Step [1400/2266], Loss: 0.1570\n",
      "Epoch [92/100], Step [1500/2266], Loss: 0.1408\n",
      "Epoch [92/100], Step [1600/2266], Loss: 0.2130\n",
      "Epoch [92/100], Step [1700/2266], Loss: 0.1512\n",
      "Epoch [92/100], Step [1800/2266], Loss: 0.0727\n",
      "Epoch [92/100], Step [1900/2266], Loss: 0.4243\n",
      "Epoch [92/100], Step [2000/2266], Loss: 0.1840\n",
      "Epoch [92/100], Step [2100/2266], Loss: 0.3125\n",
      "Epoch [92/100], Step [2200/2266], Loss: 0.6159\n",
      "Epoch [93/100], Step [100/2266], Loss: 0.0674\n",
      "Epoch [93/100], Step [200/2266], Loss: 0.5352\n",
      "Epoch [93/100], Step [300/2266], Loss: 0.1305\n",
      "Epoch [93/100], Step [400/2266], Loss: 0.0710\n",
      "Epoch [93/100], Step [500/2266], Loss: 0.2172\n",
      "Epoch [93/100], Step [600/2266], Loss: 0.1298\n",
      "Epoch [93/100], Step [700/2266], Loss: 0.1766\n",
      "Epoch [93/100], Step [800/2266], Loss: 0.3130\n",
      "Epoch [93/100], Step [900/2266], Loss: 0.0255\n",
      "Epoch [93/100], Step [1000/2266], Loss: 0.1783\n",
      "Epoch [93/100], Step [1100/2266], Loss: 0.0589\n",
      "Epoch [93/100], Step [1200/2266], Loss: 0.4342\n",
      "Epoch [93/100], Step [1300/2266], Loss: 0.1307\n",
      "Epoch [93/100], Step [1400/2266], Loss: 0.0920\n",
      "Epoch [93/100], Step [1500/2266], Loss: 0.3445\n",
      "Epoch [93/100], Step [1600/2266], Loss: 0.2609\n",
      "Epoch [93/100], Step [1700/2266], Loss: 0.1865\n",
      "Epoch [93/100], Step [1800/2266], Loss: 0.2226\n",
      "Epoch [93/100], Step [1900/2266], Loss: 0.3596\n",
      "Epoch [93/100], Step [2000/2266], Loss: 0.0950\n",
      "Epoch [93/100], Step [2100/2266], Loss: 0.2874\n",
      "Epoch [93/100], Step [2200/2266], Loss: 0.2405\n",
      "Epoch [94/100], Step [100/2266], Loss: 0.0397\n",
      "Epoch [94/100], Step [200/2266], Loss: 0.0485\n",
      "Epoch [94/100], Step [300/2266], Loss: 0.1590\n",
      "Epoch [94/100], Step [400/2266], Loss: 0.0569\n",
      "Epoch [94/100], Step [500/2266], Loss: 0.2140\n",
      "Epoch [94/100], Step [600/2266], Loss: 0.0930\n",
      "Epoch [94/100], Step [700/2266], Loss: 0.1188\n",
      "Epoch [94/100], Step [800/2266], Loss: 0.0552\n",
      "Epoch [94/100], Step [900/2266], Loss: 0.1582\n",
      "Epoch [94/100], Step [1000/2266], Loss: 0.3035\n",
      "Epoch [94/100], Step [1100/2266], Loss: 0.0930\n",
      "Epoch [94/100], Step [1200/2266], Loss: 0.4477\n",
      "Epoch [94/100], Step [1300/2266], Loss: 0.1895\n",
      "Epoch [94/100], Step [1400/2266], Loss: 0.4193\n",
      "Epoch [94/100], Step [1500/2266], Loss: 0.0474\n",
      "Epoch [94/100], Step [1600/2266], Loss: 0.3853\n",
      "Epoch [94/100], Step [1700/2266], Loss: 0.2875\n",
      "Epoch [94/100], Step [1800/2266], Loss: 0.1213\n",
      "Epoch [94/100], Step [1900/2266], Loss: 0.0347\n",
      "Epoch [94/100], Step [2000/2266], Loss: 0.1896\n",
      "Epoch [94/100], Step [2100/2266], Loss: 0.3148\n",
      "Epoch [94/100], Step [2200/2266], Loss: 0.2446\n",
      "Epoch [95/100], Step [100/2266], Loss: 0.1826\n",
      "Epoch [95/100], Step [200/2266], Loss: 0.0275\n",
      "Epoch [95/100], Step [300/2266], Loss: 0.0334\n",
      "Epoch [95/100], Step [400/2266], Loss: 0.3652\n",
      "Epoch [95/100], Step [500/2266], Loss: 0.2710\n",
      "Epoch [95/100], Step [600/2266], Loss: 0.1767\n",
      "Epoch [95/100], Step [700/2266], Loss: 0.0424\n",
      "Epoch [95/100], Step [800/2266], Loss: 0.1355\n",
      "Epoch [95/100], Step [900/2266], Loss: 0.2361\n",
      "Epoch [95/100], Step [1000/2266], Loss: 0.0936\n",
      "Epoch [95/100], Step [1100/2266], Loss: 0.0986\n",
      "Epoch [95/100], Step [1200/2266], Loss: 0.2725\n",
      "Epoch [95/100], Step [1300/2266], Loss: 0.2751\n",
      "Epoch [95/100], Step [1400/2266], Loss: 0.3325\n",
      "Epoch [95/100], Step [1500/2266], Loss: 0.1386\n",
      "Epoch [95/100], Step [1600/2266], Loss: 0.2380\n",
      "Epoch [95/100], Step [1700/2266], Loss: 0.4298\n",
      "Epoch [95/100], Step [1800/2266], Loss: 0.1843\n",
      "Epoch [95/100], Step [1900/2266], Loss: 0.0998\n",
      "Epoch [95/100], Step [2000/2266], Loss: 0.6809\n",
      "Epoch [95/100], Step [2100/2266], Loss: 0.2921\n",
      "Epoch [95/100], Step [2200/2266], Loss: 0.0201\n",
      "Epoch [96/100], Step [100/2266], Loss: 0.2799\n",
      "Epoch [96/100], Step [200/2266], Loss: 0.1643\n",
      "Epoch [96/100], Step [300/2266], Loss: 0.0453\n",
      "Epoch [96/100], Step [400/2266], Loss: 0.1106\n",
      "Epoch [96/100], Step [500/2266], Loss: 0.1567\n",
      "Epoch [96/100], Step [600/2266], Loss: 0.1018\n",
      "Epoch [96/100], Step [700/2266], Loss: 0.1787\n",
      "Epoch [96/100], Step [800/2266], Loss: 0.3794\n",
      "Epoch [96/100], Step [900/2266], Loss: 0.0403\n",
      "Epoch [96/100], Step [1000/2266], Loss: 0.0333\n",
      "Epoch [96/100], Step [1100/2266], Loss: 0.1297\n",
      "Epoch [96/100], Step [1200/2266], Loss: 0.1294\n",
      "Epoch [96/100], Step [1300/2266], Loss: 0.3323\n",
      "Epoch [96/100], Step [1400/2266], Loss: 0.0138\n",
      "Epoch [96/100], Step [1500/2266], Loss: 0.2853\n",
      "Epoch [96/100], Step [1600/2266], Loss: 0.2272\n",
      "Epoch [96/100], Step [1700/2266], Loss: 0.0656\n",
      "Epoch [96/100], Step [1800/2266], Loss: 0.2274\n",
      "Epoch [96/100], Step [1900/2266], Loss: 0.2161\n",
      "Epoch [96/100], Step [2000/2266], Loss: 0.4993\n",
      "Epoch [96/100], Step [2100/2266], Loss: 0.4317\n",
      "Epoch [96/100], Step [2200/2266], Loss: 0.1338\n",
      "Epoch [97/100], Step [100/2266], Loss: 0.1153\n",
      "Epoch [97/100], Step [200/2266], Loss: 0.1133\n",
      "Epoch [97/100], Step [300/2266], Loss: 0.2134\n",
      "Epoch [97/100], Step [400/2266], Loss: 0.3385\n",
      "Epoch [97/100], Step [500/2266], Loss: 0.2470\n",
      "Epoch [97/100], Step [600/2266], Loss: 0.3623\n",
      "Epoch [97/100], Step [700/2266], Loss: 0.2061\n",
      "Epoch [97/100], Step [800/2266], Loss: 0.2723\n",
      "Epoch [97/100], Step [900/2266], Loss: 0.1632\n",
      "Epoch [97/100], Step [1000/2266], Loss: 0.1062\n",
      "Epoch [97/100], Step [1100/2266], Loss: 0.0975\n",
      "Epoch [97/100], Step [1200/2266], Loss: 0.3868\n",
      "Epoch [97/100], Step [1300/2266], Loss: 0.0461\n",
      "Epoch [97/100], Step [1400/2266], Loss: 0.1984\n",
      "Epoch [97/100], Step [1500/2266], Loss: 0.2260\n",
      "Epoch [97/100], Step [1600/2266], Loss: 0.0747\n",
      "Epoch [97/100], Step [1700/2266], Loss: 0.0613\n",
      "Epoch [97/100], Step [1800/2266], Loss: 0.2484\n",
      "Epoch [97/100], Step [1900/2266], Loss: 0.4492\n",
      "Epoch [97/100], Step [2000/2266], Loss: 0.2564\n",
      "Epoch [97/100], Step [2100/2266], Loss: 0.6114\n",
      "Epoch [97/100], Step [2200/2266], Loss: 0.4655\n",
      "Epoch [98/100], Step [100/2266], Loss: 0.4266\n",
      "Epoch [98/100], Step [200/2266], Loss: 0.1004\n",
      "Epoch [98/100], Step [300/2266], Loss: 0.4962\n",
      "Epoch [98/100], Step [400/2266], Loss: 0.0112\n",
      "Epoch [98/100], Step [500/2266], Loss: 0.0745\n",
      "Epoch [98/100], Step [600/2266], Loss: 0.1152\n",
      "Epoch [98/100], Step [700/2266], Loss: 0.2890\n",
      "Epoch [98/100], Step [800/2266], Loss: 0.1128\n",
      "Epoch [98/100], Step [900/2266], Loss: 0.4901\n",
      "Epoch [98/100], Step [1000/2266], Loss: 0.0422\n",
      "Epoch [98/100], Step [1100/2266], Loss: 0.1618\n",
      "Epoch [98/100], Step [1200/2266], Loss: 0.1521\n",
      "Epoch [98/100], Step [1300/2266], Loss: 0.1730\n",
      "Epoch [98/100], Step [1400/2266], Loss: 0.3741\n",
      "Epoch [98/100], Step [1500/2266], Loss: 0.2159\n",
      "Epoch [98/100], Step [1600/2266], Loss: 0.1027\n",
      "Epoch [98/100], Step [1700/2266], Loss: 0.3501\n",
      "Epoch [98/100], Step [1800/2266], Loss: 1.0621\n",
      "Epoch [98/100], Step [1900/2266], Loss: 0.1596\n",
      "Epoch [98/100], Step [2000/2266], Loss: 0.2158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [98/100], Step [2100/2266], Loss: 0.1133\n",
      "Epoch [98/100], Step [2200/2266], Loss: 0.1196\n",
      "Epoch [99/100], Step [100/2266], Loss: 0.0327\n",
      "Epoch [99/100], Step [200/2266], Loss: 0.3925\n",
      "Epoch [99/100], Step [300/2266], Loss: 0.2096\n",
      "Epoch [99/100], Step [400/2266], Loss: 0.0414\n",
      "Epoch [99/100], Step [500/2266], Loss: 0.1283\n",
      "Epoch [99/100], Step [600/2266], Loss: 0.2157\n",
      "Epoch [99/100], Step [700/2266], Loss: 0.2944\n",
      "Epoch [99/100], Step [800/2266], Loss: 0.0891\n",
      "Epoch [99/100], Step [900/2266], Loss: 0.2876\n",
      "Epoch [99/100], Step [1000/2266], Loss: 0.0900\n",
      "Epoch [99/100], Step [1100/2266], Loss: 0.1432\n",
      "Epoch [99/100], Step [1200/2266], Loss: 0.1239\n",
      "Epoch [99/100], Step [1300/2266], Loss: 0.3998\n",
      "Epoch [99/100], Step [1400/2266], Loss: 0.1214\n",
      "Epoch [99/100], Step [1500/2266], Loss: 0.1454\n",
      "Epoch [99/100], Step [1600/2266], Loss: 0.1435\n",
      "Epoch [99/100], Step [1700/2266], Loss: 0.3243\n",
      "Epoch [99/100], Step [1800/2266], Loss: 0.2187\n",
      "Epoch [99/100], Step [1900/2266], Loss: 0.1069\n",
      "Epoch [99/100], Step [2000/2266], Loss: 0.2094\n",
      "Epoch [99/100], Step [2100/2266], Loss: 0.2293\n",
      "Epoch [99/100], Step [2200/2266], Loss: 0.0912\n",
      "Epoch [100/100], Step [100/2266], Loss: 0.0505\n",
      "Epoch [100/100], Step [200/2266], Loss: 0.0446\n",
      "Epoch [100/100], Step [300/2266], Loss: 0.0796\n",
      "Epoch [100/100], Step [400/2266], Loss: 0.3081\n",
      "Epoch [100/100], Step [500/2266], Loss: 0.1363\n",
      "Epoch [100/100], Step [600/2266], Loss: 0.0793\n"
     ]
    }
   ],
   "source": [
    "# model run\n",
    "args = get_args()\n",
    "trainer = Trainer(args)\n",
    "\n",
    "if args.mode == 'train':\n",
    "  trainer.train()\n",
    "else:\n",
    "  trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rN2Bw-eZCFVj"
   },
   "outputs": [],
   "source": [
    "# model run\n",
    "args = get_args2()\n",
    "trainer = Trainer(args)\n",
    "\n",
    "if args.mode == 'train':\n",
    "  trainer.train()\n",
    "else:\n",
    "  trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eEQjpy0WWCS1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"64x64\\\\test_binary.pkl\", 'rb') as f:\n",
    "    d = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_ = d[10][0]\n",
    "d_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarizer = Binarizer(threshold=123)\n",
    "x___ = binarizer.transform(x__.reshape(64,64))\n",
    "print(x___)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(d_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join('64x64', 'train_norm_binary') + '.pkl'\n",
    "\n",
    "df = pd.read_csv(os.path.join('64x64', 'train') + '.csv', names=range(0,4097))\n",
    "\n",
    "data = []\n",
    "\n",
    "y = df[0]\n",
    "x = np.array(df.drop(labels=[0], axis=1)).reshape(-1, 64, 64)\n",
    "binarizer = Binarizer(threshold=123)\n",
    "\n",
    "for i in range(len(y)):\n",
    "    x[i] = binarizer.transform(x[i])\n",
    "    print(x[i].shape)\n",
    "    x[i].reshape(1,64,64)\n",
    "    data.append((x[i], y[i]))\n",
    "    if(i % 1000 == 0):\n",
    "        print(i)\n",
    "print(len(data))\n",
    "with open(data_path, 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join('64x64', 'test_binary') + '.pkl'\n",
    "\n",
    "df = pd.read_csv(os.path.join('64x64', 'test') + '.csv', names=range(0,4097))\n",
    "\n",
    "data = []\n",
    "\n",
    "y = df[0]\n",
    "x = np.array(df.drop(labels=[0], axis=1)).reshape(-1, 64, 64)\n",
    "binarizer = Binarizer(threshold=123)\n",
    "\n",
    "for i in range(len(y)):\n",
    "    x[i] = binarizer.transform(x[i])\n",
    "    x[i].reshape(1,64,64)\n",
    "    data.append((x[i], y[i]))\n",
    "    if(i % 1000 == 0):\n",
    "        print(i)\n",
    "print(len(data))\n",
    "with open(data_path, 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join('64x64', 'val_binary') + '.pkl'\n",
    "\n",
    "df = pd.read_csv(os.path.join('64x64', 'val') + '.csv', names=range(0,4097))\n",
    "\n",
    "data = []\n",
    "\n",
    "y = df[0]\n",
    "x = np.array(df.drop(labels=[0], axis=1)).reshape(-1, 64, 64)\n",
    "binarizer = Binarizer(threshold=123)\n",
    "\n",
    "for i in range(len(y)):\n",
    "    x[i] = binarizer.transform(x[i])\n",
    "    x[i].reshape(1,64,64)\n",
    "    data.append((x[i], y[i]))\n",
    "    if(i % 1000 == 0):\n",
    "        print(i)\n",
    "print(len(data))\n",
    "with open(data_path, 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "normalized_cnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
